{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHvdyxa8XwTQ8tYZhys8+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikalitt1/Clementine-Agriculture/blob/main/Clementine_Ag_W_A_S_P_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![clementine_logo_noback_small.PNG](data:image/png;base64,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)\n",
        "\n",
        "# Clementine Agriculture Tutorial: Training, deployment and setup of scripts for the W.A.S.P and other modules\n",
        "\n",
        "Welcome!\n",
        "\n",
        "This tutorial will include:\n",
        "* Setting up Jetson Orin Nano for deploymeny in Clementine Ag products\n",
        "*Scripts for product operation, training, and datacollection\n"
      ],
      "metadata": {
        "id": "TzIwCOw6cl5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, run the following commands on the Jetson orin nano after initial boot into Jetpack 6.2 in order have all the packages run properly. These setup commands can be found in the following links as well:\n",
        "* https://docs.luxonis.com/hardware/platform/deploy/to-jetson/\n",
        "* https://docs.ultralytics.com/guides/nvidia-jetson/#install-ultralytics-package\n",
        "* https://docs.ultralytics.com/modes/train/#apple-silicon-mps-training\n"
      ],
      "metadata": {
        "id": "afqB6KCVc6sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initial Jetson Orin Nano setup"
      ],
      "metadata": {
        "id": "n69TS7qQhmZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt update && sudo apt upgrade\n",
        "sudo reboot now"
      ],
      "metadata": {
        "id": "Vlm5DCfAe1FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable ZRAM:\n",
        "sudo systemctl disable nvzramconfig\n",
        "# Create 4GB swap file\n",
        "sudo fallocate -l 4G /mnt/4GB.swap\n",
        "sudo chmod 600 /mnt/4GB.swap\n",
        "sudo mkswap /mnt/4GB.swap"
      ],
      "metadata": {
        "id": "6769Ytvje5Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo vi /etc/fstab\n",
        "# Add this line at the bottom of the file\n",
        "/mnt/4GB.swap swap swap defaults 0 0\n",
        "# Reboot\n",
        "sudo reboot now"
      ],
      "metadata": {
        "id": "83PclkeUe84P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo wget -qO- https://docs.luxonis.com/install_dependencies.sh | bash"
      ],
      "metadata": {
        "id": "PMZ-xjSLfAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next step is to start a virtual environment on the Jetson ON, but because this computer will not be used as a general device and is only running the scripts for the W.A.S.P, its not mandatory."
      ],
      "metadata": {
        "id": "LGeSwPqtfMBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt install python3-venv\n",
        "python3 -m venv depthai\n",
        "source depthai/bin/activate"
      ],
      "metadata": {
        "id": "tB5NsN60fCcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone github repository\n",
        "git clone https://github.com/luxonis/depthai-python.git\n",
        "cd depthai-python\n",
        "python3 examples/install_requirements.py"
      ],
      "metadata": {
        "id": "43y641egfFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"export OPENBLAS_CORETYPE=ARMV8\" >> ~/.bashrc"
      ],
      "metadata": {
        "id": "J5neOPOBff_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have copy and pasted each of these commands into a freshly botted Jetson ON running Jetpack 6.2, the OAK D Lite camera should work when connected to the Jetson ON via USB port.\n",
        "\n",
        "The next commands should also be run on the Jetson ON, they are the setup commands so that Ultralytics YOLO models can run on the Jetson ON properly."
      ],
      "metadata": {
        "id": "diJH3S1SfkB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt update\n",
        "sudo apt install python3-pip -y\n",
        "pip install -U pip"
      ],
      "metadata": {
        "id": "N9vZtxuagH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "id": "fYh1ppgJgcpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo reboot"
      ],
      "metadata": {
        "id": "CkmIOYiAggbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.5.0a0+872d972e41.nv24.08-cp310-cp310-linux_aarch64.whl\n",
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.20.0a0+afc54f7-cp310-cp310-linux_aarch64.whl"
      ],
      "metadata": {
        "id": "33cd-Ak2gjNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/arm64/cuda-keyring_1.1-1_all.deb\n",
        "sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install libcusparselt0 libcusparselt-dev"
      ],
      "metadata": {
        "id": "Dhuzs-vqgm0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/onnxruntime_gpu-1.20.0-cp310-cp310-linux_aarch64.whl"
      ],
      "metadata": {
        "id": "j5UA-DCJgpoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "0IOBfKIQgszv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since numpy will be version 1.23.5 on the Jetson ON, you also need to install that version in this colab to ensure when any training scripts are run in this colab it will use the same version of numpy. If you do not do this the trained model will not work once you transfer it to the Jeton ON. Run the command below in this colab:"
      ],
      "metadata": {
        "id": "JgSRaCSugwhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "41OIb71lhFUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c5705-a79e-4761-89ed-11773ad68255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Collecting Data for Object Detection"
      ],
      "metadata": {
        "id": "oc4FqfhgLg_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect an oak d lite camera to this computer via a usbc cable, run the below script and collect as much high uality video feed of the objecdts you want to identify, keep in mind colab does not have access to the computer hardware so the below script needs to be run on jupyter notebook instead:"
      ],
      "metadata": {
        "id": "DgZNbk6sLsJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import depthai as dai\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = dai.Pipeline()\n",
        "\n",
        "# Define sources and outputs\n",
        "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
        "xout_video = pipeline.create(dai.node.XLinkOut)\n",
        "\n",
        "xout_video.setStreamName(\"video\")\n",
        "\n",
        "# Set camera properties\n",
        "cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
        "cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
        "cam_rgb.setFps(30)\n",
        "\n",
        "# Link the camera's 'preview' output to the XLinkOut node\n",
        "cam_rgb.video.link(xout_video.input)\n",
        "\n",
        "# Output file properties\n",
        "output_file = \"oakd_pro_out4.avi\"\n",
        "frame_width = 1920\n",
        "frame_height = 1080\n",
        "fps = 30\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Connect to the device and start the pipeline\n",
        "with dai.Device(pipeline) as device:\n",
        "    print(\"Connected to OAK-D Lite. Press 'q' to quit.\")\n",
        "\n",
        "    # Get the output queue for the video stream\n",
        "    video_queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
        "\n",
        "    while True:\n",
        "        # Check if a new frame is available\n",
        "        in_video = video_queue.tryGet()\n",
        "        if in_video:\n",
        "            frame = in_video.getCvFrame()\n",
        "\n",
        "            # Write the frame to the video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # Display the frame\n",
        "            cv2.imshow(\"OAK-D Lite Video\", frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "# Release resources\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(f\"Video saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "b2ifvGjwL2UN",
        "outputId": "082522c7-103c-4ac2-a507-f59d7d1950bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a5e381080b58>:14: DeprecationWarning: RGB is deprecated, use CAM_A or address camera by name instead.\n",
            "  cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot find any device with given deviceInfo",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a5e381080b58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Connect to the device and start the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connected to OAK-D Lite. Press 'q' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot find any device with given deviceInfo"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training Ultralytics YOLO11n Model"
      ],
      "metadata": {
        "id": "MKcechtjhjHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The annotating and dataset preparation can be done for any model in Roboflow, make sure to export the dataset as a zip file in \"YOLO V5 Pytorch format\". From here on it will be assumed the dataset is already prepared and saved."
      ],
      "metadata": {
        "id": "x6QX4RJnh5sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the dataset zip file from roboflow in the side bar, then run the following commands to trin the yolo11n model. make sure to change the file names accordingly, and modify the data.yaml file to match the actual paths to the train, test, and validation paths."
      ],
      "metadata": {
        "id": "eupLTHgrjDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/roboflow_dataset.zip -d /content/dataset/"
      ],
      "metadata": {
        "id": "hT1Yr_IOnepJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!yolo detect train data=data.yaml model=yolo11n.yaml epochs=100 imgsz=640"
      ],
      "metadata": {
        "id": "4s9BFLFIjl1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "once run, the model weights can be found in the following file path in the side bar: /yolov5/runs/train/exp/weights/best.pt\n",
        "\n",
        "you can now send the model weights (best.pt) to the Jetson ON."
      ],
      "metadata": {
        "id": "L5ovZ-KCoRRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now on the Jetson ON, you cannot directly run the best.pt file, you first need to convert it to a .engine file with the following command, make sure to run this command on the Jetson ON!"
      ],
      "metadata": {
        "id": "iAyPQbbzoraP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo export model=yolo11n.pt format=engine # creates 'yolo11n.engine'"
      ],
      "metadata": {
        "id": "qXYkq93lo1G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now the object detection setup is done, now its time to train two random forest models to get the W.A.S.P arms to aim accurately at the targets identified by the YOLO11n model."
      ],
      "metadata": {
        "id": "5Uldq807pAwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Training Random Forest Aiming Model"
      ],
      "metadata": {
        "id": "0ibMl9ejpM8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests are a type of decision tree which can be thought of as a precursor to neural nets. They are better for maping pixel and depth coordinates to servo motor agles then traditional nurel nets because they are more robust when new data it has not been trainined on is presented. In the future the W.A.S.P will be training using reinforced learning methods in order to make sure it aims accurately and properly in a number of different scenarios. Unfortunately, Random forests cannot be fine tuned in real time like reinforced learning models. but for ealy models random forests are more then enough to aim accurately in most settings. The code below should be ran on the Jetson ON, it is used to collect data for the random forests model, remember to change the pin numbers for each arm:"
      ],
      "metadata": {
        "id": "0svO5lmJpToF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import depthai as dai\n",
        "import time\n",
        "import threading\n",
        "import numpy as np\n",
        "import csv\n",
        "from smbus2 import SMBus\n",
        "import math  # For atan2\n",
        "\n",
        "# PCA9685 Constants\n",
        "PCA9685_ADDRESS = 0x40\n",
        "MODE1 = 0x00\n",
        "MODE2 = 0x01\n",
        "LED0_ON_L = 0x06\n",
        "LED0_ON_H = 0x07\n",
        "LED0_OFF_L = 0x08\n",
        "LED0_OFF_H = 0x09\n",
        "\n",
        "bus = SMBus(7)\n",
        "\n",
        "# Initialize PCA9685\n",
        "def init_pca9685():\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE1, 0x00)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE2, 0x04)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Set PWM for servo motors\n",
        "def set_pwm(channel, pulse_width):\n",
        "    on_time = 0\n",
        "    off_time = int(pulse_width)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_ON_L + 4 * channel, on_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_ON_H + 4 * channel, (on_time >> 8) & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_OFF_L + 4 * channel, off_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_OFF_H + 4 * channel, (off_time >> 8) & 0xFF)\n",
        "\n",
        "def angle_to_pulse_width(angle, min_us=500, max_us=2500):\n",
        "    return int((angle / 180.0) * (max_us - min_us) + min_us)\n",
        "\n",
        "# Control the laser diode\n",
        "def control_diode(on=True):\n",
        "    pulse_width = int((0.3 / 100.0) * 4095)\n",
        "    set_pwm(13, pulse_width if on else 0)\n",
        "\n",
        "# Save data to CSV\n",
        "def save_to_csv(angle_0, angle_1, depth, center_x, center_y, tilt_x, tilt_y):\n",
        "    with open('mlp_training_data_long.csv', mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([angle_0, angle_1, depth, center_x, center_y, tilt_x, tilt_y])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_zigzag_path(x_range, y_range, x_step, y_step, start_corner='top-left', zigzag_axis='horizontal'):\n",
        "    path = []\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "\n",
        "    # Generate the axis ranges using numpy to support floats\n",
        "    x_forward = np.arange(x_min, x_max + x_step, x_step)\n",
        "    x_backward = x_forward[::-1]\n",
        "    y_forward = np.arange(y_min, y_max + y_step, y_step)\n",
        "    y_backward = y_forward[::-1]\n",
        "\n",
        "    corners = {\n",
        "        'top-left':     (x_forward,  y_forward),\n",
        "        'top-right':    (x_backward, y_forward),\n",
        "        'bottom-left':  (x_forward,  y_backward),\n",
        "        'bottom-right': (x_backward, y_backward)\n",
        "    }\n",
        "\n",
        "    if start_corner not in corners:\n",
        "        raise ValueError(f\"Invalid start_corner '{start_corner}'. Choose from: {list(corners.keys())}\")\n",
        "    if zigzag_axis not in ['horizontal', 'vertical']:\n",
        "        raise ValueError(\"zigzag_axis must be 'horizontal' or 'vertical'\")\n",
        "\n",
        "    x_order, y_order = corners[start_corner]\n",
        "\n",
        "    if zigzag_axis == 'horizontal':\n",
        "        for i, y in enumerate(y_order):\n",
        "            x_values = x_order if i % 2 == 0 else x_order[::-1]\n",
        "            for x in x_values:\n",
        "                path.append((round(x, 6), round(y, 6)))\n",
        "    else:  # vertical\n",
        "        for i, x in enumerate(x_order):\n",
        "            y_values = y_order if i % 2 == 0 else y_order[::-1]\n",
        "            for y in y_values:\n",
        "                path.append((round(x, 6), round(y, 6)))\n",
        "\n",
        "    return path\n",
        "\n",
        "\n",
        "\n",
        "# Calculate tilt from accelerometer data\n",
        "def calculate_tilt(accel_x, accel_y, accel_z):\n",
        "    tilt_x = math.atan2(accel_x, math.sqrt(accel_y**2 + accel_z**2))  # Pitch\n",
        "    tilt_y = math.atan2(accel_y, math.sqrt(accel_x**2 + accel_z**2))  # Roll\n",
        "    return math.degrees(tilt_x), math.degrees(tilt_y)  # Convert radians to degrees\n",
        "\n",
        "def move_servos(angle_list, video_queue, depth_queue, imu_queue, stop_event):\n",
        "    angle_index = 0\n",
        "    direction = 1  # 1 for forward, -1 for reverse\n",
        "\n",
        "    while not stop_event.is_set():\n",
        "        angle_x, angle_y = angle_list[angle_index]\n",
        "\n",
        "        # Capture frame and depth before moving the servos\n",
        "        frame_data = video_queue.get()\n",
        "        frame = frame_data.getCvFrame()\n",
        "\n",
        "        depth_data = depth_queue.get()\n",
        "        depth_frame = depth_data.getFrame()\n",
        "\n",
        "        imu_data = imu_queue.get()\n",
        "        imu_packets = imu_data.packets\n",
        "\n",
        "        tilt_x, tilt_y = 0, 0  # Default tilt values\n",
        "        for imu_packet in imu_packets:\n",
        "            gyro_data = imu_packet.gyroscope\n",
        "            accel_data = imu_packet.acceleroMeter  # Assuming accelerometer data is available\n",
        "\n",
        "            accel_x = accel_data.x\n",
        "            accel_y = accel_data.y\n",
        "            accel_z = accel_data.z\n",
        "\n",
        "            tilt_x, tilt_y = calculate_tilt(accel_x, accel_y, accel_z)\n",
        "\n",
        "            print(f\"Tilt Data - Pitch: {tilt_x:.2f}, Roll: {tilt_y:.2f}\")\n",
        "\n",
        "        # Process frame to find brightest spot\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        # Convert to grayscale and blur to reduce noise\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # Apply a threshold to extract bright regions\n",
        "        _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Find contours of bright areas\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if contours:\n",
        "            # Find the largest bright area (by contour area)\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "            # Get the bounding box and center of the largest contour\n",
        "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "            center_x = x + w // 2\n",
        "            center_y = y + h // 2\n",
        "\n",
        "            # Optionally draw the bounding box and center\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "            cv2.circle(frame, (center_x, center_y), 3, (0, 0, 255), -1)\n",
        "        else:\n",
        "            center_x, center_y = -1, -1  # No bright spot found\n",
        "\n",
        "\n",
        "        # Draw a bounding circle around the brightest spot\n",
        "        cv2.circle(frame, (center_x, center_y), 2, (0, 255, 0), 2)\n",
        "\n",
        "        depth_value = np.mean(depth_frame)\n",
        "\n",
        "        # Save the data\n",
        "        save_to_csv(angle_x, angle_y, round(depth_value), center_x, center_y, round(tilt_x, 2), round(tilt_y, 2))\n",
        "\n",
        "        # Move servos\n",
        "        set_pwm(15, angle_to_pulse_width(angle_x))\n",
        "        set_pwm(14, angle_to_pulse_width(angle_y))\n",
        "\n",
        "        control_diode(True)  # Turn on laser\n",
        "\n",
        "        # Update index with direction\n",
        "        angle_index += direction\n",
        "\n",
        "        # Reverse direction at ends\n",
        "        if angle_index == len(angle_list) - 1 or angle_index == 0:\n",
        "            direction *= -1\n",
        "\n",
        "        time.sleep(0.5)  # Adjust delay if needed\n",
        "\n",
        "\n",
        "# Setup depthai pipeline\n",
        "def setup_depthai_pipeline():\n",
        "    pipeline = dai.Pipeline()\n",
        "\n",
        "    # Color Camera\n",
        "    cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
        "    cam_rgb.setPreviewSize(640, 360)\n",
        "    cam_rgb.setBoardSocket(dai.CameraBoardSocket.CAM_A)\n",
        "    cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
        "    cam_rgb.setInterleaved(False)\n",
        "    cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
        "    cam_rgb.setFps(35)\n",
        "\n",
        "    # Depth Camera\n",
        "    mono_left = pipeline.create(dai.node.MonoCamera)\n",
        "    mono_right = pipeline.create(dai.node.MonoCamera)\n",
        "    stereo = pipeline.create(dai.node.StereoDepth)\n",
        "\n",
        "    mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "    mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "    mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
        "    mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
        "\n",
        "    stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
        "    stereo.setLeftRightCheck(True)\n",
        "    stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)\n",
        "    stereo.setSubpixel(True)\n",
        "\n",
        "    mono_left.out.link(stereo.left)\n",
        "    mono_right.out.link(stereo.right)\n",
        "\n",
        "    #IMU\n",
        "    imu = pipeline.create(dai.node.IMU)\n",
        "    imu.enableIMUSensor(dai.IMUSensor.GYROSCOPE_RAW, 100)\n",
        "    imu.enableIMUSensor(dai.IMUSensor.ACCELEROMETER_RAW, 100)\n",
        "    imu_queue = pipeline.create(dai.node.XLinkOut)\n",
        "    imu_queue.setStreamName(\"imu\")\n",
        "    imu.out.link(imu_queue.input)\n",
        "\n",
        "    # Output Streams\n",
        "    xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
        "    xout_rgb.setStreamName(\"rgb\")\n",
        "    xout_depth = pipeline.create(dai.node.XLinkOut)\n",
        "    xout_depth.setStreamName(\"depth\")\n",
        "\n",
        "    cam_rgb.preview.link(xout_rgb.input)\n",
        "    stereo.depth.link(xout_depth.input)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def main():\n",
        "    init_pca9685()\n",
        "\n",
        "    # Generate the angle list for flat its (25, 90) and (25, 59) for long its (20, 80) and (40, 75)\n",
        "    angle_list = generate_zigzag_path((25, 80), (40, 70), 1, 1, 'top-left', 'horizontal')\n",
        "\n",
        "    # Setup depthai pipeline\n",
        "    pipeline = setup_depthai_pipeline()\n",
        "\n",
        "    # Start depthai device\n",
        "    with dai.Device(pipeline) as device:\n",
        "        video_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
        "        depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
        "        imu_queue = device.getOutputQueue(name=\"imu\", maxSize=4, blocking=False)\n",
        "        # Stop event to manage the thread\n",
        "        stop_event = threading.Event()\n",
        "\n",
        "        # Start the servo movement and data collection in a separate thread\n",
        "        threading.Thread(target=move_servos, args=(angle_list, video_queue, depth_queue, imu_queue, stop_event), daemon=True).start()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                frame_data = video_queue.get()\n",
        "                frame = frame_data.getCvFrame()\n",
        "\n",
        "                # Convert to grayscale and blur to reduce noise\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "                # Apply a threshold to extract bright regions\n",
        "                _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "                # Find contours of bright areas\n",
        "                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                if contours:\n",
        "                    # Find the largest bright area (by contour area)\n",
        "                    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "                    # Get the bounding box and center of the largest contour\n",
        "                    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "                    center_x = x + w // 2\n",
        "                    center_y = y + h // 2\n",
        "\n",
        "                    # Draw the bounding box and center\n",
        "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "                    cv2.circle(frame, (center_x, center_y), 3, (0, 0, 255), -1)\n",
        "                else:\n",
        "                    center_x, center_y = -1, -1\n",
        "                    cv2.circle(frame, (center_x, center_y), 2, (0, 255, 0), 2)\n",
        "\n",
        "                # Display the current frame with circle\n",
        "                cv2.imshow(\"Detection\", frame)\n",
        "\n",
        "                if cv2.waitKey(1) == ord(\"q\"):\n",
        "                    stop_event.set()\n",
        "                    break\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"Shutting down...\")\n",
        "                stop_event.set()\n",
        "                break\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "q2bk22GxFzR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once enough data with diverse depth values has been collected, upload the csv files to the sidebar on this notebook and run the code below to train the random forest model, remeber to change the file names and train both arms using seperate csv files. This means you need to run the following code twice for each csv file for each arm."
      ],
      "metadata": {
        "id": "B8jjw5qjF2eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Load CSV\n",
        "csv_filename = \"/content/mlp_training_data_flat_cleaned.csv\"\n",
        "df = pd.read_csv(csv_filename, header=None)\n",
        "df.columns = [\"angle_0\", \"angle_1\", \"depth\", \"center_x\", \"center_y\", \"tilt_x\", \"tilt_y\"]\n",
        "\n",
        "# Features and targets\n",
        "y = df[[\"angle_0\", \"angle_1\"]].values\n",
        "x = df[[\"center_x\", \"center_y\", \"depth\", \"tilt_x\", \"tilt_y\"]].values\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Decision Tree Regressor ==========\n",
        "# Initialize and train decision tree\n",
        "tree = DecisionTreeRegressor(max_depth=22, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Save the decision tree model\n",
        "tree_model_path = \"/content/decision_tree_flat_fine2.pkl\"\n",
        "with open(tree_model_path, 'wb') as f:\n",
        "    pickle.dump(tree, f)\n",
        "\n",
        "print(f\"Decision tree trained and saved to {tree_model_path}\")\n",
        "files.download(tree_model_path)\n",
        "\n",
        "# Predict on validation set\n",
        "val_preds_tree = tree.predict(X_val)\n",
        "\n",
        "# Compute MAE for each angle\n",
        "mae_angle0_tree = mean_absolute_error(y_val[:, 0], val_preds_tree[:, 0])\n",
        "mae_angle1_tree = mean_absolute_error(y_val[:, 1], val_preds_tree[:, 1])\n",
        "avg_mae_tree = (mae_angle0_tree + mae_angle1_tree) / 2\n",
        "\n",
        "print(f\"\\nDecision Tree Validation MAE:\")\n",
        "print(f\"  angle_0: {mae_angle0_tree:.2f} degrees\")\n",
        "print(f\"  angle_1: {mae_angle1_tree:.2f} degrees\")\n",
        "print(f\"  Average MAE: {avg_mae_tree:.2f} degrees\")\n",
        "\n",
        "# ====== Compare Predictions to Actuals ======\n",
        "print(\"\\nSample Predictions vs Actual Values for Decision Tree:\")\n",
        "for i in range(10):\n",
        "    predicted = val_preds_tree[i]\n",
        "    actual = y_val[i]\n",
        "    print(f\"[{i}] Predicted: angle_0 = {predicted[0]:.2f}, angle_1 = {predicted[1]:.2f}  |  Actual: angle_0 = {actual[0]:.2f}, angle_1 = {actual[1]:.2f}\")\n",
        "\n",
        "\n",
        "# ========== Random Forest Regressor ==========\n",
        "# Initialize and train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Save the Random Forest model\n",
        "rf_model_path = \"/content/random_forest_flat_fine2.pkl\"\n",
        "with open(rf_model_path, 'wb') as f:\n",
        "    pickle.dump(rf, f)\n",
        "\n",
        "print(f\"Random Forest trained and saved to {rf_model_path}\")\n",
        "files.download(rf_model_path)\n",
        "\n",
        "# Predict on validation set for Random Forest\n",
        "val_preds_rf = rf.predict(X_val)\n",
        "\n",
        "# Compute MAE for Random Forest\n",
        "mae_angle0_rf = mean_absolute_error(y_val[:, 0], val_preds_rf[:, 0])\n",
        "mae_angle1_rf = mean_absolute_error(y_val[:, 1], val_preds_rf[:, 1])\n",
        "avg_mae_rf = (mae_angle0_rf + mae_angle1_rf) / 2\n",
        "\n",
        "print(f\"\\nRandom Forest Validation MAE:\")\n",
        "print(f\"  angle_0: {mae_angle0_rf:.2f} degrees\")\n",
        "print(f\"  angle_1: {mae_angle1_rf:.2f} degrees\")\n",
        "print(f\"  Average MAE: {avg_mae_rf:.2f} degrees\")\n",
        "\n",
        "# ====== Compare Predictions to Actuals ======\n",
        "print(\"\\nSample Predictions vs Actual Values for Random Forest:\")\n",
        "for i in range(10):\n",
        "    predicted = val_preds_rf[i]\n",
        "    actual = y_val[i]\n",
        "    print(f\"[{i}] Predicted: angle_0 = {predicted[0]:.2f}, angle_1 = {predicted[1]:.2f}  |  Actual: angle_0 = {actual[0]:.2f}, angle_1 = {actual[1]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uriUGi1lGPdn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "b7640cf1-a6c5-44b2-a207-154f153884de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree trained and saved to /content/decision_tree_flat_fine2.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_043548e2-4c57-414a-b900-6869f58e84c4\", \"decision_tree_flat_fine2.pkl\", 5889007)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Validation MAE:\n",
            "  angle_0: 1.03 degrees\n",
            "  angle_1: 0.17 degrees\n",
            "  Average MAE: 0.60 degrees\n",
            "\n",
            "Sample Predictions vs Actual Values for Decision Tree:\n",
            "[0] Predicted: angle_0 = 59.00, angle_1 = 28.60  |  Actual: angle_0 = 59.20, angle_1 = 28.60\n",
            "[1] Predicted: angle_0 = 28.60, angle_1 = 41.60  |  Actual: angle_0 = 28.60, angle_1 = 41.20\n",
            "[2] Predicted: angle_0 = 48.80, angle_1 = 25.80  |  Actual: angle_0 = 46.80, angle_1 = 26.40\n",
            "[3] Predicted: angle_0 = 51.40, angle_1 = 27.20  |  Actual: angle_0 = 52.00, angle_1 = 26.80\n",
            "[4] Predicted: angle_0 = 59.60, angle_1 = 31.40  |  Actual: angle_0 = 59.80, angle_1 = 31.40\n",
            "[5] Predicted: angle_0 = 49.60, angle_1 = 51.00  |  Actual: angle_0 = 49.40, angle_1 = 51.00\n",
            "[6] Predicted: angle_0 = 74.20, angle_1 = 35.40  |  Actual: angle_0 = 74.00, angle_1 = 35.40\n",
            "[7] Predicted: angle_0 = 28.40, angle_1 = 50.00  |  Actual: angle_0 = 28.20, angle_1 = 50.00\n",
            "[8] Predicted: angle_0 = 51.80, angle_1 = 29.00  |  Actual: angle_0 = 51.80, angle_1 = 28.20\n",
            "[9] Predicted: angle_0 = 32.60, angle_1 = 47.00  |  Actual: angle_0 = 30.80, angle_1 = 46.80\n",
            "Random Forest trained and saved to /content/random_forest_flat_fine2.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b580c1ec-169e-400b-a095-f03f857cf408\", \"random_forest_flat_fine2.pkl\", 372675891)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Validation MAE:\n",
            "  angle_0: 0.92 degrees\n",
            "  angle_1: 0.13 degrees\n",
            "  Average MAE: 0.52 degrees\n",
            "\n",
            "Sample Predictions vs Actual Values for Random Forest:\n",
            "[0] Predicted: angle_0 = 57.99, angle_1 = 28.70  |  Actual: angle_0 = 59.20, angle_1 = 28.60\n",
            "[1] Predicted: angle_0 = 28.70, angle_1 = 41.39  |  Actual: angle_0 = 28.60, angle_1 = 41.20\n",
            "[2] Predicted: angle_0 = 48.98, angle_1 = 26.24  |  Actual: angle_0 = 46.80, angle_1 = 26.40\n",
            "[3] Predicted: angle_0 = 52.68, angle_1 = 26.78  |  Actual: angle_0 = 52.00, angle_1 = 26.80\n",
            "[4] Predicted: angle_0 = 59.51, angle_1 = 31.31  |  Actual: angle_0 = 59.80, angle_1 = 31.40\n",
            "[5] Predicted: angle_0 = 48.73, angle_1 = 51.05  |  Actual: angle_0 = 49.40, angle_1 = 51.00\n",
            "[6] Predicted: angle_0 = 73.95, angle_1 = 35.54  |  Actual: angle_0 = 74.00, angle_1 = 35.40\n",
            "[7] Predicted: angle_0 = 28.52, angle_1 = 50.05  |  Actual: angle_0 = 28.20, angle_1 = 50.00\n",
            "[8] Predicted: angle_0 = 51.42, angle_1 = 28.23  |  Actual: angle_0 = 51.80, angle_1 = 28.20\n",
            "[9] Predicted: angle_0 = 32.36, angle_1 = 46.90  |  Actual: angle_0 = 30.80, angle_1 = 46.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the .pkl random forest files have been trained, upload them to the Jetson ON."
      ],
      "metadata": {
        "id": "-l4sAq0nu9DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are NaN values in the csv files you can run the following code to clean it up:"
      ],
      "metadata": {
        "id": "S0Bh_GadUE0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to your original CSV file\n",
        "input_csv = \"/content/mlp_training_data_fine_flat.csv\"\n",
        "\n",
        "# Output path for cleaned CSV\n",
        "output_csv = \"/content/mlp_training_data_flat_cleaned.csv\"\n",
        "\n",
        "# Load the CSV file (no header)\n",
        "df = pd.read_csv(input_csv, header=None)\n",
        "\n",
        "# Set column names\n",
        "df.columns = [\"angle_0\", \"angle_1\", \"depth\", \"center_x\", \"center_y\", \"tiltx\", \"tilty\"]\n",
        "\n",
        "# Drop rows with NaNs in input or target columns\n",
        "df_cleaned = df.dropna(subset=[\"angle_0\", \"angle_1\", \"center_x\", \"center_y\", \"depth\"])\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV\n",
        "df_cleaned.to_csv(output_csv, index=False, header=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_csv}\")\n",
        "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_cleaned)}\")"
      ],
      "metadata": {
        "id": "hJaYMFhgUNQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a58c98-a34f-40ef-8db1-9166ecda0fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset saved to: /content/mlp_training_data_flat_cleaned.csv\n",
            "Original rows: 46129, Cleaned rows: 46078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Aiming While in Motion"
      ],
      "metadata": {
        "id": "PhLW9lIPGZAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aiming while in motion can be difficult but can be done by predicting the target objects position based on the velocity fo the camera. the velocity of the camera can be derived based on GPS information but for the toolbar model this would require extra hardware. Thats why I decided to calcualte velocity based on the depth-optical flow in frame."
      ],
      "metadata": {
        "id": "UYY0DDsHGhZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This consists of looking at how the depth pixels in frame change overtime, for example if there was some peak in frame the camaera would be able to compare how far it moved in the frame by tracking it. Traditionally optical flow is used on RGB data but because there is a blue laser targeting objects in frame it can make RGB based optical flow very inaccurate."
      ],
      "metadata": {
        "id": "0oHKGGDcHwis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no training that needs to be done in this section, it will just be added in the main code."
      ],
      "metadata": {
        "id": "7byJYaSGILXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Main code"
      ],
      "metadata": {
        "id": "z1qBndkzIRWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the main code, it loads the YOLO11n model and both random forest models to aim at targets specified in the code. Copy and paste the following file onto the Jetson ON as well as intall any necesary packages:"
      ],
      "metadata": {
        "id": "u-tqOfpEIU7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import depthai as dai\n",
        "import torch\n",
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import joblib\n",
        "from smbus2 import SMBus\n",
        "import time\n",
        "import logging\n",
        "import threading\n",
        "import queue\n",
        "import math\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"toolbar_test3.engine\")\n",
        "\n",
        "# Load Random Forest models\n",
        "rf_model_long = joblib.load(\"random_forest_long_fine2.pkl\")\n",
        "rf_model_flat = joblib.load(\"random_forest_flat_fine2.pkl\")\n",
        "\n",
        "class KalmanFilter:\n",
        "    def __init__(self):\n",
        "        # Define the Kalman filter\n",
        "        self.dt = 1.0\n",
        "\n",
        "        self.A = np.array([[1, 0, self.dt, 0],\n",
        "                           [0, 1, 0, self.dt],\n",
        "                           [0, 0, 1, 0],\n",
        "                           [0, 0, 0, 1]])\n",
        "\n",
        "        self.B = np.array([[0],\n",
        "                           [0],\n",
        "                           [0],\n",
        "                           [0]])\n",
        "\n",
        "        self.H = np.array([[1, 0, 0, 0],\n",
        "                           [0, 1, 0, 0]])\n",
        "\n",
        "        self.Q = np.eye(4) * 0.1\n",
        "        self.R = np.eye(2) * 10\n",
        "        self.P = np.eye(4)\n",
        "        self.x = np.zeros((4, 1))\n",
        "        self.u = 0\n",
        "\n",
        "    def predict(self, coordX, coordY):\n",
        "        measured = np.array([[np.float32(coordX)], [np.float32(coordY)]])\n",
        "        self.x = self.A @ self.x + self.B * self.u\n",
        "        self.P = self.A @ self.P @ self.A.T + self.Q\n",
        "\n",
        "        S = self.H @ self.P @ self.H.T + self.R\n",
        "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
        "        y = measured - self.H @ self.x\n",
        "        self.x += K @ y\n",
        "        self.P = (np.eye(4) - K @ self.H) @ self.P\n",
        "        return int(self.x[0][0]), int(self.x[1][0])\n",
        "\n",
        "    def predict_future(self, steps=5):\n",
        "        future_x = self.x.copy()\n",
        "        for _ in range(steps):\n",
        "            future_x = self.A @ future_x + self.B * self.u\n",
        "        return int(future_x[0][0]), int(future_x[1][0])\n",
        "\n",
        "\n",
        "# Constants for PCA9685\n",
        "PCA9685_ADDRESS = 0x40\n",
        "MODE1, MODE2 = 0x00, 0x01\n",
        "LED0_ON_L, LED0_OFF_L = 0x06, 0x08\n",
        "LED0_ON_H, LED0_OFF_H = 0x07, 0x09\n",
        "\n",
        "# I2C initialization\n",
        "bus = SMBus(7)\n",
        "\n",
        "def init_pca9685():\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE1, 0x00)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE2, 0x04)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "def set_pwm(channel, pulse_width):\n",
        "    on_time = 0\n",
        "    off_time = int(pulse_width)\n",
        "    base = LED0_ON_L + 4 * channel\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, base, on_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, base + 1, (on_time >> 8) & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, base + 2, off_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, base + 3, (off_time >> 8) & 0xFF)\n",
        "\n",
        "def angle_to_pulse_width(angle, min_us=500, max_us=2500):\n",
        "    return int((angle / 180.0) * (max_us - min_us) + min_us)\n",
        "\n",
        "# Calculate tilt from accelerometer data\n",
        "def calculate_tilt(accel_x, accel_y, accel_z):\n",
        "    tilt_x = math.atan2(accel_x, math.sqrt(accel_y**2 + accel_z**2))  # Pitch\n",
        "    tilt_y = math.atan2(accel_y, math.sqrt(accel_x**2 + accel_z**2))  # Roll\n",
        "    return math.degrees(tilt_x), math.degrees(tilt_y)  # Convert radians to degrees\n",
        "\n",
        "def control_diode(on=True):\n",
        "    pulse_width = int((0.1 / 100.0) * 4095)\n",
        "    set_pwm(13, pulse_width if on else 0)\n",
        "    set_pwm(9, pulse_width if on else 0)\n",
        "\n",
        "init_pca9685()\n",
        "\n",
        "# OAK-D Lite pipeline setup\n",
        "pipeline = dai.Pipeline()\n",
        "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
        "cam_rgb.setPreviewSize(640, 360)\n",
        "cam_rgb.setBoardSocket(dai.CameraBoardSocket.CAM_A)\n",
        "cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
        "cam_rgb.setInterleaved(False)\n",
        "cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
        "cam_rgb.setFps(30)\n",
        "\n",
        "mono_left = pipeline.create(dai.node.MonoCamera)\n",
        "mono_right = pipeline.create(dai.node.MonoCamera)\n",
        "mono_left.setBoardSocket(dai.CameraBoardSocket.CAM_B)\n",
        "mono_right.setBoardSocket(dai.CameraBoardSocket.CAM_C)\n",
        "mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "\n",
        "stereo = pipeline.create(dai.node.StereoDepth)\n",
        "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.DEFAULT)\n",
        "mono_left.out.link(stereo.left)\n",
        "mono_right.out.link(stereo.right)\n",
        "\n",
        "#IMU\n",
        "imu = pipeline.create(dai.node.IMU)\n",
        "imu.enableIMUSensor(dai.IMUSensor.GYROSCOPE_RAW, 100)\n",
        "imu.enableIMUSensor(dai.IMUSensor.ACCELEROMETER_RAW, 100)\n",
        "imu_queue = pipeline.create(dai.node.XLinkOut)\n",
        "imu_queue.setStreamName(\"imu\")\n",
        "imu.out.link(imu_queue.input)\n",
        "\n",
        "xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
        "xout_rgb.setStreamName(\"rgb\")\n",
        "cam_rgb.preview.link(xout_rgb.input)\n",
        "\n",
        "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
        "xout_depth.setStreamName(\"depth\")\n",
        "stereo.depth.link(xout_depth.input)\n",
        "\n",
        "frame_queue = queue.Queue(maxsize=1)\n",
        "\n",
        "trackers = {}\n",
        "\n",
        "\n",
        "def frame_capture_thread():\n",
        "    with dai.Device(pipeline) as device:\n",
        "        video_queue = device.getOutputQueue(name=\"rgb\", maxSize=1, blocking=False)\n",
        "        depth_queue = device.getOutputQueue(name=\"depth\", maxSize=1, blocking=False)\n",
        "        imu_queue = device.getOutputQueue(name=\"imu\", maxSize=1, blocking=False)\n",
        "        while True:\n",
        "            try:\n",
        "                frame = video_queue.get().getCvFrame()\n",
        "                depth = depth_queue.get().getCvFrame()\n",
        "                imu = imu_queue.get()\n",
        "                frame_queue.put((frame, depth, imu))\n",
        "            except KeyboardInterrupt:\n",
        "                break\n",
        "\n",
        "kf = KalmanFilter()\n",
        "# Global video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter('output.avi', fourcc, 30.0, (640, 360))\n",
        "\n",
        "\n",
        "def detection_and_control_thread():\n",
        "    alpha = 0.4  # Unused here but keep if you apply smoothing later\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if not frame_queue.empty():\n",
        "                frame, depth, imu = frame_queue.get()\n",
        "                results = model.track(frame, persist=True)\n",
        "                current_time = time.time()\n",
        "                found_mock_weed = False\n",
        "\n",
        "                imu_packets = imu.packets\n",
        "                tilt_x, tilt_y = 0, 0\n",
        "\n",
        "                for imu_packet in imu_packets:\n",
        "                    accel_data = imu_packet.acceleroMeter\n",
        "                    accel_x = accel_data.x\n",
        "                    accel_y = accel_data.y\n",
        "                    accel_z = accel_data.z\n",
        "                    tilt_x, tilt_y = calculate_tilt(accel_x, accel_y, accel_z)\n",
        "\n",
        "                for result in results:\n",
        "                    if result.boxes is not None and len(result.boxes) > 0:\n",
        "                        boxes = result.boxes.xyxy.cpu().numpy()\n",
        "                        track_ids = result.boxes.id.int().cpu().tolist() if result.boxes.id is not None else []\n",
        "                        labels = result.boxes.cls.cpu().numpy()\n",
        "\n",
        "                        for i, box in enumerate(boxes):\n",
        "                            label = labels[i]\n",
        "                            label_name = result.names[int(label)]\n",
        "                            track_id = track_ids[i] if i < len(track_ids) else None\n",
        "                            if track_id is None:\n",
        "                                continue\n",
        "\n",
        "                            x1, y1, x2, y2 = box\n",
        "                            center_x = (x1 + x2) / 2\n",
        "                            center_y = (y1 + y2) / 2\n",
        "\n",
        "\n",
        "                            if label_name == \"crop\":\n",
        "                                cv2.circle(frame, (int(center_x), int(center_y)), 40, (0, 255, 0), 2)\n",
        "\n",
        "                            elif label_name == \"mock_weed\":\n",
        "                                found_mock_weed = True\n",
        "\n",
        "                                # Kalman filter prediction and correction\n",
        "                                kf.predict(center_x, center_y)\n",
        "                                predicted_X, predicted_y = kf.predict_future(steps=5)\n",
        "\n",
        "\n",
        "                                depth_at_center = np.mean(depth)\n",
        "\n",
        "                                cv2.circle(frame, (int(center_x), int(center_y)), 20, (0, 0, 255), 2)\n",
        "                                cv2.circle(frame, (int(center_x), int(predicted_y)), 10, (0, 0, 255), 2)\n",
        "\n",
        "                                if predicted_y > frame.shape[0] // 2:\n",
        "                                    input_data = np.array([[center_x, predicted_y, depth_at_center, tilt_x, tilt_y]])\n",
        "                                    angle_x_long, angle_y_long = rf_model_long.predict(input_data).flatten()\n",
        "                                    angle_x_flat, angle_y_flat = rf_model_flat.predict(input_data).flatten()\n",
        "\n",
        "                                    set_pwm(15, angle_to_pulse_width(round(angle_x_long)))\n",
        "                                    set_pwm(14, angle_to_pulse_width(round(angle_y_long)))\n",
        "                                    set_pwm(11, angle_to_pulse_width(round(angle_x_flat)))\n",
        "                                    set_pwm(10, angle_to_pulse_width(round(angle_y_flat)))\n",
        "                                    control_diode(True)\n",
        "                                else:\n",
        "                                    control_diode(False)\n",
        "\n",
        "                if not found_mock_weed:\n",
        "                    control_diode(False)\n",
        "\n",
        "                cv2.imshow(\"Detection Output\", frame)\n",
        "                video_writer.write(frame)\n",
        "                if cv2.waitKey(1) == ord(\"q\"):\n",
        "                    break\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "\n",
        "# Start threads\n",
        "threading.Thread(target=frame_capture_thread, daemon=True).start()\n",
        "threading.Thread(target=detection_and_control_thread, daemon=True).start()\n",
        "\n",
        "# Keep main thread alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "it-o08R-Igni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}