{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNfi8RHJKUl9+HkhK8I1Nji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikalitt1/Clementine-Agriculture/blob/main/Population_based_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BGo8bM0pLWj",
        "outputId": "8fb516af-dbe9-4526-e534-b05714c89424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Model 8 - Finished Epoch 7\n",
            "Model 8 - Starting Epoch 8\n",
            "Model 8 - Epoch 8 - Batch 10/88 - Loss: 2.2841\n",
            "Model 8 - Epoch 8 - Batch 20/88 - Loss: 2.1546\n",
            "Model 8 - Epoch 8 - Batch 30/88 - Loss: 2.1479\n",
            "Model 8 - Epoch 8 - Batch 40/88 - Loss: 2.2014\n",
            "Model 8 - Epoch 8 - Batch 50/88 - Loss: 2.2855\n",
            "Model 8 - Epoch 8 - Batch 60/88 - Loss: 2.2039\n",
            "Model 8 - Epoch 8 - Batch 70/88 - Loss: 2.1026\n",
            "Model 8 - Epoch 8 - Batch 80/88 - Loss: 2.2541\n",
            "Model 8 - Finished Epoch 8\n",
            "Model 8 - Starting Epoch 9\n",
            "Model 8 - Epoch 9 - Batch 10/88 - Loss: 2.1639\n",
            "Model 8 - Epoch 9 - Batch 20/88 - Loss: 2.1646\n",
            "Model 8 - Epoch 9 - Batch 30/88 - Loss: 2.1701\n",
            "Model 8 - Epoch 9 - Batch 40/88 - Loss: 2.1713\n",
            "Model 8 - Epoch 9 - Batch 50/88 - Loss: 2.2274\n",
            "Model 8 - Epoch 9 - Batch 60/88 - Loss: 2.2529\n",
            "Model 8 - Epoch 9 - Batch 70/88 - Loss: 2.1996\n",
            "Model 8 - Epoch 9 - Batch 80/88 - Loss: 2.0628\n",
            "Model 8 - Finished Epoch 9\n",
            "Model 8 - Starting Epoch 10\n",
            "Model 8 - Epoch 10 - Batch 10/88 - Loss: 2.0903\n",
            "Model 8 - Epoch 10 - Batch 20/88 - Loss: 2.0903\n",
            "Model 8 - Epoch 10 - Batch 30/88 - Loss: 2.1942\n",
            "Model 8 - Epoch 10 - Batch 40/88 - Loss: 2.1573\n",
            "Model 8 - Epoch 10 - Batch 50/88 - Loss: 2.1356\n",
            "Model 8 - Epoch 10 - Batch 60/88 - Loss: 2.1735\n",
            "Model 8 - Epoch 10 - Batch 70/88 - Loss: 2.1735\n",
            "Model 8 - Epoch 10 - Batch 80/88 - Loss: 2.2027\n",
            "Model 8 - Finished Epoch 10\n",
            "Model 8 done training for 10 epochs in generation 5\n",
            "Model 9 - Starting Epoch 1\n",
            "Model 9 - Epoch 1 - Batch 10/88 - Loss: 2.2820\n",
            "Model 9 - Epoch 1 - Batch 20/88 - Loss: 2.2477\n",
            "Model 9 - Epoch 1 - Batch 30/88 - Loss: 2.2263\n",
            "Model 9 - Epoch 1 - Batch 40/88 - Loss: 2.3081\n",
            "Model 9 - Epoch 1 - Batch 50/88 - Loss: 2.4396\n",
            "Model 9 - Epoch 1 - Batch 60/88 - Loss: 2.3647\n",
            "Model 9 - Epoch 1 - Batch 70/88 - Loss: 2.3477\n",
            "Model 9 - Epoch 1 - Batch 80/88 - Loss: 2.3715\n",
            "Model 9 - Finished Epoch 1\n",
            "Model 9 - Starting Epoch 2\n",
            "Model 9 - Epoch 2 - Batch 10/88 - Loss: 2.2653\n",
            "Model 9 - Epoch 2 - Batch 20/88 - Loss: 2.3157\n",
            "Model 9 - Epoch 2 - Batch 30/88 - Loss: 2.3789\n",
            "Model 9 - Epoch 2 - Batch 40/88 - Loss: 2.3890\n",
            "Model 9 - Epoch 2 - Batch 50/88 - Loss: 2.3197\n",
            "Model 9 - Epoch 2 - Batch 60/88 - Loss: 2.2157\n",
            "Model 9 - Epoch 2 - Batch 70/88 - Loss: 2.3752\n",
            "Model 9 - Epoch 2 - Batch 80/88 - Loss: 2.4427\n",
            "Model 9 - Finished Epoch 2\n",
            "Model 9 - Starting Epoch 3\n",
            "Model 9 - Epoch 3 - Batch 10/88 - Loss: 2.2869\n",
            "Model 9 - Epoch 3 - Batch 20/88 - Loss: 2.3290\n",
            "Model 9 - Epoch 3 - Batch 30/88 - Loss: 2.2878\n",
            "Model 9 - Epoch 3 - Batch 40/88 - Loss: 2.3650\n",
            "Model 9 - Epoch 3 - Batch 50/88 - Loss: 2.2724\n",
            "Model 9 - Epoch 3 - Batch 60/88 - Loss: 2.2993\n",
            "Model 9 - Epoch 3 - Batch 70/88 - Loss: 2.2994\n",
            "Model 9 - Epoch 3 - Batch 80/88 - Loss: 2.3484\n",
            "Model 9 - Finished Epoch 3\n",
            "Model 9 - Starting Epoch 4\n",
            "Model 9 - Epoch 4 - Batch 10/88 - Loss: 2.1687\n",
            "Model 9 - Epoch 4 - Batch 20/88 - Loss: 2.3399\n",
            "Model 9 - Epoch 4 - Batch 30/88 - Loss: 2.2432\n",
            "Model 9 - Epoch 4 - Batch 40/88 - Loss: 2.3957\n",
            "Model 9 - Epoch 4 - Batch 50/88 - Loss: 2.4159\n",
            "Model 9 - Epoch 4 - Batch 60/88 - Loss: 2.3287\n",
            "Model 9 - Epoch 4 - Batch 70/88 - Loss: 2.2447\n",
            "Model 9 - Epoch 4 - Batch 80/88 - Loss: 2.3529\n",
            "Model 9 - Finished Epoch 4\n",
            "Model 9 - Starting Epoch 5\n",
            "Model 9 - Epoch 5 - Batch 10/88 - Loss: 2.3192\n",
            "Model 9 - Epoch 5 - Batch 20/88 - Loss: 2.2584\n",
            "Model 9 - Epoch 5 - Batch 30/88 - Loss: 2.1682\n",
            "Model 9 - Epoch 5 - Batch 40/88 - Loss: 2.2677\n",
            "Model 9 - Epoch 5 - Batch 50/88 - Loss: 2.2486\n",
            "Model 9 - Epoch 5 - Batch 60/88 - Loss: 2.2255\n",
            "Model 9 - Epoch 5 - Batch 70/88 - Loss: 2.3175\n",
            "Model 9 - Epoch 5 - Batch 80/88 - Loss: 2.2100\n",
            "Model 9 - Finished Epoch 5\n",
            "Model 9 - Starting Epoch 6\n",
            "Model 9 - Epoch 6 - Batch 10/88 - Loss: 2.2715\n",
            "Model 9 - Epoch 6 - Batch 20/88 - Loss: 2.2393\n",
            "Model 9 - Epoch 6 - Batch 30/88 - Loss: 2.1828\n",
            "Model 9 - Epoch 6 - Batch 40/88 - Loss: 2.1248\n",
            "Model 9 - Epoch 6 - Batch 50/88 - Loss: 2.2942\n",
            "Model 9 - Epoch 6 - Batch 60/88 - Loss: 2.3232\n",
            "Model 9 - Epoch 6 - Batch 70/88 - Loss: 2.2928\n",
            "Model 9 - Epoch 6 - Batch 80/88 - Loss: 2.2461\n",
            "Model 9 - Finished Epoch 6\n",
            "Model 9 - Starting Epoch 7\n",
            "Model 9 - Epoch 7 - Batch 10/88 - Loss: 2.2976\n",
            "Model 9 - Epoch 7 - Batch 20/88 - Loss: 2.1809\n",
            "Model 9 - Epoch 7 - Batch 30/88 - Loss: 2.1134\n",
            "Model 9 - Epoch 7 - Batch 40/88 - Loss: 2.2341\n",
            "Model 9 - Epoch 7 - Batch 50/88 - Loss: 2.2962\n",
            "Model 9 - Epoch 7 - Batch 60/88 - Loss: 2.4059\n",
            "Model 9 - Epoch 7 - Batch 70/88 - Loss: 2.2515\n",
            "Model 9 - Epoch 7 - Batch 80/88 - Loss: 2.3033\n",
            "Model 9 - Finished Epoch 7\n",
            "Model 9 - Starting Epoch 8\n",
            "Model 9 - Epoch 8 - Batch 10/88 - Loss: 2.2945\n",
            "Model 9 - Epoch 8 - Batch 20/88 - Loss: 2.2063\n",
            "Model 9 - Epoch 8 - Batch 30/88 - Loss: 2.1787\n",
            "Model 9 - Epoch 8 - Batch 40/88 - Loss: 2.2342\n",
            "Model 9 - Epoch 8 - Batch 50/88 - Loss: 2.2876\n",
            "Model 9 - Epoch 8 - Batch 60/88 - Loss: 2.2086\n",
            "Model 9 - Epoch 8 - Batch 70/88 - Loss: 2.1351\n",
            "Model 9 - Epoch 8 - Batch 80/88 - Loss: 2.3997\n",
            "Model 9 - Finished Epoch 8\n",
            "Model 9 - Starting Epoch 9\n",
            "Model 9 - Epoch 9 - Batch 10/88 - Loss: 2.2217\n",
            "Model 9 - Epoch 9 - Batch 20/88 - Loss: 2.2715\n",
            "Model 9 - Epoch 9 - Batch 30/88 - Loss: 2.3203\n",
            "Model 9 - Epoch 9 - Batch 40/88 - Loss: 2.1990\n",
            "Model 9 - Epoch 9 - Batch 50/88 - Loss: 2.1812\n",
            "Model 9 - Epoch 9 - Batch 60/88 - Loss: 2.3128\n",
            "Model 9 - Epoch 9 - Batch 70/88 - Loss: 2.2016\n",
            "Model 9 - Epoch 9 - Batch 80/88 - Loss: 2.2112\n",
            "Model 9 - Finished Epoch 9\n",
            "Model 9 - Starting Epoch 10\n",
            "Model 9 - Epoch 10 - Batch 10/88 - Loss: 2.0554\n",
            "Model 9 - Epoch 10 - Batch 20/88 - Loss: 2.1659\n",
            "Model 9 - Epoch 10 - Batch 30/88 - Loss: 2.2572\n",
            "Model 9 - Epoch 10 - Batch 40/88 - Loss: 2.2901\n",
            "Model 9 - Epoch 10 - Batch 50/88 - Loss: 2.3091\n",
            "Model 9 - Epoch 10 - Batch 60/88 - Loss: 2.2462\n",
            "Model 9 - Epoch 10 - Batch 70/88 - Loss: 2.1859\n",
            "Model 9 - Epoch 10 - Batch 80/88 - Loss: 2.1986\n",
            "Model 9 - Finished Epoch 10\n",
            "Model 9 done training for 10 epochs in generation 5\n",
            "Model 10 - Starting Epoch 1\n",
            "Model 10 - Epoch 1 - Batch 10/88 - Loss: 2.3097\n",
            "Model 10 - Epoch 1 - Batch 20/88 - Loss: 2.2608\n",
            "Model 10 - Epoch 1 - Batch 30/88 - Loss: 2.3206\n",
            "Model 10 - Epoch 1 - Batch 40/88 - Loss: 2.2663\n",
            "Model 10 - Epoch 1 - Batch 50/88 - Loss: 2.3068\n",
            "Model 10 - Epoch 1 - Batch 60/88 - Loss: 2.2977\n",
            "Model 10 - Epoch 1 - Batch 70/88 - Loss: 2.3135\n",
            "Model 10 - Epoch 1 - Batch 80/88 - Loss: 2.2080\n",
            "Model 10 - Finished Epoch 1\n",
            "Model 10 - Starting Epoch 2\n",
            "Model 10 - Epoch 2 - Batch 10/88 - Loss: 2.2105\n",
            "Model 10 - Epoch 2 - Batch 20/88 - Loss: 2.2690\n",
            "Model 10 - Epoch 2 - Batch 30/88 - Loss: 2.3223\n",
            "Model 10 - Epoch 2 - Batch 40/88 - Loss: 2.1196\n",
            "Model 10 - Epoch 2 - Batch 50/88 - Loss: 2.2575\n",
            "Model 10 - Epoch 2 - Batch 60/88 - Loss: 2.3346\n",
            "Model 10 - Epoch 2 - Batch 70/88 - Loss: 2.2169\n",
            "Model 10 - Epoch 2 - Batch 80/88 - Loss: 2.3111\n",
            "Model 10 - Finished Epoch 2\n",
            "Model 10 - Starting Epoch 3\n",
            "Model 10 - Epoch 3 - Batch 10/88 - Loss: 2.1809\n",
            "Model 10 - Epoch 3 - Batch 20/88 - Loss: 2.2079\n",
            "Model 10 - Epoch 3 - Batch 30/88 - Loss: 2.1135\n",
            "Model 10 - Epoch 3 - Batch 40/88 - Loss: 2.2714\n",
            "Model 10 - Epoch 3 - Batch 50/88 - Loss: 2.2294\n",
            "Model 10 - Epoch 3 - Batch 60/88 - Loss: 2.3578\n",
            "Model 10 - Epoch 3 - Batch 70/88 - Loss: 2.3123\n",
            "Model 10 - Epoch 3 - Batch 80/88 - Loss: 2.2477\n",
            "Model 10 - Finished Epoch 3\n",
            "Model 10 - Starting Epoch 4\n",
            "Model 10 - Epoch 4 - Batch 10/88 - Loss: 2.0657\n",
            "Model 10 - Epoch 4 - Batch 20/88 - Loss: 2.1858\n",
            "Model 10 - Epoch 4 - Batch 30/88 - Loss: 2.3128\n",
            "Model 10 - Epoch 4 - Batch 40/88 - Loss: 2.2347\n",
            "Model 10 - Epoch 4 - Batch 50/88 - Loss: 2.2401\n",
            "Model 10 - Epoch 4 - Batch 60/88 - Loss: 2.2536\n",
            "Model 10 - Epoch 4 - Batch 70/88 - Loss: 2.2512\n",
            "Model 10 - Epoch 4 - Batch 80/88 - Loss: 2.3348\n",
            "Model 10 - Finished Epoch 4\n",
            "Model 10 - Starting Epoch 5\n",
            "Model 10 - Epoch 5 - Batch 10/88 - Loss: 2.1420\n",
            "Model 10 - Epoch 5 - Batch 20/88 - Loss: 2.2303\n",
            "Model 10 - Epoch 5 - Batch 30/88 - Loss: 2.1908\n",
            "Model 10 - Epoch 5 - Batch 40/88 - Loss: 2.2271\n",
            "Model 10 - Epoch 5 - Batch 50/88 - Loss: 2.1339\n",
            "Model 10 - Epoch 5 - Batch 60/88 - Loss: 2.1488\n",
            "Model 10 - Epoch 5 - Batch 70/88 - Loss: 2.2317\n",
            "Model 10 - Epoch 5 - Batch 80/88 - Loss: 2.3057\n",
            "Model 10 - Finished Epoch 5\n",
            "Model 10 - Starting Epoch 6\n",
            "Model 10 - Epoch 6 - Batch 10/88 - Loss: 2.1611\n",
            "Model 10 - Epoch 6 - Batch 20/88 - Loss: 2.2355\n",
            "Model 10 - Epoch 6 - Batch 30/88 - Loss: 2.1326\n",
            "Model 10 - Epoch 6 - Batch 40/88 - Loss: 2.2538\n",
            "Model 10 - Epoch 6 - Batch 50/88 - Loss: 2.1458\n",
            "Model 10 - Epoch 6 - Batch 60/88 - Loss: 2.1627\n",
            "Model 10 - Epoch 6 - Batch 70/88 - Loss: 2.2107\n",
            "Model 10 - Epoch 6 - Batch 80/88 - Loss: 2.3395\n",
            "Model 10 - Finished Epoch 6\n",
            "Model 10 - Starting Epoch 7\n",
            "Model 10 - Epoch 7 - Batch 10/88 - Loss: 2.1713\n",
            "Model 10 - Epoch 7 - Batch 20/88 - Loss: 2.1546\n",
            "Model 10 - Epoch 7 - Batch 30/88 - Loss: 2.2465\n",
            "Model 10 - Epoch 7 - Batch 40/88 - Loss: 2.1403\n",
            "Model 10 - Epoch 7 - Batch 50/88 - Loss: 2.2831\n",
            "Model 10 - Epoch 7 - Batch 60/88 - Loss: 2.0800\n",
            "Model 10 - Epoch 7 - Batch 70/88 - Loss: 2.2655\n",
            "Model 10 - Epoch 7 - Batch 80/88 - Loss: 2.1937\n",
            "Model 10 - Finished Epoch 7\n",
            "Model 10 - Starting Epoch 8\n",
            "Model 10 - Epoch 8 - Batch 10/88 - Loss: 2.1590\n",
            "Model 10 - Epoch 8 - Batch 20/88 - Loss: 2.0571\n",
            "Model 10 - Epoch 8 - Batch 30/88 - Loss: 2.1577\n",
            "Model 10 - Epoch 8 - Batch 40/88 - Loss: 2.0808\n",
            "Model 10 - Epoch 8 - Batch 50/88 - Loss: 2.1667\n",
            "Model 10 - Epoch 8 - Batch 60/88 - Loss: 2.2319\n",
            "Model 10 - Epoch 8 - Batch 70/88 - Loss: 2.1442\n",
            "Model 10 - Epoch 8 - Batch 80/88 - Loss: 2.1252\n",
            "Model 10 - Finished Epoch 8\n",
            "Model 10 - Starting Epoch 9\n",
            "Model 10 - Epoch 9 - Batch 10/88 - Loss: 2.0634\n",
            "Model 10 - Epoch 9 - Batch 20/88 - Loss: 2.0337\n",
            "Model 10 - Epoch 9 - Batch 30/88 - Loss: 2.1684\n",
            "Model 10 - Epoch 9 - Batch 40/88 - Loss: 2.2518\n",
            "Model 10 - Epoch 9 - Batch 50/88 - Loss: 2.1805\n",
            "Model 10 - Epoch 9 - Batch 60/88 - Loss: 2.2241\n",
            "Model 10 - Epoch 9 - Batch 70/88 - Loss: 2.2049\n",
            "Model 10 - Epoch 9 - Batch 80/88 - Loss: 2.1671\n",
            "Model 10 - Finished Epoch 9\n",
            "Model 10 - Starting Epoch 10\n",
            "Model 10 - Epoch 10 - Batch 10/88 - Loss: 2.1545\n",
            "Model 10 - Epoch 10 - Batch 20/88 - Loss: 2.2586\n",
            "Model 10 - Epoch 10 - Batch 30/88 - Loss: 2.1876\n",
            "Model 10 - Epoch 10 - Batch 40/88 - Loss: 2.1431\n",
            "Model 10 - Epoch 10 - Batch 50/88 - Loss: 2.0218\n",
            "Model 10 - Epoch 10 - Batch 60/88 - Loss: 2.2137\n",
            "Model 10 - Epoch 10 - Batch 70/88 - Loss: 2.0297\n",
            "Model 10 - Epoch 10 - Batch 80/88 - Loss: 2.2811\n",
            "Model 10 - Finished Epoch 10\n",
            "Model 10 done training for 10 epochs in generation 5\n",
            "Model 11 - Starting Epoch 1\n",
            "Model 11 - Epoch 1 - Batch 10/88 - Loss: 1.9864\n",
            "Model 11 - Epoch 1 - Batch 20/88 - Loss: 2.1099\n",
            "Model 11 - Epoch 1 - Batch 30/88 - Loss: 2.1283\n",
            "Model 11 - Epoch 1 - Batch 40/88 - Loss: 2.0594\n",
            "Model 11 - Epoch 1 - Batch 50/88 - Loss: 2.2485\n",
            "Model 11 - Epoch 1 - Batch 60/88 - Loss: 2.2266\n",
            "Model 11 - Epoch 1 - Batch 70/88 - Loss: 2.1142\n",
            "Model 11 - Epoch 1 - Batch 80/88 - Loss: 2.2408\n",
            "Model 11 - Finished Epoch 1\n",
            "Model 11 - Starting Epoch 2\n",
            "Model 11 - Epoch 2 - Batch 10/88 - Loss: 2.1418\n",
            "Model 11 - Epoch 2 - Batch 20/88 - Loss: 2.0662\n",
            "Model 11 - Epoch 2 - Batch 30/88 - Loss: 2.1481\n",
            "Model 11 - Epoch 2 - Batch 40/88 - Loss: 2.1228\n",
            "Model 11 - Epoch 2 - Batch 50/88 - Loss: 2.0733\n",
            "Model 11 - Epoch 2 - Batch 60/88 - Loss: 2.1370\n",
            "Model 11 - Epoch 2 - Batch 70/88 - Loss: 2.0511\n",
            "Model 11 - Epoch 2 - Batch 80/88 - Loss: 2.1278\n",
            "Model 11 - Finished Epoch 2\n",
            "Model 11 - Starting Epoch 3\n",
            "Model 11 - Epoch 3 - Batch 10/88 - Loss: 2.1463\n",
            "Model 11 - Epoch 3 - Batch 20/88 - Loss: 2.0232\n",
            "Model 11 - Epoch 3 - Batch 30/88 - Loss: 2.0820\n",
            "Model 11 - Epoch 3 - Batch 40/88 - Loss: 2.0527\n",
            "Model 11 - Epoch 3 - Batch 50/88 - Loss: 2.0540\n",
            "Model 11 - Epoch 3 - Batch 60/88 - Loss: 2.0669\n",
            "Model 11 - Epoch 3 - Batch 70/88 - Loss: 2.0414\n",
            "Model 11 - Epoch 3 - Batch 80/88 - Loss: 2.1791\n",
            "Model 11 - Finished Epoch 3\n",
            "Model 11 - Starting Epoch 4\n",
            "Model 11 - Epoch 4 - Batch 10/88 - Loss: 1.9240\n",
            "Model 11 - Epoch 4 - Batch 20/88 - Loss: 2.0048\n",
            "Model 11 - Epoch 4 - Batch 30/88 - Loss: 2.0605\n",
            "Model 11 - Epoch 4 - Batch 40/88 - Loss: 2.0767\n",
            "Model 11 - Epoch 4 - Batch 50/88 - Loss: 2.1857\n",
            "Model 11 - Epoch 4 - Batch 60/88 - Loss: 2.1833\n",
            "Model 11 - Epoch 4 - Batch 70/88 - Loss: 2.2125\n",
            "Model 11 - Epoch 4 - Batch 80/88 - Loss: 2.1297\n",
            "Model 11 - Finished Epoch 4\n",
            "Model 11 - Starting Epoch 5\n",
            "Model 11 - Epoch 5 - Batch 10/88 - Loss: 1.9931\n",
            "Model 11 - Epoch 5 - Batch 20/88 - Loss: 2.0498\n",
            "Model 11 - Epoch 5 - Batch 30/88 - Loss: 2.0356\n",
            "Model 11 - Epoch 5 - Batch 40/88 - Loss: 2.0615\n",
            "Model 11 - Epoch 5 - Batch 50/88 - Loss: 2.2102\n",
            "Model 11 - Epoch 5 - Batch 60/88 - Loss: 1.9305\n",
            "Model 11 - Epoch 5 - Batch 70/88 - Loss: 2.1521\n",
            "Model 11 - Epoch 5 - Batch 80/88 - Loss: 2.0509\n",
            "Model 11 - Finished Epoch 5\n",
            "Model 11 - Starting Epoch 6\n",
            "Model 11 - Epoch 6 - Batch 10/88 - Loss: 1.9309\n",
            "Model 11 - Epoch 6 - Batch 20/88 - Loss: 2.0401\n",
            "Model 11 - Epoch 6 - Batch 30/88 - Loss: 2.0807\n",
            "Model 11 - Epoch 6 - Batch 40/88 - Loss: 1.9890\n",
            "Model 11 - Epoch 6 - Batch 50/88 - Loss: 2.0184\n",
            "Model 11 - Epoch 6 - Batch 60/88 - Loss: 2.0789\n",
            "Model 11 - Epoch 6 - Batch 70/88 - Loss: 2.1398\n",
            "Model 11 - Epoch 6 - Batch 80/88 - Loss: 1.9574\n",
            "Model 11 - Finished Epoch 6\n",
            "Model 11 - Starting Epoch 7\n",
            "Model 11 - Epoch 7 - Batch 10/88 - Loss: 1.9610\n",
            "Model 11 - Epoch 7 - Batch 20/88 - Loss: 1.9640\n",
            "Model 11 - Epoch 7 - Batch 30/88 - Loss: 1.9562\n",
            "Model 11 - Epoch 7 - Batch 40/88 - Loss: 2.0530\n",
            "Model 11 - Epoch 7 - Batch 50/88 - Loss: 2.1300\n",
            "Model 11 - Epoch 7 - Batch 60/88 - Loss: 2.1019\n",
            "Model 11 - Epoch 7 - Batch 70/88 - Loss: 2.0417\n",
            "Model 11 - Epoch 7 - Batch 80/88 - Loss: 1.9447\n",
            "Model 11 - Finished Epoch 7\n",
            "Model 11 - Starting Epoch 8\n",
            "Model 11 - Epoch 8 - Batch 10/88 - Loss: 1.9741\n",
            "Model 11 - Epoch 8 - Batch 20/88 - Loss: 1.9735\n",
            "Model 11 - Epoch 8 - Batch 30/88 - Loss: 2.0307\n",
            "Model 11 - Epoch 8 - Batch 40/88 - Loss: 2.0844\n",
            "Model 11 - Epoch 8 - Batch 50/88 - Loss: 2.0884\n",
            "Model 11 - Epoch 8 - Batch 60/88 - Loss: 1.9848\n",
            "Model 11 - Epoch 8 - Batch 70/88 - Loss: 2.0242\n",
            "Model 11 - Epoch 8 - Batch 80/88 - Loss: 1.9476\n",
            "Model 11 - Finished Epoch 8\n",
            "Model 11 - Starting Epoch 9\n",
            "Model 11 - Epoch 9 - Batch 10/88 - Loss: 1.9654\n",
            "Model 11 - Epoch 9 - Batch 20/88 - Loss: 1.9482\n",
            "Model 11 - Epoch 9 - Batch 30/88 - Loss: 1.9133\n",
            "Model 11 - Epoch 9 - Batch 40/88 - Loss: 1.9314\n",
            "Model 11 - Epoch 9 - Batch 50/88 - Loss: 1.9337\n",
            "Model 11 - Epoch 9 - Batch 60/88 - Loss: 1.9309\n",
            "Model 11 - Epoch 9 - Batch 70/88 - Loss: 2.0847\n",
            "Model 11 - Epoch 9 - Batch 80/88 - Loss: 1.9785\n",
            "Model 11 - Finished Epoch 9\n",
            "Model 11 - Starting Epoch 10\n",
            "Model 11 - Epoch 10 - Batch 10/88 - Loss: 2.0471\n",
            "Model 11 - Epoch 10 - Batch 20/88 - Loss: 1.9235\n",
            "Model 11 - Epoch 10 - Batch 30/88 - Loss: 2.0365\n",
            "Model 11 - Epoch 10 - Batch 40/88 - Loss: 1.8684\n",
            "Model 11 - Epoch 10 - Batch 50/88 - Loss: 1.9371\n",
            "Model 11 - Epoch 10 - Batch 60/88 - Loss: 1.9602\n",
            "Model 11 - Epoch 10 - Batch 70/88 - Loss: 2.0244\n",
            "Model 11 - Epoch 10 - Batch 80/88 - Loss: 1.9739\n",
            "Model 11 - Finished Epoch 10\n",
            "Model 11 done training for 10 epochs in generation 5\n",
            "Model 12 - Starting Epoch 1\n",
            "Model 12 - Epoch 1 - Batch 10/88 - Loss: 2.3181\n",
            "Model 12 - Epoch 1 - Batch 20/88 - Loss: 2.2838\n",
            "Model 12 - Epoch 1 - Batch 30/88 - Loss: 2.3233\n",
            "Model 12 - Epoch 1 - Batch 40/88 - Loss: 2.3782\n",
            "Model 12 - Epoch 1 - Batch 50/88 - Loss: 2.2906\n",
            "Model 12 - Epoch 1 - Batch 60/88 - Loss: 2.3996\n",
            "Model 12 - Epoch 1 - Batch 70/88 - Loss: 2.3432\n",
            "Model 12 - Epoch 1 - Batch 80/88 - Loss: 2.5057\n",
            "Model 12 - Finished Epoch 1\n",
            "Model 12 - Starting Epoch 2\n",
            "Model 12 - Epoch 2 - Batch 10/88 - Loss: 2.2187\n",
            "Model 12 - Epoch 2 - Batch 20/88 - Loss: 2.2395\n",
            "Model 12 - Epoch 2 - Batch 30/88 - Loss: 2.3197\n",
            "Model 12 - Epoch 2 - Batch 40/88 - Loss: 2.2424\n",
            "Model 12 - Epoch 2 - Batch 50/88 - Loss: 2.3174\n",
            "Model 12 - Epoch 2 - Batch 60/88 - Loss: 2.3791\n",
            "Model 12 - Epoch 2 - Batch 70/88 - Loss: 2.3067\n",
            "Model 12 - Epoch 2 - Batch 80/88 - Loss: 2.3415\n",
            "Model 12 - Finished Epoch 2\n",
            "Model 12 - Starting Epoch 3\n",
            "Model 12 - Epoch 3 - Batch 10/88 - Loss: 2.2698\n",
            "Model 12 - Epoch 3 - Batch 20/88 - Loss: 2.2086\n",
            "Model 12 - Epoch 3 - Batch 30/88 - Loss: 2.3054\n",
            "Model 12 - Epoch 3 - Batch 40/88 - Loss: 2.2941\n",
            "Model 12 - Epoch 3 - Batch 50/88 - Loss: 2.4013\n",
            "Model 12 - Epoch 3 - Batch 60/88 - Loss: 2.3124\n",
            "Model 12 - Epoch 3 - Batch 70/88 - Loss: 2.4221\n",
            "Model 12 - Epoch 3 - Batch 80/88 - Loss: 2.3216\n",
            "Model 12 - Finished Epoch 3\n",
            "Model 12 - Starting Epoch 4\n",
            "Model 12 - Epoch 4 - Batch 10/88 - Loss: 2.2815\n",
            "Model 12 - Epoch 4 - Batch 20/88 - Loss: 2.3232\n",
            "Model 12 - Epoch 4 - Batch 30/88 - Loss: 2.3501\n",
            "Model 12 - Epoch 4 - Batch 40/88 - Loss: 2.1427\n",
            "Model 12 - Epoch 4 - Batch 50/88 - Loss: 2.2767\n",
            "Model 12 - Epoch 4 - Batch 60/88 - Loss: 2.2881\n",
            "Model 12 - Epoch 4 - Batch 70/88 - Loss: 2.4013\n",
            "Model 12 - Epoch 4 - Batch 80/88 - Loss: 2.3033\n",
            "Model 12 - Finished Epoch 4\n",
            "Model 12 - Starting Epoch 5\n",
            "Model 12 - Epoch 5 - Batch 10/88 - Loss: 2.2181\n",
            "Model 12 - Epoch 5 - Batch 20/88 - Loss: 2.2415\n",
            "Model 12 - Epoch 5 - Batch 30/88 - Loss: 2.3086\n",
            "Model 12 - Epoch 5 - Batch 40/88 - Loss: 2.2804\n",
            "Model 12 - Epoch 5 - Batch 50/88 - Loss: 2.3164\n",
            "Model 12 - Epoch 5 - Batch 60/88 - Loss: 2.3085\n",
            "Model 12 - Epoch 5 - Batch 70/88 - Loss: 2.2410\n",
            "Model 12 - Epoch 5 - Batch 80/88 - Loss: 2.3152\n",
            "Model 12 - Finished Epoch 5\n",
            "Model 12 - Starting Epoch 6\n",
            "Model 12 - Epoch 6 - Batch 10/88 - Loss: 2.1944\n",
            "Model 12 - Epoch 6 - Batch 20/88 - Loss: 2.1990\n",
            "Model 12 - Epoch 6 - Batch 30/88 - Loss: 2.2343\n",
            "Model 12 - Epoch 6 - Batch 40/88 - Loss: 2.2017\n",
            "Model 12 - Epoch 6 - Batch 50/88 - Loss: 2.2394\n",
            "Model 12 - Epoch 6 - Batch 60/88 - Loss: 2.3467\n",
            "Model 12 - Epoch 6 - Batch 70/88 - Loss: 2.2813\n",
            "Model 12 - Epoch 6 - Batch 80/88 - Loss: 2.2765\n",
            "Model 12 - Finished Epoch 6\n",
            "Model 12 - Starting Epoch 7\n",
            "Model 12 - Epoch 7 - Batch 10/88 - Loss: 2.1930\n",
            "Model 12 - Epoch 7 - Batch 20/88 - Loss: 2.2007\n",
            "Model 12 - Epoch 7 - Batch 30/88 - Loss: 2.1230\n",
            "Model 12 - Epoch 7 - Batch 40/88 - Loss: 2.3102\n",
            "Model 12 - Epoch 7 - Batch 50/88 - Loss: 2.1111\n",
            "Model 12 - Epoch 7 - Batch 60/88 - Loss: 2.2634\n",
            "Model 12 - Epoch 7 - Batch 70/88 - Loss: 2.2119\n",
            "Model 12 - Epoch 7 - Batch 80/88 - Loss: 2.2713\n",
            "Model 12 - Finished Epoch 7\n",
            "Model 12 - Starting Epoch 8\n",
            "Model 12 - Epoch 8 - Batch 10/88 - Loss: 2.2370\n",
            "Model 12 - Epoch 8 - Batch 20/88 - Loss: 2.1705\n",
            "Model 12 - Epoch 8 - Batch 30/88 - Loss: 2.1940\n",
            "Model 12 - Epoch 8 - Batch 40/88 - Loss: 2.1976\n",
            "Model 12 - Epoch 8 - Batch 50/88 - Loss: 2.1627\n",
            "Model 12 - Epoch 8 - Batch 60/88 - Loss: 2.2874\n",
            "Model 12 - Epoch 8 - Batch 70/88 - Loss: 2.1912\n",
            "Model 12 - Epoch 8 - Batch 80/88 - Loss: 2.3206\n",
            "Model 12 - Finished Epoch 8\n",
            "Model 12 - Starting Epoch 9\n",
            "Model 12 - Epoch 9 - Batch 10/88 - Loss: 2.1210\n",
            "Model 12 - Epoch 9 - Batch 20/88 - Loss: 2.1727\n",
            "Model 12 - Epoch 9 - Batch 30/88 - Loss: 2.2057\n",
            "Model 12 - Epoch 9 - Batch 40/88 - Loss: 2.1344\n",
            "Model 12 - Epoch 9 - Batch 50/88 - Loss: 2.3264\n",
            "Model 12 - Epoch 9 - Batch 60/88 - Loss: 2.2728\n",
            "Model 12 - Epoch 9 - Batch 70/88 - Loss: 2.2131\n",
            "Model 12 - Epoch 9 - Batch 80/88 - Loss: 2.2562\n",
            "Model 12 - Finished Epoch 9\n",
            "Model 12 - Starting Epoch 10\n",
            "Model 12 - Epoch 10 - Batch 10/88 - Loss: 2.1718\n",
            "Model 12 - Epoch 10 - Batch 20/88 - Loss: 2.2668\n",
            "Model 12 - Epoch 10 - Batch 30/88 - Loss: 2.2454\n",
            "Model 12 - Epoch 10 - Batch 40/88 - Loss: 2.1500\n",
            "Model 12 - Epoch 10 - Batch 50/88 - Loss: 2.2176\n",
            "Model 12 - Epoch 10 - Batch 60/88 - Loss: 2.2528\n",
            "Model 12 - Epoch 10 - Batch 70/88 - Loss: 2.0958\n",
            "Model 12 - Epoch 10 - Batch 80/88 - Loss: 2.2845\n",
            "Model 12 - Finished Epoch 10\n",
            "Model 12 done training for 10 epochs in generation 5\n",
            "Model 13 - Starting Epoch 1\n",
            "Model 13 - Epoch 1 - Batch 10/88 - Loss: 2.3843\n",
            "Model 13 - Epoch 1 - Batch 20/88 - Loss: 2.2716\n",
            "Model 13 - Epoch 1 - Batch 30/88 - Loss: 2.4418\n",
            "Model 13 - Epoch 1 - Batch 40/88 - Loss: 2.2910\n",
            "Model 13 - Epoch 1 - Batch 50/88 - Loss: 2.3094\n",
            "Model 13 - Epoch 1 - Batch 60/88 - Loss: 2.4098\n",
            "Model 13 - Epoch 1 - Batch 70/88 - Loss: 2.3580\n",
            "Model 13 - Epoch 1 - Batch 80/88 - Loss: 2.3171\n",
            "Model 13 - Finished Epoch 1\n",
            "Model 13 - Starting Epoch 2\n",
            "Model 13 - Epoch 2 - Batch 10/88 - Loss: 2.2962\n",
            "Model 13 - Epoch 2 - Batch 20/88 - Loss: 2.3445\n",
            "Model 13 - Epoch 2 - Batch 30/88 - Loss: 2.2933\n",
            "Model 13 - Epoch 2 - Batch 40/88 - Loss: 2.2303\n",
            "Model 13 - Epoch 2 - Batch 50/88 - Loss: 2.2895\n",
            "Model 13 - Epoch 2 - Batch 60/88 - Loss: 2.3072\n",
            "Model 13 - Epoch 2 - Batch 70/88 - Loss: 2.2790\n",
            "Model 13 - Epoch 2 - Batch 80/88 - Loss: 2.3332\n",
            "Model 13 - Finished Epoch 2\n",
            "Model 13 - Starting Epoch 3\n",
            "Model 13 - Epoch 3 - Batch 10/88 - Loss: 2.2615\n",
            "Model 13 - Epoch 3 - Batch 20/88 - Loss: 2.2595\n",
            "Model 13 - Epoch 3 - Batch 30/88 - Loss: 2.2303\n",
            "Model 13 - Epoch 3 - Batch 40/88 - Loss: 2.3671\n",
            "Model 13 - Epoch 3 - Batch 50/88 - Loss: 2.3228\n",
            "Model 13 - Epoch 3 - Batch 60/88 - Loss: 2.3343\n",
            "Model 13 - Epoch 3 - Batch 70/88 - Loss: 2.2424\n",
            "Model 13 - Epoch 3 - Batch 80/88 - Loss: 2.3645\n",
            "Model 13 - Finished Epoch 3\n",
            "Model 13 - Starting Epoch 4\n",
            "Model 13 - Epoch 4 - Batch 10/88 - Loss: 2.3405\n",
            "Model 13 - Epoch 4 - Batch 20/88 - Loss: 2.2628\n",
            "Model 13 - Epoch 4 - Batch 30/88 - Loss: 2.3184\n",
            "Model 13 - Epoch 4 - Batch 40/88 - Loss: 2.1098\n",
            "Model 13 - Epoch 4 - Batch 50/88 - Loss: 2.3276\n",
            "Model 13 - Epoch 4 - Batch 60/88 - Loss: 2.4276\n",
            "Model 13 - Epoch 4 - Batch 70/88 - Loss: 2.2964\n",
            "Model 13 - Epoch 4 - Batch 80/88 - Loss: 2.2030\n",
            "Model 13 - Finished Epoch 4\n",
            "Model 13 - Starting Epoch 5\n",
            "Model 13 - Epoch 5 - Batch 10/88 - Loss: 2.2183\n",
            "Model 13 - Epoch 5 - Batch 20/88 - Loss: 2.1996\n",
            "Model 13 - Epoch 5 - Batch 30/88 - Loss: 2.2531\n",
            "Model 13 - Epoch 5 - Batch 40/88 - Loss: 2.2030\n",
            "Model 13 - Epoch 5 - Batch 50/88 - Loss: 2.2546\n",
            "Model 13 - Epoch 5 - Batch 60/88 - Loss: 2.2917\n",
            "Model 13 - Epoch 5 - Batch 70/88 - Loss: 2.3072\n",
            "Model 13 - Epoch 5 - Batch 80/88 - Loss: 2.2384\n",
            "Model 13 - Finished Epoch 5\n",
            "Model 13 - Starting Epoch 6\n",
            "Model 13 - Epoch 6 - Batch 10/88 - Loss: 2.0433\n",
            "Model 13 - Epoch 6 - Batch 20/88 - Loss: 2.2662\n",
            "Model 13 - Epoch 6 - Batch 30/88 - Loss: 2.1983\n",
            "Model 13 - Epoch 6 - Batch 40/88 - Loss: 2.2558\n",
            "Model 13 - Epoch 6 - Batch 50/88 - Loss: 2.2311\n",
            "Model 13 - Epoch 6 - Batch 60/88 - Loss: 2.1594\n",
            "Model 13 - Epoch 6 - Batch 70/88 - Loss: 2.2869\n",
            "Model 13 - Epoch 6 - Batch 80/88 - Loss: 2.3308\n",
            "Model 13 - Finished Epoch 6\n",
            "Model 13 - Starting Epoch 7\n",
            "Model 13 - Epoch 7 - Batch 10/88 - Loss: 2.2710\n",
            "Model 13 - Epoch 7 - Batch 20/88 - Loss: 2.1530\n",
            "Model 13 - Epoch 7 - Batch 30/88 - Loss: 2.2183\n",
            "Model 13 - Epoch 7 - Batch 40/88 - Loss: 2.2907\n",
            "Model 13 - Epoch 7 - Batch 50/88 - Loss: 2.2513\n",
            "Model 13 - Epoch 7 - Batch 60/88 - Loss: 2.3480\n",
            "Model 13 - Epoch 7 - Batch 70/88 - Loss: 2.2152\n",
            "Model 13 - Epoch 7 - Batch 80/88 - Loss: 2.1914\n",
            "Model 13 - Finished Epoch 7\n",
            "Model 13 - Starting Epoch 8\n",
            "Model 13 - Epoch 8 - Batch 10/88 - Loss: 2.1911\n",
            "Model 13 - Epoch 8 - Batch 20/88 - Loss: 2.1932\n",
            "Model 13 - Epoch 8 - Batch 30/88 - Loss: 2.2445\n",
            "Model 13 - Epoch 8 - Batch 40/88 - Loss: 2.2051\n",
            "Model 13 - Epoch 8 - Batch 50/88 - Loss: 2.1389\n",
            "Model 13 - Epoch 8 - Batch 60/88 - Loss: 2.2680\n",
            "Model 13 - Epoch 8 - Batch 70/88 - Loss: 2.3141\n",
            "Model 13 - Epoch 8 - Batch 80/88 - Loss: 2.2978\n",
            "Model 13 - Finished Epoch 8\n",
            "Model 13 - Starting Epoch 9\n",
            "Model 13 - Epoch 9 - Batch 10/88 - Loss: 2.1758\n",
            "Model 13 - Epoch 9 - Batch 20/88 - Loss: 2.2965\n",
            "Model 13 - Epoch 9 - Batch 30/88 - Loss: 2.1966\n",
            "Model 13 - Epoch 9 - Batch 40/88 - Loss: 2.1593\n",
            "Model 13 - Epoch 9 - Batch 50/88 - Loss: 2.3262\n",
            "Model 13 - Epoch 9 - Batch 60/88 - Loss: 2.2150\n",
            "Model 13 - Epoch 9 - Batch 70/88 - Loss: 2.2896\n",
            "Model 13 - Epoch 9 - Batch 80/88 - Loss: 2.2583\n",
            "Model 13 - Finished Epoch 9\n",
            "Model 13 - Starting Epoch 10\n",
            "Model 13 - Epoch 10 - Batch 10/88 - Loss: 2.1684\n",
            "Model 13 - Epoch 10 - Batch 20/88 - Loss: 2.2221\n",
            "Model 13 - Epoch 10 - Batch 30/88 - Loss: 2.1059\n",
            "Model 13 - Epoch 10 - Batch 40/88 - Loss: 2.2332\n",
            "Model 13 - Epoch 10 - Batch 50/88 - Loss: 2.1982\n",
            "Model 13 - Epoch 10 - Batch 60/88 - Loss: 2.2939\n",
            "Model 13 - Epoch 10 - Batch 70/88 - Loss: 2.2502\n",
            "Model 13 - Epoch 10 - Batch 80/88 - Loss: 2.2094\n",
            "Model 13 - Finished Epoch 10\n",
            "Model 13 done training for 10 epochs in generation 5\n",
            "Model 14 - Starting Epoch 1\n",
            "Model 14 - Epoch 1 - Batch 10/88 - Loss: 2.3690\n",
            "Model 14 - Epoch 1 - Batch 20/88 - Loss: 2.4527\n",
            "Model 14 - Epoch 1 - Batch 30/88 - Loss: 2.3381\n",
            "Model 14 - Epoch 1 - Batch 40/88 - Loss: 2.5163\n",
            "Model 14 - Epoch 1 - Batch 50/88 - Loss: 2.4595\n",
            "Model 14 - Epoch 1 - Batch 60/88 - Loss: 2.4460\n",
            "Model 14 - Epoch 1 - Batch 70/88 - Loss: 2.3212\n",
            "Model 14 - Epoch 1 - Batch 80/88 - Loss: 2.4413\n",
            "Model 14 - Finished Epoch 1\n",
            "Model 14 - Starting Epoch 2\n",
            "Model 14 - Epoch 2 - Batch 10/88 - Loss: 2.5031\n",
            "Model 14 - Epoch 2 - Batch 20/88 - Loss: 2.2691\n",
            "Model 14 - Epoch 2 - Batch 30/88 - Loss: 2.3034\n",
            "Model 14 - Epoch 2 - Batch 40/88 - Loss: 2.3507\n",
            "Model 14 - Epoch 2 - Batch 50/88 - Loss: 2.3884\n",
            "Model 14 - Epoch 2 - Batch 60/88 - Loss: 2.3740\n",
            "Model 14 - Epoch 2 - Batch 70/88 - Loss: 2.3646\n",
            "Model 14 - Epoch 2 - Batch 80/88 - Loss: 2.3391\n",
            "Model 14 - Finished Epoch 2\n",
            "Model 14 - Starting Epoch 3\n",
            "Model 14 - Epoch 3 - Batch 10/88 - Loss: 2.3426\n",
            "Model 14 - Epoch 3 - Batch 20/88 - Loss: 2.3634\n",
            "Model 14 - Epoch 3 - Batch 30/88 - Loss: 2.3206\n",
            "Model 14 - Epoch 3 - Batch 40/88 - Loss: 2.3608\n",
            "Model 14 - Epoch 3 - Batch 50/88 - Loss: 2.4255\n",
            "Model 14 - Epoch 3 - Batch 60/88 - Loss: 2.4080\n",
            "Model 14 - Epoch 3 - Batch 70/88 - Loss: 2.3532\n",
            "Model 14 - Epoch 3 - Batch 80/88 - Loss: 2.3438\n",
            "Model 14 - Finished Epoch 3\n",
            "Model 14 - Starting Epoch 4\n",
            "Model 14 - Epoch 4 - Batch 10/88 - Loss: 2.3068\n",
            "Model 14 - Epoch 4 - Batch 20/88 - Loss: 2.2950\n",
            "Model 14 - Epoch 4 - Batch 30/88 - Loss: 2.3459\n",
            "Model 14 - Epoch 4 - Batch 40/88 - Loss: 2.2972\n",
            "Model 14 - Epoch 4 - Batch 50/88 - Loss: 2.5099\n",
            "Model 14 - Epoch 4 - Batch 60/88 - Loss: 2.4266\n",
            "Model 14 - Epoch 4 - Batch 70/88 - Loss: 2.4469\n",
            "Model 14 - Epoch 4 - Batch 80/88 - Loss: 2.4320\n",
            "Model 14 - Finished Epoch 4\n",
            "Model 14 - Starting Epoch 5\n",
            "Model 14 - Epoch 5 - Batch 10/88 - Loss: 2.2348\n",
            "Model 14 - Epoch 5 - Batch 20/88 - Loss: 2.2206\n",
            "Model 14 - Epoch 5 - Batch 30/88 - Loss: 2.3865\n",
            "Model 14 - Epoch 5 - Batch 40/88 - Loss: 2.3793\n",
            "Model 14 - Epoch 5 - Batch 50/88 - Loss: 2.3228\n",
            "Model 14 - Epoch 5 - Batch 60/88 - Loss: 2.3394\n",
            "Model 14 - Epoch 5 - Batch 70/88 - Loss: 2.3416\n",
            "Model 14 - Epoch 5 - Batch 80/88 - Loss: 2.4138\n",
            "Model 14 - Finished Epoch 5\n",
            "Model 14 - Starting Epoch 6\n",
            "Model 14 - Epoch 6 - Batch 10/88 - Loss: 2.2890\n",
            "Model 14 - Epoch 6 - Batch 20/88 - Loss: 2.2308\n",
            "Model 14 - Epoch 6 - Batch 30/88 - Loss: 2.2914\n",
            "Model 14 - Epoch 6 - Batch 40/88 - Loss: 2.3137\n",
            "Model 14 - Epoch 6 - Batch 50/88 - Loss: 2.3260\n",
            "Model 14 - Epoch 6 - Batch 60/88 - Loss: 2.3031\n",
            "Model 14 - Epoch 6 - Batch 70/88 - Loss: 2.4125\n",
            "Model 14 - Epoch 6 - Batch 80/88 - Loss: 2.3369\n",
            "Model 14 - Finished Epoch 6\n",
            "Model 14 - Starting Epoch 7\n",
            "Model 14 - Epoch 7 - Batch 10/88 - Loss: 2.2904\n",
            "Model 14 - Epoch 7 - Batch 20/88 - Loss: 2.2407\n",
            "Model 14 - Epoch 7 - Batch 30/88 - Loss: 2.2148\n",
            "Model 14 - Epoch 7 - Batch 40/88 - Loss: 2.3521\n",
            "Model 14 - Epoch 7 - Batch 50/88 - Loss: 2.3029\n",
            "Model 14 - Epoch 7 - Batch 60/88 - Loss: 2.2960\n",
            "Model 14 - Epoch 7 - Batch 70/88 - Loss: 2.4561\n",
            "Model 14 - Epoch 7 - Batch 80/88 - Loss: 2.3360\n",
            "Model 14 - Finished Epoch 7\n",
            "Model 14 - Starting Epoch 8\n",
            "Model 14 - Epoch 8 - Batch 10/88 - Loss: 2.2847\n",
            "Model 14 - Epoch 8 - Batch 20/88 - Loss: 2.3118\n",
            "Model 14 - Epoch 8 - Batch 30/88 - Loss: 2.2726\n",
            "Model 14 - Epoch 8 - Batch 40/88 - Loss: 2.3450\n",
            "Model 14 - Epoch 8 - Batch 50/88 - Loss: 2.2823\n",
            "Model 14 - Epoch 8 - Batch 60/88 - Loss: 2.3513\n",
            "Model 14 - Epoch 8 - Batch 70/88 - Loss: 2.3150\n",
            "Model 14 - Epoch 8 - Batch 80/88 - Loss: 2.3225\n",
            "Model 14 - Finished Epoch 8\n",
            "Model 14 - Starting Epoch 9\n",
            "Model 14 - Epoch 9 - Batch 10/88 - Loss: 2.2290\n",
            "Model 14 - Epoch 9 - Batch 20/88 - Loss: 2.2780\n",
            "Model 14 - Epoch 9 - Batch 30/88 - Loss: 2.2765\n",
            "Model 14 - Epoch 9 - Batch 40/88 - Loss: 2.2089\n",
            "Model 14 - Epoch 9 - Batch 50/88 - Loss: 2.2536\n",
            "Model 14 - Epoch 9 - Batch 60/88 - Loss: 2.2435\n",
            "Model 14 - Epoch 9 - Batch 70/88 - Loss: 2.3981\n",
            "Model 14 - Epoch 9 - Batch 80/88 - Loss: 2.1666\n",
            "Model 14 - Finished Epoch 9\n",
            "Model 14 - Starting Epoch 10\n",
            "Model 14 - Epoch 10 - Batch 10/88 - Loss: 2.2328\n",
            "Model 14 - Epoch 10 - Batch 20/88 - Loss: 2.1285\n",
            "Model 14 - Epoch 10 - Batch 30/88 - Loss: 2.2165\n",
            "Model 14 - Epoch 10 - Batch 40/88 - Loss: 2.2968\n",
            "Model 14 - Epoch 10 - Batch 50/88 - Loss: 2.2123\n",
            "Model 14 - Epoch 10 - Batch 60/88 - Loss: 2.3649\n",
            "Model 14 - Epoch 10 - Batch 70/88 - Loss: 2.2473\n",
            "Model 14 - Epoch 10 - Batch 80/88 - Loss: 2.3733\n",
            "Model 14 - Finished Epoch 10\n",
            "Model 14 done training for 10 epochs in generation 5\n",
            "Model 15 - Starting Epoch 1\n",
            "Model 15 - Epoch 1 - Batch 10/88 - Loss: 2.4472\n",
            "Model 15 - Epoch 1 - Batch 20/88 - Loss: 2.4631\n",
            "Model 15 - Epoch 1 - Batch 30/88 - Loss: 2.5186\n",
            "Model 15 - Epoch 1 - Batch 40/88 - Loss: 2.5482\n",
            "Model 15 - Epoch 1 - Batch 50/88 - Loss: 2.5109\n",
            "Model 15 - Epoch 1 - Batch 60/88 - Loss: 2.4635\n",
            "Model 15 - Epoch 1 - Batch 70/88 - Loss: 2.3960\n",
            "Model 15 - Epoch 1 - Batch 80/88 - Loss: 2.5338\n",
            "Model 15 - Finished Epoch 1\n",
            "Model 15 - Starting Epoch 2\n",
            "Model 15 - Epoch 2 - Batch 10/88 - Loss: 2.3279\n",
            "Model 15 - Epoch 2 - Batch 20/88 - Loss: 2.3290\n",
            "Model 15 - Epoch 2 - Batch 30/88 - Loss: 2.5005\n",
            "Model 15 - Epoch 2 - Batch 40/88 - Loss: 2.3828\n",
            "Model 15 - Epoch 2 - Batch 50/88 - Loss: 2.5412\n",
            "Model 15 - Epoch 2 - Batch 60/88 - Loss: 2.4217\n",
            "Model 15 - Epoch 2 - Batch 70/88 - Loss: 2.6511\n",
            "Model 15 - Epoch 2 - Batch 80/88 - Loss: 2.5840\n",
            "Model 15 - Finished Epoch 2\n",
            "Model 15 - Starting Epoch 3\n",
            "Model 15 - Epoch 3 - Batch 10/88 - Loss: 2.4387\n",
            "Model 15 - Epoch 3 - Batch 20/88 - Loss: 2.3491\n",
            "Model 15 - Epoch 3 - Batch 30/88 - Loss: 2.3487\n",
            "Model 15 - Epoch 3 - Batch 40/88 - Loss: 2.4522\n",
            "Model 15 - Epoch 3 - Batch 50/88 - Loss: 2.4405\n",
            "Model 15 - Epoch 3 - Batch 60/88 - Loss: 2.2918\n",
            "Model 15 - Epoch 3 - Batch 70/88 - Loss: 2.5489\n",
            "Model 15 - Epoch 3 - Batch 80/88 - Loss: 2.4646\n",
            "Model 15 - Finished Epoch 3\n",
            "Model 15 - Starting Epoch 4\n",
            "Model 15 - Epoch 4 - Batch 10/88 - Loss: 2.5031\n",
            "Model 15 - Epoch 4 - Batch 20/88 - Loss: 2.3961\n",
            "Model 15 - Epoch 4 - Batch 30/88 - Loss: 2.5200\n",
            "Model 15 - Epoch 4 - Batch 40/88 - Loss: 2.3903\n",
            "Model 15 - Epoch 4 - Batch 50/88 - Loss: 2.4066\n",
            "Model 15 - Epoch 4 - Batch 60/88 - Loss: 2.4621\n",
            "Model 15 - Epoch 4 - Batch 70/88 - Loss: 2.3975\n",
            "Model 15 - Epoch 4 - Batch 80/88 - Loss: 2.4621\n",
            "Model 15 - Finished Epoch 4\n",
            "Model 15 - Starting Epoch 5\n",
            "Model 15 - Epoch 5 - Batch 10/88 - Loss: 2.2792\n",
            "Model 15 - Epoch 5 - Batch 20/88 - Loss: 2.4254\n",
            "Model 15 - Epoch 5 - Batch 30/88 - Loss: 2.4291\n",
            "Model 15 - Epoch 5 - Batch 40/88 - Loss: 2.5032\n",
            "Model 15 - Epoch 5 - Batch 50/88 - Loss: 2.4892\n",
            "Model 15 - Epoch 5 - Batch 60/88 - Loss: 2.3950\n",
            "Model 15 - Epoch 5 - Batch 70/88 - Loss: 2.4766\n",
            "Model 15 - Epoch 5 - Batch 80/88 - Loss: 2.5811\n",
            "Model 15 - Finished Epoch 5\n",
            "Model 15 - Starting Epoch 6\n",
            "Model 15 - Epoch 6 - Batch 10/88 - Loss: 2.3602\n",
            "Model 15 - Epoch 6 - Batch 20/88 - Loss: 2.3937\n",
            "Model 15 - Epoch 6 - Batch 30/88 - Loss: 2.3961\n",
            "Model 15 - Epoch 6 - Batch 40/88 - Loss: 2.3521\n",
            "Model 15 - Epoch 6 - Batch 50/88 - Loss: 2.3462\n",
            "Model 15 - Epoch 6 - Batch 60/88 - Loss: 2.3859\n",
            "Model 15 - Epoch 6 - Batch 70/88 - Loss: 2.4569\n",
            "Model 15 - Epoch 6 - Batch 80/88 - Loss: 2.4317\n",
            "Model 15 - Finished Epoch 6\n",
            "Model 15 - Starting Epoch 7\n",
            "Model 15 - Epoch 7 - Batch 10/88 - Loss: 2.3123\n",
            "Model 15 - Epoch 7 - Batch 20/88 - Loss: 2.3802\n",
            "Model 15 - Epoch 7 - Batch 30/88 - Loss: 2.2754\n",
            "Model 15 - Epoch 7 - Batch 40/88 - Loss: 2.2539\n",
            "Model 15 - Epoch 7 - Batch 50/88 - Loss: 2.4355\n",
            "Model 15 - Epoch 7 - Batch 60/88 - Loss: 2.5192\n",
            "Model 15 - Epoch 7 - Batch 70/88 - Loss: 2.3755\n",
            "Model 15 - Epoch 7 - Batch 80/88 - Loss: 2.2998\n",
            "Model 15 - Finished Epoch 7\n",
            "Model 15 - Starting Epoch 8\n",
            "Model 15 - Epoch 8 - Batch 10/88 - Loss: 2.3532\n",
            "Model 15 - Epoch 8 - Batch 20/88 - Loss: 2.3863\n",
            "Model 15 - Epoch 8 - Batch 30/88 - Loss: 2.3358\n",
            "Model 15 - Epoch 8 - Batch 40/88 - Loss: 2.3586\n",
            "Model 15 - Epoch 8 - Batch 50/88 - Loss: 2.4351\n",
            "Model 15 - Epoch 8 - Batch 60/88 - Loss: 2.3267\n",
            "Model 15 - Epoch 8 - Batch 70/88 - Loss: 2.4411\n",
            "Model 15 - Epoch 8 - Batch 80/88 - Loss: 2.4270\n",
            "Model 15 - Finished Epoch 8\n",
            "Model 15 - Starting Epoch 9\n",
            "Model 15 - Epoch 9 - Batch 10/88 - Loss: 2.3177\n",
            "Model 15 - Epoch 9 - Batch 20/88 - Loss: 2.4211\n",
            "Model 15 - Epoch 9 - Batch 30/88 - Loss: 2.1907\n",
            "Model 15 - Epoch 9 - Batch 40/88 - Loss: 2.2148\n",
            "Model 15 - Epoch 9 - Batch 50/88 - Loss: 2.3288\n",
            "Model 15 - Epoch 9 - Batch 60/88 - Loss: 2.5300\n",
            "Model 15 - Epoch 9 - Batch 70/88 - Loss: 2.3991\n",
            "Model 15 - Epoch 9 - Batch 80/88 - Loss: 2.3300\n",
            "Model 15 - Finished Epoch 9\n",
            "Model 15 - Starting Epoch 10\n",
            "Model 15 - Epoch 10 - Batch 10/88 - Loss: 2.2249\n",
            "Model 15 - Epoch 10 - Batch 20/88 - Loss: 2.4158\n",
            "Model 15 - Epoch 10 - Batch 30/88 - Loss: 2.3345\n",
            "Model 15 - Epoch 10 - Batch 40/88 - Loss: 2.2179\n",
            "Model 15 - Epoch 10 - Batch 50/88 - Loss: 2.3046\n",
            "Model 15 - Epoch 10 - Batch 60/88 - Loss: 2.3056\n",
            "Model 15 - Epoch 10 - Batch 70/88 - Loss: 2.3815\n",
            "Model 15 - Epoch 10 - Batch 80/88 - Loss: 2.3001\n",
            "Model 15 - Finished Epoch 10\n",
            "Model 15 done training for 10 epochs in generation 5\n",
            "Model 16 - Starting Epoch 1\n",
            "Model 16 - Epoch 1 - Batch 10/88 - Loss: 2.3206\n",
            "Model 16 - Epoch 1 - Batch 20/88 - Loss: 2.4132\n",
            "Model 16 - Epoch 1 - Batch 30/88 - Loss: 2.4916\n",
            "Model 16 - Epoch 1 - Batch 40/88 - Loss: 2.2761\n",
            "Model 16 - Epoch 1 - Batch 50/88 - Loss: 2.3892\n",
            "Model 16 - Epoch 1 - Batch 60/88 - Loss: 2.3192\n",
            "Model 16 - Epoch 1 - Batch 70/88 - Loss: 2.2819\n",
            "Model 16 - Epoch 1 - Batch 80/88 - Loss: 2.4346\n",
            "Model 16 - Finished Epoch 1\n",
            "Model 16 - Starting Epoch 2\n",
            "Model 16 - Epoch 2 - Batch 10/88 - Loss: 2.1864\n",
            "Model 16 - Epoch 2 - Batch 20/88 - Loss: 2.3099\n",
            "Model 16 - Epoch 2 - Batch 30/88 - Loss: 2.3409\n",
            "Model 16 - Epoch 2 - Batch 40/88 - Loss: 2.3389\n",
            "Model 16 - Epoch 2 - Batch 50/88 - Loss: 2.3555\n",
            "Model 16 - Epoch 2 - Batch 60/88 - Loss: 2.2577\n",
            "Model 16 - Epoch 2 - Batch 70/88 - Loss: 2.2904\n",
            "Model 16 - Epoch 2 - Batch 80/88 - Loss: 2.3938\n",
            "Model 16 - Finished Epoch 2\n",
            "Model 16 - Starting Epoch 3\n",
            "Model 16 - Epoch 3 - Batch 10/88 - Loss: 2.3205\n",
            "Model 16 - Epoch 3 - Batch 20/88 - Loss: 2.2707\n",
            "Model 16 - Epoch 3 - Batch 30/88 - Loss: 2.3028\n",
            "Model 16 - Epoch 3 - Batch 40/88 - Loss: 2.2268\n",
            "Model 16 - Epoch 3 - Batch 50/88 - Loss: 2.3415\n",
            "Model 16 - Epoch 3 - Batch 60/88 - Loss: 2.2826\n",
            "Model 16 - Epoch 3 - Batch 70/88 - Loss: 2.4179\n",
            "Model 16 - Epoch 3 - Batch 80/88 - Loss: 2.2903\n",
            "Model 16 - Finished Epoch 3\n",
            "Model 16 - Starting Epoch 4\n",
            "Model 16 - Epoch 4 - Batch 10/88 - Loss: 2.2900\n",
            "Model 16 - Epoch 4 - Batch 20/88 - Loss: 2.1674\n",
            "Model 16 - Epoch 4 - Batch 30/88 - Loss: 2.3476\n",
            "Model 16 - Epoch 4 - Batch 40/88 - Loss: 2.2489\n",
            "Model 16 - Epoch 4 - Batch 50/88 - Loss: 2.3514\n",
            "Model 16 - Epoch 4 - Batch 60/88 - Loss: 2.3171\n",
            "Model 16 - Epoch 4 - Batch 70/88 - Loss: 2.1805\n",
            "Model 16 - Epoch 4 - Batch 80/88 - Loss: 2.1594\n",
            "Model 16 - Finished Epoch 4\n",
            "Model 16 - Starting Epoch 5\n",
            "Model 16 - Epoch 5 - Batch 10/88 - Loss: 2.3412\n",
            "Model 16 - Epoch 5 - Batch 20/88 - Loss: 2.2184\n",
            "Model 16 - Epoch 5 - Batch 30/88 - Loss: 2.2971\n",
            "Model 16 - Epoch 5 - Batch 40/88 - Loss: 2.3341\n",
            "Model 16 - Epoch 5 - Batch 50/88 - Loss: 2.2290\n",
            "Model 16 - Epoch 5 - Batch 60/88 - Loss: 2.2722\n",
            "Model 16 - Epoch 5 - Batch 70/88 - Loss: 2.1775\n",
            "Model 16 - Epoch 5 - Batch 80/88 - Loss: 2.3360\n",
            "Model 16 - Finished Epoch 5\n",
            "Model 16 - Starting Epoch 6\n",
            "Model 16 - Epoch 6 - Batch 10/88 - Loss: 2.2056\n",
            "Model 16 - Epoch 6 - Batch 20/88 - Loss: 2.2428\n",
            "Model 16 - Epoch 6 - Batch 30/88 - Loss: 2.1855\n",
            "Model 16 - Epoch 6 - Batch 40/88 - Loss: 2.2587\n",
            "Model 16 - Epoch 6 - Batch 50/88 - Loss: 2.2976\n",
            "Model 16 - Epoch 6 - Batch 60/88 - Loss: 2.2522\n",
            "Model 16 - Epoch 6 - Batch 70/88 - Loss: 2.2999\n",
            "Model 16 - Epoch 6 - Batch 80/88 - Loss: 2.3904\n",
            "Model 16 - Finished Epoch 6\n",
            "Model 16 - Starting Epoch 7\n",
            "Model 16 - Epoch 7 - Batch 10/88 - Loss: 2.1714\n",
            "Model 16 - Epoch 7 - Batch 20/88 - Loss: 2.2507\n",
            "Model 16 - Epoch 7 - Batch 30/88 - Loss: 2.1517\n",
            "Model 16 - Epoch 7 - Batch 40/88 - Loss: 2.2723\n",
            "Model 16 - Epoch 7 - Batch 50/88 - Loss: 2.2609\n",
            "Model 16 - Epoch 7 - Batch 60/88 - Loss: 2.3126\n",
            "Model 16 - Epoch 7 - Batch 70/88 - Loss: 2.2818\n",
            "Model 16 - Epoch 7 - Batch 80/88 - Loss: 2.1926\n",
            "Model 16 - Finished Epoch 7\n",
            "Model 16 - Starting Epoch 8\n",
            "Model 16 - Epoch 8 - Batch 10/88 - Loss: 2.1269\n",
            "Model 16 - Epoch 8 - Batch 20/88 - Loss: 2.1146\n",
            "Model 16 - Epoch 8 - Batch 30/88 - Loss: 2.2460\n",
            "Model 16 - Epoch 8 - Batch 40/88 - Loss: 2.1973\n",
            "Model 16 - Epoch 8 - Batch 50/88 - Loss: 2.3337\n",
            "Model 16 - Epoch 8 - Batch 60/88 - Loss: 2.2657\n",
            "Model 16 - Epoch 8 - Batch 70/88 - Loss: 2.2706\n",
            "Model 16 - Epoch 8 - Batch 80/88 - Loss: 2.2400\n",
            "Model 16 - Finished Epoch 8\n",
            "Model 16 - Starting Epoch 9\n",
            "Model 16 - Epoch 9 - Batch 10/88 - Loss: 2.2061\n",
            "Model 16 - Epoch 9 - Batch 20/88 - Loss: 2.1884\n",
            "Model 16 - Epoch 9 - Batch 30/88 - Loss: 2.2374\n",
            "Model 16 - Epoch 9 - Batch 40/88 - Loss: 2.0657\n",
            "Model 16 - Epoch 9 - Batch 50/88 - Loss: 2.2346\n",
            "Model 16 - Epoch 9 - Batch 60/88 - Loss: 2.2420\n",
            "Model 16 - Epoch 9 - Batch 70/88 - Loss: 2.1116\n",
            "Model 16 - Epoch 9 - Batch 80/88 - Loss: 2.2769\n",
            "Model 16 - Finished Epoch 9\n",
            "Model 16 - Starting Epoch 10\n",
            "Model 16 - Epoch 10 - Batch 10/88 - Loss: 2.0781\n",
            "Model 16 - Epoch 10 - Batch 20/88 - Loss: 2.0515\n",
            "Model 16 - Epoch 10 - Batch 30/88 - Loss: 2.1878\n",
            "Model 16 - Epoch 10 - Batch 40/88 - Loss: 2.1174\n",
            "Model 16 - Epoch 10 - Batch 50/88 - Loss: 2.1577\n",
            "Model 16 - Epoch 10 - Batch 60/88 - Loss: 2.2225\n",
            "Model 16 - Epoch 10 - Batch 70/88 - Loss: 2.2730\n",
            "Model 16 - Epoch 10 - Batch 80/88 - Loss: 2.2162\n",
            "Model 16 - Finished Epoch 10\n",
            "Model 16 done training for 10 epochs in generation 5\n",
            "Model 17 - Starting Epoch 1\n",
            "Model 17 - Epoch 1 - Batch 10/88 - Loss: 2.3180\n",
            "Model 17 - Epoch 1 - Batch 20/88 - Loss: 2.4154\n",
            "Model 17 - Epoch 1 - Batch 30/88 - Loss: 2.4340\n",
            "Model 17 - Epoch 1 - Batch 40/88 - Loss: 2.3450\n",
            "Model 17 - Epoch 1 - Batch 50/88 - Loss: 2.3434\n",
            "Model 17 - Epoch 1 - Batch 60/88 - Loss: 2.4173\n",
            "Model 17 - Epoch 1 - Batch 70/88 - Loss: 2.4535\n",
            "Model 17 - Epoch 1 - Batch 80/88 - Loss: 2.3503\n",
            "Model 17 - Finished Epoch 1\n",
            "Model 17 - Starting Epoch 2\n",
            "Model 17 - Epoch 2 - Batch 10/88 - Loss: 2.3317\n",
            "Model 17 - Epoch 2 - Batch 20/88 - Loss: 2.2194\n",
            "Model 17 - Epoch 2 - Batch 30/88 - Loss: 2.2097\n",
            "Model 17 - Epoch 2 - Batch 40/88 - Loss: 2.4307\n",
            "Model 17 - Epoch 2 - Batch 50/88 - Loss: 2.3284\n",
            "Model 17 - Epoch 2 - Batch 60/88 - Loss: 2.3438\n",
            "Model 17 - Epoch 2 - Batch 70/88 - Loss: 2.4653\n",
            "Model 17 - Epoch 2 - Batch 80/88 - Loss: 2.4324\n",
            "Model 17 - Finished Epoch 2\n",
            "Model 17 - Starting Epoch 3\n",
            "Model 17 - Epoch 3 - Batch 10/88 - Loss: 2.3288\n",
            "Model 17 - Epoch 3 - Batch 20/88 - Loss: 2.2069\n",
            "Model 17 - Epoch 3 - Batch 30/88 - Loss: 2.3601\n",
            "Model 17 - Epoch 3 - Batch 40/88 - Loss: 2.2675\n",
            "Model 17 - Epoch 3 - Batch 50/88 - Loss: 2.3298\n",
            "Model 17 - Epoch 3 - Batch 60/88 - Loss: 2.2413\n",
            "Model 17 - Epoch 3 - Batch 70/88 - Loss: 2.4858\n",
            "Model 17 - Epoch 3 - Batch 80/88 - Loss: 2.4461\n",
            "Model 17 - Finished Epoch 3\n",
            "Model 17 - Starting Epoch 4\n",
            "Model 17 - Epoch 4 - Batch 10/88 - Loss: 2.3710\n",
            "Model 17 - Epoch 4 - Batch 20/88 - Loss: 2.2181\n",
            "Model 17 - Epoch 4 - Batch 30/88 - Loss: 2.2459\n",
            "Model 17 - Epoch 4 - Batch 40/88 - Loss: 2.3011\n",
            "Model 17 - Epoch 4 - Batch 50/88 - Loss: 2.3079\n",
            "Model 17 - Epoch 4 - Batch 60/88 - Loss: 2.2791\n",
            "Model 17 - Epoch 4 - Batch 70/88 - Loss: 2.4338\n",
            "Model 17 - Epoch 4 - Batch 80/88 - Loss: 2.3653\n",
            "Model 17 - Finished Epoch 4\n",
            "Model 17 - Starting Epoch 5\n",
            "Model 17 - Epoch 5 - Batch 10/88 - Loss: 2.2963\n",
            "Model 17 - Epoch 5 - Batch 20/88 - Loss: 2.2446\n",
            "Model 17 - Epoch 5 - Batch 30/88 - Loss: 2.3389\n",
            "Model 17 - Epoch 5 - Batch 40/88 - Loss: 2.3449\n",
            "Model 17 - Epoch 5 - Batch 50/88 - Loss: 2.2927\n",
            "Model 17 - Epoch 5 - Batch 60/88 - Loss: 2.2891\n",
            "Model 17 - Epoch 5 - Batch 70/88 - Loss: 2.3295\n",
            "Model 17 - Epoch 5 - Batch 80/88 - Loss: 2.3181\n",
            "Model 17 - Finished Epoch 5\n",
            "Model 17 - Starting Epoch 6\n",
            "Model 17 - Epoch 6 - Batch 10/88 - Loss: 2.1894\n",
            "Model 17 - Epoch 6 - Batch 20/88 - Loss: 2.2257\n",
            "Model 17 - Epoch 6 - Batch 30/88 - Loss: 2.1947\n",
            "Model 17 - Epoch 6 - Batch 40/88 - Loss: 2.2172\n",
            "Model 17 - Epoch 6 - Batch 50/88 - Loss: 2.2718\n",
            "Model 17 - Epoch 6 - Batch 60/88 - Loss: 2.3602\n",
            "Model 17 - Epoch 6 - Batch 70/88 - Loss: 2.2962\n",
            "Model 17 - Epoch 6 - Batch 80/88 - Loss: 2.3892\n",
            "Model 17 - Finished Epoch 6\n",
            "Model 17 - Starting Epoch 7\n",
            "Model 17 - Epoch 7 - Batch 10/88 - Loss: 2.3273\n",
            "Model 17 - Epoch 7 - Batch 20/88 - Loss: 2.3574\n",
            "Model 17 - Epoch 7 - Batch 30/88 - Loss: 2.2330\n",
            "Model 17 - Epoch 7 - Batch 40/88 - Loss: 2.3187\n",
            "Model 17 - Epoch 7 - Batch 50/88 - Loss: 2.2740\n",
            "Model 17 - Epoch 7 - Batch 60/88 - Loss: 2.3322\n",
            "Model 17 - Epoch 7 - Batch 70/88 - Loss: 2.3241\n",
            "Model 17 - Epoch 7 - Batch 80/88 - Loss: 2.4127\n",
            "Model 17 - Finished Epoch 7\n",
            "Model 17 - Starting Epoch 8\n",
            "Model 17 - Epoch 8 - Batch 10/88 - Loss: 2.2578\n",
            "Model 17 - Epoch 8 - Batch 20/88 - Loss: 2.1634\n",
            "Model 17 - Epoch 8 - Batch 30/88 - Loss: 2.3951\n",
            "Model 17 - Epoch 8 - Batch 40/88 - Loss: 2.3133\n",
            "Model 17 - Epoch 8 - Batch 50/88 - Loss: 2.2305\n",
            "Model 17 - Epoch 8 - Batch 60/88 - Loss: 2.3422\n",
            "Model 17 - Epoch 8 - Batch 70/88 - Loss: 2.3459\n",
            "Model 17 - Epoch 8 - Batch 80/88 - Loss: 2.3276\n",
            "Model 17 - Finished Epoch 8\n",
            "Model 17 - Starting Epoch 9\n",
            "Model 17 - Epoch 9 - Batch 10/88 - Loss: 2.3275\n",
            "Model 17 - Epoch 9 - Batch 20/88 - Loss: 2.3287\n",
            "Model 17 - Epoch 9 - Batch 30/88 - Loss: 2.1766\n",
            "Model 17 - Epoch 9 - Batch 40/88 - Loss: 2.2390\n",
            "Model 17 - Epoch 9 - Batch 50/88 - Loss: 2.3662\n",
            "Model 17 - Epoch 9 - Batch 60/88 - Loss: 2.3648\n",
            "Model 17 - Epoch 9 - Batch 70/88 - Loss: 2.3287\n",
            "Model 17 - Epoch 9 - Batch 80/88 - Loss: 2.2728\n",
            "Model 17 - Finished Epoch 9\n",
            "Model 17 - Starting Epoch 10\n",
            "Model 17 - Epoch 10 - Batch 10/88 - Loss: 2.2194\n",
            "Model 17 - Epoch 10 - Batch 20/88 - Loss: 2.1699\n",
            "Model 17 - Epoch 10 - Batch 30/88 - Loss: 2.2005\n",
            "Model 17 - Epoch 10 - Batch 40/88 - Loss: 2.3392\n",
            "Model 17 - Epoch 10 - Batch 50/88 - Loss: 2.3236\n",
            "Model 17 - Epoch 10 - Batch 60/88 - Loss: 2.1844\n",
            "Model 17 - Epoch 10 - Batch 70/88 - Loss: 2.1975\n",
            "Model 17 - Epoch 10 - Batch 80/88 - Loss: 2.3200\n",
            "Model 17 - Finished Epoch 10\n",
            "Model 17 done training for 10 epochs in generation 5\n",
            "Model 18 - Starting Epoch 1\n",
            "Model 18 - Epoch 1 - Batch 10/88 - Loss: 2.4159\n",
            "Model 18 - Epoch 1 - Batch 20/88 - Loss: 2.4645\n",
            "Model 18 - Epoch 1 - Batch 30/88 - Loss: 2.2928\n",
            "Model 18 - Epoch 1 - Batch 40/88 - Loss: 2.3754\n",
            "Model 18 - Epoch 1 - Batch 50/88 - Loss: 2.5784\n",
            "Model 18 - Epoch 1 - Batch 60/88 - Loss: 2.6070\n",
            "Model 18 - Epoch 1 - Batch 70/88 - Loss: 2.4085\n",
            "Model 18 - Epoch 1 - Batch 80/88 - Loss: 2.4541\n",
            "Model 18 - Finished Epoch 1\n",
            "Model 18 - Starting Epoch 2\n",
            "Model 18 - Epoch 2 - Batch 10/88 - Loss: 2.4838\n",
            "Model 18 - Epoch 2 - Batch 20/88 - Loss: 2.3314\n",
            "Model 18 - Epoch 2 - Batch 30/88 - Loss: 2.4617\n",
            "Model 18 - Epoch 2 - Batch 40/88 - Loss: 2.3486\n",
            "Model 18 - Epoch 2 - Batch 50/88 - Loss: 2.4006\n",
            "Model 18 - Epoch 2 - Batch 60/88 - Loss: 2.3847\n",
            "Model 18 - Epoch 2 - Batch 70/88 - Loss: 2.4113\n",
            "Model 18 - Epoch 2 - Batch 80/88 - Loss: 2.5027\n",
            "Model 18 - Finished Epoch 2\n",
            "Model 18 - Starting Epoch 3\n",
            "Model 18 - Epoch 3 - Batch 10/88 - Loss: 2.3845\n",
            "Model 18 - Epoch 3 - Batch 20/88 - Loss: 2.4104\n",
            "Model 18 - Epoch 3 - Batch 30/88 - Loss: 2.3514\n",
            "Model 18 - Epoch 3 - Batch 40/88 - Loss: 2.4070\n",
            "Model 18 - Epoch 3 - Batch 50/88 - Loss: 2.5371\n",
            "Model 18 - Epoch 3 - Batch 60/88 - Loss: 2.2409\n",
            "Model 18 - Epoch 3 - Batch 70/88 - Loss: 2.3115\n",
            "Model 18 - Epoch 3 - Batch 80/88 - Loss: 2.4939\n",
            "Model 18 - Finished Epoch 3\n",
            "Model 18 - Starting Epoch 4\n",
            "Model 18 - Epoch 4 - Batch 10/88 - Loss: 2.3053\n",
            "Model 18 - Epoch 4 - Batch 20/88 - Loss: 2.3515\n",
            "Model 18 - Epoch 4 - Batch 30/88 - Loss: 2.3600\n",
            "Model 18 - Epoch 4 - Batch 40/88 - Loss: 2.3800\n",
            "Model 18 - Epoch 4 - Batch 50/88 - Loss: 2.3294\n",
            "Model 18 - Epoch 4 - Batch 60/88 - Loss: 2.4097\n",
            "Model 18 - Epoch 4 - Batch 70/88 - Loss: 2.4564\n",
            "Model 18 - Epoch 4 - Batch 80/88 - Loss: 2.3258\n",
            "Model 18 - Finished Epoch 4\n",
            "Model 18 - Starting Epoch 5\n",
            "Model 18 - Epoch 5 - Batch 10/88 - Loss: 2.3845\n",
            "Model 18 - Epoch 5 - Batch 20/88 - Loss: 2.2834\n",
            "Model 18 - Epoch 5 - Batch 30/88 - Loss: 2.3773\n",
            "Model 18 - Epoch 5 - Batch 40/88 - Loss: 2.2646\n",
            "Model 18 - Epoch 5 - Batch 50/88 - Loss: 2.3965\n",
            "Model 18 - Epoch 5 - Batch 60/88 - Loss: 2.4330\n",
            "Model 18 - Epoch 5 - Batch 70/88 - Loss: 2.4306\n",
            "Model 18 - Epoch 5 - Batch 80/88 - Loss: 2.2755\n",
            "Model 18 - Finished Epoch 5\n",
            "Model 18 - Starting Epoch 6\n",
            "Model 18 - Epoch 6 - Batch 10/88 - Loss: 2.2811\n",
            "Model 18 - Epoch 6 - Batch 20/88 - Loss: 2.3552\n",
            "Model 18 - Epoch 6 - Batch 30/88 - Loss: 2.3793\n",
            "Model 18 - Epoch 6 - Batch 40/88 - Loss: 2.1632\n",
            "Model 18 - Epoch 6 - Batch 50/88 - Loss: 2.3174\n",
            "Model 18 - Epoch 6 - Batch 60/88 - Loss: 2.4965\n",
            "Model 18 - Epoch 6 - Batch 70/88 - Loss: 2.4497\n",
            "Model 18 - Epoch 6 - Batch 80/88 - Loss: 2.4439\n",
            "Model 18 - Finished Epoch 6\n",
            "Model 18 - Starting Epoch 7\n",
            "Model 18 - Epoch 7 - Batch 10/88 - Loss: 2.2760\n",
            "Model 18 - Epoch 7 - Batch 20/88 - Loss: 2.2943\n",
            "Model 18 - Epoch 7 - Batch 30/88 - Loss: 2.3943\n",
            "Model 18 - Epoch 7 - Batch 40/88 - Loss: 2.3083\n",
            "Model 18 - Epoch 7 - Batch 50/88 - Loss: 2.3968\n",
            "Model 18 - Epoch 7 - Batch 60/88 - Loss: 2.3827\n",
            "Model 18 - Epoch 7 - Batch 70/88 - Loss: 2.3456\n",
            "Model 18 - Epoch 7 - Batch 80/88 - Loss: 2.3880\n",
            "Model 18 - Finished Epoch 7\n",
            "Model 18 - Starting Epoch 8\n",
            "Model 18 - Epoch 8 - Batch 10/88 - Loss: 2.2909\n",
            "Model 18 - Epoch 8 - Batch 20/88 - Loss: 2.2045\n",
            "Model 18 - Epoch 8 - Batch 30/88 - Loss: 2.2235\n",
            "Model 18 - Epoch 8 - Batch 40/88 - Loss: 2.2472\n",
            "Model 18 - Epoch 8 - Batch 50/88 - Loss: 2.4598\n",
            "Model 18 - Epoch 8 - Batch 60/88 - Loss: 2.4780\n",
            "Model 18 - Epoch 8 - Batch 70/88 - Loss: 2.3828\n",
            "Model 18 - Epoch 8 - Batch 80/88 - Loss: 2.3514\n",
            "Model 18 - Finished Epoch 8\n",
            "Model 18 - Starting Epoch 9\n",
            "Model 18 - Epoch 9 - Batch 10/88 - Loss: 2.2761\n",
            "Model 18 - Epoch 9 - Batch 20/88 - Loss: 2.2419\n",
            "Model 18 - Epoch 9 - Batch 30/88 - Loss: 2.2809\n",
            "Model 18 - Epoch 9 - Batch 40/88 - Loss: 2.2989\n",
            "Model 18 - Epoch 9 - Batch 50/88 - Loss: 2.1491\n",
            "Model 18 - Epoch 9 - Batch 60/88 - Loss: 2.2694\n",
            "Model 18 - Epoch 9 - Batch 70/88 - Loss: 2.3629\n",
            "Model 18 - Epoch 9 - Batch 80/88 - Loss: 2.3213\n",
            "Model 18 - Finished Epoch 9\n",
            "Model 18 - Starting Epoch 10\n",
            "Model 18 - Epoch 10 - Batch 10/88 - Loss: 2.3179\n",
            "Model 18 - Epoch 10 - Batch 20/88 - Loss: 2.3270\n",
            "Model 18 - Epoch 10 - Batch 30/88 - Loss: 2.3876\n",
            "Model 18 - Epoch 10 - Batch 40/88 - Loss: 2.3219\n",
            "Model 18 - Epoch 10 - Batch 50/88 - Loss: 2.4169\n",
            "Model 18 - Epoch 10 - Batch 60/88 - Loss: 2.2879\n",
            "Model 18 - Epoch 10 - Batch 70/88 - Loss: 2.4210\n",
            "Model 18 - Epoch 10 - Batch 80/88 - Loss: 2.3378\n",
            "Model 18 - Finished Epoch 10\n",
            "Model 18 done training for 10 epochs in generation 5\n",
            "Model 19 - Starting Epoch 1\n",
            "Model 19 - Epoch 1 - Batch 10/88 - Loss: 2.4814\n",
            "Model 19 - Epoch 1 - Batch 20/88 - Loss: 2.3671\n",
            "Model 19 - Epoch 1 - Batch 30/88 - Loss: 2.4348\n",
            "Model 19 - Epoch 1 - Batch 40/88 - Loss: 2.5046\n",
            "Model 19 - Epoch 1 - Batch 50/88 - Loss: 2.4860\n",
            "Model 19 - Epoch 1 - Batch 60/88 - Loss: 2.5429\n",
            "Model 19 - Epoch 1 - Batch 70/88 - Loss: 2.5693\n",
            "Model 19 - Epoch 1 - Batch 80/88 - Loss: 2.5226\n",
            "Model 19 - Finished Epoch 1\n",
            "Model 19 - Starting Epoch 2\n",
            "Model 19 - Epoch 2 - Batch 10/88 - Loss: 2.3345\n",
            "Model 19 - Epoch 2 - Batch 20/88 - Loss: 2.4095\n",
            "Model 19 - Epoch 2 - Batch 30/88 - Loss: 2.4408\n",
            "Model 19 - Epoch 2 - Batch 40/88 - Loss: 2.4173\n",
            "Model 19 - Epoch 2 - Batch 50/88 - Loss: 2.5090\n",
            "Model 19 - Epoch 2 - Batch 60/88 - Loss: 2.3754\n",
            "Model 19 - Epoch 2 - Batch 70/88 - Loss: 2.4135\n",
            "Model 19 - Epoch 2 - Batch 80/88 - Loss: 2.5672\n",
            "Model 19 - Finished Epoch 2\n",
            "Model 19 - Starting Epoch 3\n",
            "Model 19 - Epoch 3 - Batch 10/88 - Loss: 2.3420\n",
            "Model 19 - Epoch 3 - Batch 20/88 - Loss: 2.4100\n",
            "Model 19 - Epoch 3 - Batch 30/88 - Loss: 2.4170\n",
            "Model 19 - Epoch 3 - Batch 40/88 - Loss: 2.4833\n",
            "Model 19 - Epoch 3 - Batch 50/88 - Loss: 2.4779\n",
            "Model 19 - Epoch 3 - Batch 60/88 - Loss: 2.3725\n",
            "Model 19 - Epoch 3 - Batch 70/88 - Loss: 2.4960\n",
            "Model 19 - Epoch 3 - Batch 80/88 - Loss: 2.5409\n",
            "Model 19 - Finished Epoch 3\n",
            "Model 19 - Starting Epoch 4\n",
            "Model 19 - Epoch 4 - Batch 10/88 - Loss: 2.4105\n",
            "Model 19 - Epoch 4 - Batch 20/88 - Loss: 2.4461\n",
            "Model 19 - Epoch 4 - Batch 30/88 - Loss: 2.4419\n",
            "Model 19 - Epoch 4 - Batch 40/88 - Loss: 2.3482\n",
            "Model 19 - Epoch 4 - Batch 50/88 - Loss: 2.5185\n",
            "Model 19 - Epoch 4 - Batch 60/88 - Loss: 2.3669\n",
            "Model 19 - Epoch 4 - Batch 70/88 - Loss: 2.4011\n",
            "Model 19 - Epoch 4 - Batch 80/88 - Loss: 2.4002\n",
            "Model 19 - Finished Epoch 4\n",
            "Model 19 - Starting Epoch 5\n",
            "Model 19 - Epoch 5 - Batch 10/88 - Loss: 2.3438\n",
            "Model 19 - Epoch 5 - Batch 20/88 - Loss: 2.3552\n",
            "Model 19 - Epoch 5 - Batch 30/88 - Loss: 2.3907\n",
            "Model 19 - Epoch 5 - Batch 40/88 - Loss: 2.4452\n",
            "Model 19 - Epoch 5 - Batch 50/88 - Loss: 2.3768\n",
            "Model 19 - Epoch 5 - Batch 60/88 - Loss: 2.4826\n",
            "Model 19 - Epoch 5 - Batch 70/88 - Loss: 2.4178\n",
            "Model 19 - Epoch 5 - Batch 80/88 - Loss: 2.3760\n",
            "Model 19 - Finished Epoch 5\n",
            "Model 19 - Starting Epoch 6\n",
            "Model 19 - Epoch 6 - Batch 10/88 - Loss: 2.3393\n",
            "Model 19 - Epoch 6 - Batch 20/88 - Loss: 2.3283\n",
            "Model 19 - Epoch 6 - Batch 30/88 - Loss: 2.3998\n",
            "Model 19 - Epoch 6 - Batch 40/88 - Loss: 2.3007\n",
            "Model 19 - Epoch 6 - Batch 50/88 - Loss: 2.3993\n",
            "Model 19 - Epoch 6 - Batch 60/88 - Loss: 2.3599\n",
            "Model 19 - Epoch 6 - Batch 70/88 - Loss: 2.5344\n",
            "Model 19 - Epoch 6 - Batch 80/88 - Loss: 2.3727\n",
            "Model 19 - Finished Epoch 6\n",
            "Model 19 - Starting Epoch 7\n",
            "Model 19 - Epoch 7 - Batch 10/88 - Loss: 2.4124\n",
            "Model 19 - Epoch 7 - Batch 20/88 - Loss: 2.2970\n",
            "Model 19 - Epoch 7 - Batch 30/88 - Loss: 2.3547\n",
            "Model 19 - Epoch 7 - Batch 40/88 - Loss: 2.3315\n",
            "Model 19 - Epoch 7 - Batch 50/88 - Loss: 2.3520\n",
            "Model 19 - Epoch 7 - Batch 60/88 - Loss: 2.4031\n",
            "Model 19 - Epoch 7 - Batch 70/88 - Loss: 2.5619\n",
            "Model 19 - Epoch 7 - Batch 80/88 - Loss: 2.4447\n",
            "Model 19 - Finished Epoch 7\n",
            "Model 19 - Starting Epoch 8\n",
            "Model 19 - Epoch 8 - Batch 10/88 - Loss: 2.3313\n",
            "Model 19 - Epoch 8 - Batch 20/88 - Loss: 2.1957\n",
            "Model 19 - Epoch 8 - Batch 30/88 - Loss: 2.5740\n",
            "Model 19 - Epoch 8 - Batch 40/88 - Loss: 2.3965\n",
            "Model 19 - Epoch 8 - Batch 50/88 - Loss: 2.2118\n",
            "Model 19 - Epoch 8 - Batch 60/88 - Loss: 2.4433\n",
            "Model 19 - Epoch 8 - Batch 70/88 - Loss: 2.3857\n",
            "Model 19 - Epoch 8 - Batch 80/88 - Loss: 2.3537\n",
            "Model 19 - Finished Epoch 8\n",
            "Model 19 - Starting Epoch 9\n",
            "Model 19 - Epoch 9 - Batch 10/88 - Loss: 2.2190\n",
            "Model 19 - Epoch 9 - Batch 20/88 - Loss: 2.2811\n",
            "Model 19 - Epoch 9 - Batch 30/88 - Loss: 2.2717\n",
            "Model 19 - Epoch 9 - Batch 40/88 - Loss: 2.2469\n",
            "Model 19 - Epoch 9 - Batch 50/88 - Loss: 2.2641\n",
            "Model 19 - Epoch 9 - Batch 60/88 - Loss: 2.1588\n",
            "Model 19 - Epoch 9 - Batch 70/88 - Loss: 2.4362\n",
            "Model 19 - Epoch 9 - Batch 80/88 - Loss: 2.4299\n",
            "Model 19 - Finished Epoch 9\n",
            "Model 19 - Starting Epoch 10\n",
            "Model 19 - Epoch 10 - Batch 10/88 - Loss: 2.2906\n",
            "Model 19 - Epoch 10 - Batch 20/88 - Loss: 2.2669\n",
            "Model 19 - Epoch 10 - Batch 30/88 - Loss: 2.2150\n",
            "Model 19 - Epoch 10 - Batch 40/88 - Loss: 2.3035\n",
            "Model 19 - Epoch 10 - Batch 50/88 - Loss: 2.1459\n",
            "Model 19 - Epoch 10 - Batch 60/88 - Loss: 2.3513\n",
            "Model 19 - Epoch 10 - Batch 70/88 - Loss: 2.3473\n",
            "Model 19 - Epoch 10 - Batch 80/88 - Loss: 2.3324\n",
            "Model 19 - Finished Epoch 10\n",
            "Model 19 done training for 10 epochs in generation 5\n",
            "Model 20 - Starting Epoch 1\n",
            "Model 20 - Epoch 1 - Batch 10/88 - Loss: 2.5631\n",
            "Model 20 - Epoch 1 - Batch 20/88 - Loss: 2.6573\n",
            "Model 20 - Epoch 1 - Batch 30/88 - Loss: 2.6081\n",
            "Model 20 - Epoch 1 - Batch 40/88 - Loss: 2.4995\n",
            "Model 20 - Epoch 1 - Batch 50/88 - Loss: 2.5839\n",
            "Model 20 - Epoch 1 - Batch 60/88 - Loss: 2.6093\n",
            "Model 20 - Epoch 1 - Batch 70/88 - Loss: 2.7212\n",
            "Model 20 - Epoch 1 - Batch 80/88 - Loss: 2.5418\n",
            "Model 20 - Finished Epoch 1\n",
            "Model 20 - Starting Epoch 2\n",
            "Model 20 - Epoch 2 - Batch 10/88 - Loss: 2.5418\n",
            "Model 20 - Epoch 2 - Batch 20/88 - Loss: 2.5635\n",
            "Model 20 - Epoch 2 - Batch 30/88 - Loss: 2.5112\n",
            "Model 20 - Epoch 2 - Batch 40/88 - Loss: 2.4169\n",
            "Model 20 - Epoch 2 - Batch 50/88 - Loss: 2.5611\n",
            "Model 20 - Epoch 2 - Batch 60/88 - Loss: 2.5437\n",
            "Model 20 - Epoch 2 - Batch 70/88 - Loss: 2.5515\n",
            "Model 20 - Epoch 2 - Batch 80/88 - Loss: 2.7385\n",
            "Model 20 - Finished Epoch 2\n",
            "Model 20 - Starting Epoch 3\n",
            "Model 20 - Epoch 3 - Batch 10/88 - Loss: 2.5313\n",
            "Model 20 - Epoch 3 - Batch 20/88 - Loss: 2.5373\n",
            "Model 20 - Epoch 3 - Batch 30/88 - Loss: 2.5480\n",
            "Model 20 - Epoch 3 - Batch 40/88 - Loss: 2.5253\n",
            "Model 20 - Epoch 3 - Batch 50/88 - Loss: 2.6365\n",
            "Model 20 - Epoch 3 - Batch 60/88 - Loss: 2.5183\n",
            "Model 20 - Epoch 3 - Batch 70/88 - Loss: 2.5063\n",
            "Model 20 - Epoch 3 - Batch 80/88 - Loss: 2.5984\n",
            "Model 20 - Finished Epoch 3\n",
            "Model 20 - Starting Epoch 4\n",
            "Model 20 - Epoch 4 - Batch 10/88 - Loss: 2.4844\n",
            "Model 20 - Epoch 4 - Batch 20/88 - Loss: 2.4775\n",
            "Model 20 - Epoch 4 - Batch 30/88 - Loss: 2.4543\n",
            "Model 20 - Epoch 4 - Batch 40/88 - Loss: 2.4534\n",
            "Model 20 - Epoch 4 - Batch 50/88 - Loss: 2.5158\n",
            "Model 20 - Epoch 4 - Batch 60/88 - Loss: 2.5552\n",
            "Model 20 - Epoch 4 - Batch 70/88 - Loss: 2.4838\n",
            "Model 20 - Epoch 4 - Batch 80/88 - Loss: 2.5085\n",
            "Model 20 - Finished Epoch 4\n",
            "Model 20 - Starting Epoch 5\n",
            "Model 20 - Epoch 5 - Batch 10/88 - Loss: 2.3737\n",
            "Model 20 - Epoch 5 - Batch 20/88 - Loss: 2.3907\n",
            "Model 20 - Epoch 5 - Batch 30/88 - Loss: 2.3116\n",
            "Model 20 - Epoch 5 - Batch 40/88 - Loss: 2.4799\n",
            "Model 20 - Epoch 5 - Batch 50/88 - Loss: 2.5083\n",
            "Model 20 - Epoch 5 - Batch 60/88 - Loss: 2.4871\n",
            "Model 20 - Epoch 5 - Batch 70/88 - Loss: 2.5498\n",
            "Model 20 - Epoch 5 - Batch 80/88 - Loss: 2.5946\n",
            "Model 20 - Finished Epoch 5\n",
            "Model 20 - Starting Epoch 6\n",
            "Model 20 - Epoch 6 - Batch 10/88 - Loss: 2.4585\n",
            "Model 20 - Epoch 6 - Batch 20/88 - Loss: 2.4910\n",
            "Model 20 - Epoch 6 - Batch 30/88 - Loss: 2.5024\n",
            "Model 20 - Epoch 6 - Batch 40/88 - Loss: 2.4236\n",
            "Model 20 - Epoch 6 - Batch 50/88 - Loss: 2.4078\n",
            "Model 20 - Epoch 6 - Batch 60/88 - Loss: 2.4627\n",
            "Model 20 - Epoch 6 - Batch 70/88 - Loss: 2.4595\n",
            "Model 20 - Epoch 6 - Batch 80/88 - Loss: 2.4917\n",
            "Model 20 - Finished Epoch 6\n",
            "Model 20 - Starting Epoch 7\n",
            "Model 20 - Epoch 7 - Batch 10/88 - Loss: 2.4031\n",
            "Model 20 - Epoch 7 - Batch 20/88 - Loss: 2.3252\n",
            "Model 20 - Epoch 7 - Batch 30/88 - Loss: 2.3393\n",
            "Model 20 - Epoch 7 - Batch 40/88 - Loss: 2.4927\n",
            "Model 20 - Epoch 7 - Batch 50/88 - Loss: 2.5533\n",
            "Model 20 - Epoch 7 - Batch 60/88 - Loss: 2.4429\n",
            "Model 20 - Epoch 7 - Batch 70/88 - Loss: 2.5053\n",
            "Model 20 - Epoch 7 - Batch 80/88 - Loss: 2.4561\n",
            "Model 20 - Finished Epoch 7\n",
            "Model 20 - Starting Epoch 8\n",
            "Model 20 - Epoch 8 - Batch 10/88 - Loss: 2.4123\n",
            "Model 20 - Epoch 8 - Batch 20/88 - Loss: 2.3997\n",
            "Model 20 - Epoch 8 - Batch 30/88 - Loss: 2.5428\n",
            "Model 20 - Epoch 8 - Batch 40/88 - Loss: 2.5434\n",
            "Model 20 - Epoch 8 - Batch 50/88 - Loss: 2.4699\n",
            "Model 20 - Epoch 8 - Batch 60/88 - Loss: 2.3360\n",
            "Model 20 - Epoch 8 - Batch 70/88 - Loss: 2.4088\n",
            "Model 20 - Epoch 8 - Batch 80/88 - Loss: 2.4267\n",
            "Model 20 - Finished Epoch 8\n",
            "Model 20 - Starting Epoch 9\n",
            "Model 20 - Epoch 9 - Batch 10/88 - Loss: 2.4459\n",
            "Model 20 - Epoch 9 - Batch 20/88 - Loss: 2.2906\n",
            "Model 20 - Epoch 9 - Batch 30/88 - Loss: 2.4217\n",
            "Model 20 - Epoch 9 - Batch 40/88 - Loss: 2.5080\n",
            "Model 20 - Epoch 9 - Batch 50/88 - Loss: 2.4420\n",
            "Model 20 - Epoch 9 - Batch 60/88 - Loss: 2.2886\n",
            "Model 20 - Epoch 9 - Batch 70/88 - Loss: 2.4253\n",
            "Model 20 - Epoch 9 - Batch 80/88 - Loss: 2.3522\n",
            "Model 20 - Finished Epoch 9\n",
            "Model 20 - Starting Epoch 10\n",
            "Model 20 - Epoch 10 - Batch 10/88 - Loss: 2.3604\n",
            "Model 20 - Epoch 10 - Batch 20/88 - Loss: 2.4130\n",
            "Model 20 - Epoch 10 - Batch 30/88 - Loss: 2.4769\n",
            "Model 20 - Epoch 10 - Batch 40/88 - Loss: 2.3636\n",
            "Model 20 - Epoch 10 - Batch 50/88 - Loss: 2.2922\n",
            "Model 20 - Epoch 10 - Batch 60/88 - Loss: 2.3296\n",
            "Model 20 - Epoch 10 - Batch 70/88 - Loss: 2.5115\n",
            "Model 20 - Epoch 10 - Batch 80/88 - Loss: 2.3152\n",
            "Model 20 - Finished Epoch 10\n",
            "Model 20 done training for 10 epochs in generation 5\n",
            "Model 21 - Starting Epoch 1\n",
            "Model 21 - Epoch 1 - Batch 10/88 - Loss: 2.3286\n",
            "Model 21 - Epoch 1 - Batch 20/88 - Loss: 2.5088\n",
            "Model 21 - Epoch 1 - Batch 30/88 - Loss: 2.4552\n",
            "Model 21 - Epoch 1 - Batch 40/88 - Loss: 2.3261\n",
            "Model 21 - Epoch 1 - Batch 50/88 - Loss: 2.3096\n",
            "Model 21 - Epoch 1 - Batch 60/88 - Loss: 2.4751\n",
            "Model 21 - Epoch 1 - Batch 70/88 - Loss: 2.4845\n",
            "Model 21 - Epoch 1 - Batch 80/88 - Loss: 2.3247\n",
            "Model 21 - Finished Epoch 1\n",
            "Model 21 - Starting Epoch 2\n",
            "Model 21 - Epoch 2 - Batch 10/88 - Loss: 2.5637\n",
            "Model 21 - Epoch 2 - Batch 20/88 - Loss: 2.2685\n",
            "Model 21 - Epoch 2 - Batch 30/88 - Loss: 2.3687\n",
            "Model 21 - Epoch 2 - Batch 40/88 - Loss: 2.4474\n",
            "Model 21 - Epoch 2 - Batch 50/88 - Loss: 2.3709\n",
            "Model 21 - Epoch 2 - Batch 60/88 - Loss: 2.4241\n",
            "Model 21 - Epoch 2 - Batch 70/88 - Loss: 2.4044\n",
            "Model 21 - Epoch 2 - Batch 80/88 - Loss: 2.3275\n",
            "Model 21 - Finished Epoch 2\n",
            "Model 21 - Starting Epoch 3\n",
            "Model 21 - Epoch 3 - Batch 10/88 - Loss: 2.3519\n",
            "Model 21 - Epoch 3 - Batch 20/88 - Loss: 2.3340\n",
            "Model 21 - Epoch 3 - Batch 30/88 - Loss: 2.4267\n",
            "Model 21 - Epoch 3 - Batch 40/88 - Loss: 2.3808\n",
            "Model 21 - Epoch 3 - Batch 50/88 - Loss: 2.3624\n",
            "Model 21 - Epoch 3 - Batch 60/88 - Loss: 2.3154\n",
            "Model 21 - Epoch 3 - Batch 70/88 - Loss: 2.3972\n",
            "Model 21 - Epoch 3 - Batch 80/88 - Loss: 2.3816\n",
            "Model 21 - Finished Epoch 3\n",
            "Model 21 - Starting Epoch 4\n",
            "Model 21 - Epoch 4 - Batch 10/88 - Loss: 2.3388\n",
            "Model 21 - Epoch 4 - Batch 20/88 - Loss: 2.2519\n",
            "Model 21 - Epoch 4 - Batch 30/88 - Loss: 2.3476\n",
            "Model 21 - Epoch 4 - Batch 40/88 - Loss: 2.2719\n",
            "Model 21 - Epoch 4 - Batch 50/88 - Loss: 2.3016\n",
            "Model 21 - Epoch 4 - Batch 60/88 - Loss: 2.4153\n",
            "Model 21 - Epoch 4 - Batch 70/88 - Loss: 2.4710\n",
            "Model 21 - Epoch 4 - Batch 80/88 - Loss: 2.3356\n",
            "Model 21 - Finished Epoch 4\n",
            "Model 21 - Starting Epoch 5\n",
            "Model 21 - Epoch 5 - Batch 10/88 - Loss: 2.2625\n",
            "Model 21 - Epoch 5 - Batch 20/88 - Loss: 2.3856\n",
            "Model 21 - Epoch 5 - Batch 30/88 - Loss: 2.3573\n",
            "Model 21 - Epoch 5 - Batch 40/88 - Loss: 2.3186\n",
            "Model 21 - Epoch 5 - Batch 50/88 - Loss: 2.3473\n",
            "Model 21 - Epoch 5 - Batch 60/88 - Loss: 2.3831\n",
            "Model 21 - Epoch 5 - Batch 70/88 - Loss: 2.3961\n",
            "Model 21 - Epoch 5 - Batch 80/88 - Loss: 2.4043\n",
            "Model 21 - Finished Epoch 5\n",
            "Model 21 - Starting Epoch 6\n",
            "Model 21 - Epoch 6 - Batch 10/88 - Loss: 2.3193\n",
            "Model 21 - Epoch 6 - Batch 20/88 - Loss: 2.2768\n",
            "Model 21 - Epoch 6 - Batch 30/88 - Loss: 2.2403\n",
            "Model 21 - Epoch 6 - Batch 40/88 - Loss: 2.3765\n",
            "Model 21 - Epoch 6 - Batch 50/88 - Loss: 2.3801\n",
            "Model 21 - Epoch 6 - Batch 60/88 - Loss: 2.2875\n",
            "Model 21 - Epoch 6 - Batch 70/88 - Loss: 2.2426\n",
            "Model 21 - Epoch 6 - Batch 80/88 - Loss: 2.2676\n",
            "Model 21 - Finished Epoch 6\n",
            "Model 21 - Starting Epoch 7\n",
            "Model 21 - Epoch 7 - Batch 10/88 - Loss: 2.2853\n",
            "Model 21 - Epoch 7 - Batch 20/88 - Loss: 2.2927\n",
            "Model 21 - Epoch 7 - Batch 30/88 - Loss: 2.3200\n",
            "Model 21 - Epoch 7 - Batch 40/88 - Loss: 2.3282\n",
            "Model 21 - Epoch 7 - Batch 50/88 - Loss: 2.3032\n",
            "Model 21 - Epoch 7 - Batch 60/88 - Loss: 2.3136\n",
            "Model 21 - Epoch 7 - Batch 70/88 - Loss: 2.3413\n",
            "Model 21 - Epoch 7 - Batch 80/88 - Loss: 2.2392\n",
            "Model 21 - Finished Epoch 7\n",
            "Model 21 - Starting Epoch 8\n",
            "Model 21 - Epoch 8 - Batch 10/88 - Loss: 2.2966\n",
            "Model 21 - Epoch 8 - Batch 20/88 - Loss: 2.3342\n",
            "Model 21 - Epoch 8 - Batch 30/88 - Loss: 2.1771\n",
            "Model 21 - Epoch 8 - Batch 40/88 - Loss: 2.4192\n",
            "Model 21 - Epoch 8 - Batch 50/88 - Loss: 2.2483\n",
            "Model 21 - Epoch 8 - Batch 60/88 - Loss: 2.1364\n",
            "Model 21 - Epoch 8 - Batch 70/88 - Loss: 2.2731\n",
            "Model 21 - Epoch 8 - Batch 80/88 - Loss: 2.1864\n",
            "Model 21 - Finished Epoch 8\n",
            "Model 21 - Starting Epoch 9\n",
            "Model 21 - Epoch 9 - Batch 10/88 - Loss: 2.2112\n",
            "Model 21 - Epoch 9 - Batch 20/88 - Loss: 2.3401\n",
            "Model 21 - Epoch 9 - Batch 30/88 - Loss: 2.2054\n",
            "Model 21 - Epoch 9 - Batch 40/88 - Loss: 2.2064\n",
            "Model 21 - Epoch 9 - Batch 50/88 - Loss: 2.1415\n",
            "Model 21 - Epoch 9 - Batch 60/88 - Loss: 2.3045\n",
            "Model 21 - Epoch 9 - Batch 70/88 - Loss: 2.2053\n",
            "Model 21 - Epoch 9 - Batch 80/88 - Loss: 2.2998\n",
            "Model 21 - Finished Epoch 9\n",
            "Model 21 - Starting Epoch 10\n",
            "Model 21 - Epoch 10 - Batch 10/88 - Loss: 2.2215\n",
            "Model 21 - Epoch 10 - Batch 20/88 - Loss: 2.2822\n",
            "Model 21 - Epoch 10 - Batch 30/88 - Loss: 2.2711\n",
            "Model 21 - Epoch 10 - Batch 40/88 - Loss: 2.3316\n",
            "Model 21 - Epoch 10 - Batch 50/88 - Loss: 2.2871\n",
            "Model 21 - Epoch 10 - Batch 60/88 - Loss: 2.2915\n",
            "Model 21 - Epoch 10 - Batch 70/88 - Loss: 2.1715\n",
            "Model 21 - Epoch 10 - Batch 80/88 - Loss: 2.3049\n",
            "Model 21 - Finished Epoch 10\n",
            "Model 21 done training for 10 epochs in generation 5\n",
            "Model 22 - Starting Epoch 1\n",
            "Model 22 - Epoch 1 - Batch 10/88 - Loss: 2.1247\n",
            "Model 22 - Epoch 1 - Batch 20/88 - Loss: 2.0992\n",
            "Model 22 - Epoch 1 - Batch 30/88 - Loss: 2.2776\n",
            "Model 22 - Epoch 1 - Batch 40/88 - Loss: 2.1527\n",
            "Model 22 - Epoch 1 - Batch 50/88 - Loss: 2.2069\n",
            "Model 22 - Epoch 1 - Batch 60/88 - Loss: 2.2283\n",
            "Model 22 - Epoch 1 - Batch 70/88 - Loss: 2.2526\n",
            "Model 22 - Epoch 1 - Batch 80/88 - Loss: 2.2712\n",
            "Model 22 - Finished Epoch 1\n",
            "Model 22 - Starting Epoch 2\n",
            "Model 22 - Epoch 2 - Batch 10/88 - Loss: 2.1103\n",
            "Model 22 - Epoch 2 - Batch 20/88 - Loss: 2.1271\n",
            "Model 22 - Epoch 2 - Batch 30/88 - Loss: 2.1107\n",
            "Model 22 - Epoch 2 - Batch 40/88 - Loss: 2.1042\n",
            "Model 22 - Epoch 2 - Batch 50/88 - Loss: 2.2444\n",
            "Model 22 - Epoch 2 - Batch 60/88 - Loss: 2.2347\n",
            "Model 22 - Epoch 2 - Batch 70/88 - Loss: 2.1835\n",
            "Model 22 - Epoch 2 - Batch 80/88 - Loss: 2.3063\n",
            "Model 22 - Finished Epoch 2\n",
            "Model 22 - Starting Epoch 3\n",
            "Model 22 - Epoch 3 - Batch 10/88 - Loss: 2.1227\n",
            "Model 22 - Epoch 3 - Batch 20/88 - Loss: 2.2340\n",
            "Model 22 - Epoch 3 - Batch 30/88 - Loss: 2.2014\n",
            "Model 22 - Epoch 3 - Batch 40/88 - Loss: 2.2176\n",
            "Model 22 - Epoch 3 - Batch 50/88 - Loss: 2.1751\n",
            "Model 22 - Epoch 3 - Batch 60/88 - Loss: 2.3003\n",
            "Model 22 - Epoch 3 - Batch 70/88 - Loss: 2.2148\n",
            "Model 22 - Epoch 3 - Batch 80/88 - Loss: 2.0970\n",
            "Model 22 - Finished Epoch 3\n",
            "Model 22 - Starting Epoch 4\n",
            "Model 22 - Epoch 4 - Batch 10/88 - Loss: 2.2339\n",
            "Model 22 - Epoch 4 - Batch 20/88 - Loss: 2.2205\n",
            "Model 22 - Epoch 4 - Batch 30/88 - Loss: 2.0712\n",
            "Model 22 - Epoch 4 - Batch 40/88 - Loss: 2.2261\n",
            "Model 22 - Epoch 4 - Batch 50/88 - Loss: 2.0726\n",
            "Model 22 - Epoch 4 - Batch 60/88 - Loss: 2.1442\n",
            "Model 22 - Epoch 4 - Batch 70/88 - Loss: 2.1313\n",
            "Model 22 - Epoch 4 - Batch 80/88 - Loss: 2.1959\n",
            "Model 22 - Finished Epoch 4\n",
            "Model 22 - Starting Epoch 5\n",
            "Model 22 - Epoch 5 - Batch 10/88 - Loss: 2.1304\n",
            "Model 22 - Epoch 5 - Batch 20/88 - Loss: 2.1189\n",
            "Model 22 - Epoch 5 - Batch 30/88 - Loss: 2.2027\n",
            "Model 22 - Epoch 5 - Batch 40/88 - Loss: 2.0834\n",
            "Model 22 - Epoch 5 - Batch 50/88 - Loss: 2.2159\n",
            "Model 22 - Epoch 5 - Batch 60/88 - Loss: 2.2343\n",
            "Model 22 - Epoch 5 - Batch 70/88 - Loss: 2.0638\n",
            "Model 22 - Epoch 5 - Batch 80/88 - Loss: 2.1468\n",
            "Model 22 - Finished Epoch 5\n",
            "Model 22 - Starting Epoch 6\n",
            "Model 22 - Epoch 6 - Batch 10/88 - Loss: 2.1583\n",
            "Model 22 - Epoch 6 - Batch 20/88 - Loss: 2.0154\n",
            "Model 22 - Epoch 6 - Batch 30/88 - Loss: 2.1195\n",
            "Model 22 - Epoch 6 - Batch 40/88 - Loss: 2.1606\n",
            "Model 22 - Epoch 6 - Batch 50/88 - Loss: 2.2372\n",
            "Model 22 - Epoch 6 - Batch 60/88 - Loss: 2.1790\n",
            "Model 22 - Epoch 6 - Batch 70/88 - Loss: 2.2371\n",
            "Model 22 - Epoch 6 - Batch 80/88 - Loss: 2.3010\n",
            "Model 22 - Finished Epoch 6\n",
            "Model 22 - Starting Epoch 7\n",
            "Model 22 - Epoch 7 - Batch 10/88 - Loss: 2.1295\n",
            "Model 22 - Epoch 7 - Batch 20/88 - Loss: 2.0539\n",
            "Model 22 - Epoch 7 - Batch 30/88 - Loss: 2.1146\n",
            "Model 22 - Epoch 7 - Batch 40/88 - Loss: 2.2257\n",
            "Model 22 - Epoch 7 - Batch 50/88 - Loss: 2.2585\n",
            "Model 22 - Epoch 7 - Batch 60/88 - Loss: 2.0501\n",
            "Model 22 - Epoch 7 - Batch 70/88 - Loss: 2.2212\n",
            "Model 22 - Epoch 7 - Batch 80/88 - Loss: 2.2551\n",
            "Model 22 - Finished Epoch 7\n",
            "Model 22 - Starting Epoch 8\n",
            "Model 22 - Epoch 8 - Batch 10/88 - Loss: 2.0544\n",
            "Model 22 - Epoch 8 - Batch 20/88 - Loss: 2.1601\n",
            "Model 22 - Epoch 8 - Batch 30/88 - Loss: 2.2059\n",
            "Model 22 - Epoch 8 - Batch 40/88 - Loss: 2.1535\n",
            "Model 22 - Epoch 8 - Batch 50/88 - Loss: 2.1882\n",
            "Model 22 - Epoch 8 - Batch 60/88 - Loss: 2.1125\n",
            "Model 22 - Epoch 8 - Batch 70/88 - Loss: 2.1160\n",
            "Model 22 - Epoch 8 - Batch 80/88 - Loss: 2.0699\n",
            "Model 22 - Finished Epoch 8\n",
            "Model 22 - Starting Epoch 9\n",
            "Model 22 - Epoch 9 - Batch 10/88 - Loss: 2.1162\n",
            "Model 22 - Epoch 9 - Batch 20/88 - Loss: 2.1501\n",
            "Model 22 - Epoch 9 - Batch 30/88 - Loss: 2.0956\n",
            "Model 22 - Epoch 9 - Batch 40/88 - Loss: 2.0720\n",
            "Model 22 - Epoch 9 - Batch 50/88 - Loss: 2.1310\n",
            "Model 22 - Epoch 9 - Batch 60/88 - Loss: 2.1270\n",
            "Model 22 - Epoch 9 - Batch 70/88 - Loss: 2.2187\n",
            "Model 22 - Epoch 9 - Batch 80/88 - Loss: 2.1247\n",
            "Model 22 - Finished Epoch 9\n",
            "Model 22 - Starting Epoch 10\n",
            "Model 22 - Epoch 10 - Batch 10/88 - Loss: 2.0127\n",
            "Model 22 - Epoch 10 - Batch 20/88 - Loss: 2.0142\n",
            "Model 22 - Epoch 10 - Batch 30/88 - Loss: 2.0421\n",
            "Model 22 - Epoch 10 - Batch 40/88 - Loss: 2.0692\n",
            "Model 22 - Epoch 10 - Batch 50/88 - Loss: 2.1909\n",
            "Model 22 - Epoch 10 - Batch 60/88 - Loss: 2.1117\n",
            "Model 22 - Epoch 10 - Batch 70/88 - Loss: 2.0904\n",
            "Model 22 - Epoch 10 - Batch 80/88 - Loss: 2.0671\n",
            "Model 22 - Finished Epoch 10\n",
            "Model 22 done training for 10 epochs in generation 5\n",
            "Model 23 - Starting Epoch 1\n",
            "Model 23 - Epoch 1 - Batch 10/88 - Loss: 2.3681\n",
            "Model 23 - Epoch 1 - Batch 20/88 - Loss: 2.5173\n",
            "Model 23 - Epoch 1 - Batch 30/88 - Loss: 2.4880\n",
            "Model 23 - Epoch 1 - Batch 40/88 - Loss: 2.4529\n",
            "Model 23 - Epoch 1 - Batch 50/88 - Loss: 2.6664\n",
            "Model 23 - Epoch 1 - Batch 60/88 - Loss: 2.4101\n",
            "Model 23 - Epoch 1 - Batch 70/88 - Loss: 2.4004\n",
            "Model 23 - Epoch 1 - Batch 80/88 - Loss: 2.5507\n",
            "Model 23 - Finished Epoch 1\n",
            "Model 23 - Starting Epoch 2\n",
            "Model 23 - Epoch 2 - Batch 10/88 - Loss: 2.3739\n",
            "Model 23 - Epoch 2 - Batch 20/88 - Loss: 2.3993\n",
            "Model 23 - Epoch 2 - Batch 30/88 - Loss: 2.3426\n",
            "Model 23 - Epoch 2 - Batch 40/88 - Loss: 2.3451\n",
            "Model 23 - Epoch 2 - Batch 50/88 - Loss: 2.5799\n",
            "Model 23 - Epoch 2 - Batch 60/88 - Loss: 2.4923\n",
            "Model 23 - Epoch 2 - Batch 70/88 - Loss: 2.4757\n",
            "Model 23 - Epoch 2 - Batch 80/88 - Loss: 2.5607\n",
            "Model 23 - Finished Epoch 2\n",
            "Model 23 - Starting Epoch 3\n",
            "Model 23 - Epoch 3 - Batch 10/88 - Loss: 2.4919\n",
            "Model 23 - Epoch 3 - Batch 20/88 - Loss: 2.3891\n",
            "Model 23 - Epoch 3 - Batch 30/88 - Loss: 2.3948\n",
            "Model 23 - Epoch 3 - Batch 40/88 - Loss: 2.3834\n",
            "Model 23 - Epoch 3 - Batch 50/88 - Loss: 2.4122\n",
            "Model 23 - Epoch 3 - Batch 60/88 - Loss: 2.5266\n",
            "Model 23 - Epoch 3 - Batch 70/88 - Loss: 2.4786\n",
            "Model 23 - Epoch 3 - Batch 80/88 - Loss: 2.5005\n",
            "Model 23 - Finished Epoch 3\n",
            "Model 23 - Starting Epoch 4\n",
            "Model 23 - Epoch 4 - Batch 10/88 - Loss: 2.4332\n",
            "Model 23 - Epoch 4 - Batch 20/88 - Loss: 2.5467\n",
            "Model 23 - Epoch 4 - Batch 30/88 - Loss: 2.4566\n",
            "Model 23 - Epoch 4 - Batch 40/88 - Loss: 2.3468\n",
            "Model 23 - Epoch 4 - Batch 50/88 - Loss: 2.3800\n",
            "Model 23 - Epoch 4 - Batch 60/88 - Loss: 2.4129\n",
            "Model 23 - Epoch 4 - Batch 70/88 - Loss: 2.3803\n",
            "Model 23 - Epoch 4 - Batch 80/88 - Loss: 2.3547\n",
            "Model 23 - Finished Epoch 4\n",
            "Model 23 - Starting Epoch 5\n",
            "Model 23 - Epoch 5 - Batch 10/88 - Loss: 2.4753\n",
            "Model 23 - Epoch 5 - Batch 20/88 - Loss: 2.3985\n",
            "Model 23 - Epoch 5 - Batch 30/88 - Loss: 2.4492\n",
            "Model 23 - Epoch 5 - Batch 40/88 - Loss: 2.3334\n",
            "Model 23 - Epoch 5 - Batch 50/88 - Loss: 2.3534\n",
            "Model 23 - Epoch 5 - Batch 60/88 - Loss: 2.4099\n",
            "Model 23 - Epoch 5 - Batch 70/88 - Loss: 2.3464\n",
            "Model 23 - Epoch 5 - Batch 80/88 - Loss: 2.4078\n",
            "Model 23 - Finished Epoch 5\n",
            "Model 23 - Starting Epoch 6\n",
            "Model 23 - Epoch 6 - Batch 10/88 - Loss: 2.2760\n",
            "Model 23 - Epoch 6 - Batch 20/88 - Loss: 2.3608\n",
            "Model 23 - Epoch 6 - Batch 30/88 - Loss: 2.2470\n",
            "Model 23 - Epoch 6 - Batch 40/88 - Loss: 2.4118\n",
            "Model 23 - Epoch 6 - Batch 50/88 - Loss: 2.3491\n",
            "Model 23 - Epoch 6 - Batch 60/88 - Loss: 2.3295\n",
            "Model 23 - Epoch 6 - Batch 70/88 - Loss: 2.4019\n",
            "Model 23 - Epoch 6 - Batch 80/88 - Loss: 2.4966\n",
            "Model 23 - Finished Epoch 6\n",
            "Model 23 - Starting Epoch 7\n",
            "Model 23 - Epoch 7 - Batch 10/88 - Loss: 2.2387\n",
            "Model 23 - Epoch 7 - Batch 20/88 - Loss: 2.3908\n",
            "Model 23 - Epoch 7 - Batch 30/88 - Loss: 2.2658\n",
            "Model 23 - Epoch 7 - Batch 40/88 - Loss: 2.4934\n",
            "Model 23 - Epoch 7 - Batch 50/88 - Loss: 2.4858\n",
            "Model 23 - Epoch 7 - Batch 60/88 - Loss: 2.3040\n",
            "Model 23 - Epoch 7 - Batch 70/88 - Loss: 2.3305\n",
            "Model 23 - Epoch 7 - Batch 80/88 - Loss: 2.4865\n",
            "Model 23 - Finished Epoch 7\n",
            "Model 23 - Starting Epoch 8\n",
            "Model 23 - Epoch 8 - Batch 10/88 - Loss: 2.4393\n",
            "Model 23 - Epoch 8 - Batch 20/88 - Loss: 2.2864\n",
            "Model 23 - Epoch 8 - Batch 30/88 - Loss: 2.2820\n",
            "Model 23 - Epoch 8 - Batch 40/88 - Loss: 2.2892\n",
            "Model 23 - Epoch 8 - Batch 50/88 - Loss: 2.4338\n",
            "Model 23 - Epoch 8 - Batch 60/88 - Loss: 2.3332\n",
            "Model 23 - Epoch 8 - Batch 70/88 - Loss: 2.4671\n",
            "Model 23 - Epoch 8 - Batch 80/88 - Loss: 2.2952\n",
            "Model 23 - Finished Epoch 8\n",
            "Model 23 - Starting Epoch 9\n",
            "Model 23 - Epoch 9 - Batch 10/88 - Loss: 2.2945\n",
            "Model 23 - Epoch 9 - Batch 20/88 - Loss: 2.3549\n",
            "Model 23 - Epoch 9 - Batch 30/88 - Loss: 2.2724\n",
            "Model 23 - Epoch 9 - Batch 40/88 - Loss: 2.3692\n",
            "Model 23 - Epoch 9 - Batch 50/88 - Loss: 2.4028\n",
            "Model 23 - Epoch 9 - Batch 60/88 - Loss: 2.2918\n",
            "Model 23 - Epoch 9 - Batch 70/88 - Loss: 2.3747\n",
            "Model 23 - Epoch 9 - Batch 80/88 - Loss: 2.2796\n",
            "Model 23 - Finished Epoch 9\n",
            "Model 23 - Starting Epoch 10\n",
            "Model 23 - Epoch 10 - Batch 10/88 - Loss: 2.3672\n",
            "Model 23 - Epoch 10 - Batch 20/88 - Loss: 2.3594\n",
            "Model 23 - Epoch 10 - Batch 30/88 - Loss: 2.3896\n",
            "Model 23 - Epoch 10 - Batch 40/88 - Loss: 2.2432\n",
            "Model 23 - Epoch 10 - Batch 50/88 - Loss: 2.2632\n",
            "Model 23 - Epoch 10 - Batch 60/88 - Loss: 2.3317\n",
            "Model 23 - Epoch 10 - Batch 70/88 - Loss: 2.2501\n",
            "Model 23 - Epoch 10 - Batch 80/88 - Loss: 2.4256\n",
            "Model 23 - Finished Epoch 10\n",
            "Model 23 done training for 10 epochs in generation 5\n",
            "Model 24 - Starting Epoch 1\n",
            "Model 24 - Epoch 1 - Batch 10/88 - Loss: 2.3195\n",
            "Model 24 - Epoch 1 - Batch 20/88 - Loss: 2.3528\n",
            "Model 24 - Epoch 1 - Batch 30/88 - Loss: 2.2839\n",
            "Model 24 - Epoch 1 - Batch 40/88 - Loss: 2.4197\n",
            "Model 24 - Epoch 1 - Batch 50/88 - Loss: 2.5505\n",
            "Model 24 - Epoch 1 - Batch 60/88 - Loss: 2.5139\n",
            "Model 24 - Epoch 1 - Batch 70/88 - Loss: 2.4302\n",
            "Model 24 - Epoch 1 - Batch 80/88 - Loss: 2.4093\n",
            "Model 24 - Finished Epoch 1\n",
            "Model 24 - Starting Epoch 2\n",
            "Model 24 - Epoch 2 - Batch 10/88 - Loss: 2.3474\n",
            "Model 24 - Epoch 2 - Batch 20/88 - Loss: 2.3353\n",
            "Model 24 - Epoch 2 - Batch 30/88 - Loss: 2.2185\n",
            "Model 24 - Epoch 2 - Batch 40/88 - Loss: 2.2348\n",
            "Model 24 - Epoch 2 - Batch 50/88 - Loss: 2.4671\n",
            "Model 24 - Epoch 2 - Batch 60/88 - Loss: 2.3821\n",
            "Model 24 - Epoch 2 - Batch 70/88 - Loss: 2.4910\n",
            "Model 24 - Epoch 2 - Batch 80/88 - Loss: 2.3094\n",
            "Model 24 - Finished Epoch 2\n",
            "Model 24 - Starting Epoch 3\n",
            "Model 24 - Epoch 3 - Batch 10/88 - Loss: 2.2740\n",
            "Model 24 - Epoch 3 - Batch 20/88 - Loss: 2.4130\n",
            "Model 24 - Epoch 3 - Batch 30/88 - Loss: 2.2456\n",
            "Model 24 - Epoch 3 - Batch 40/88 - Loss: 2.3682\n",
            "Model 24 - Epoch 3 - Batch 50/88 - Loss: 2.4252\n",
            "Model 24 - Epoch 3 - Batch 60/88 - Loss: 2.4166\n",
            "Model 24 - Epoch 3 - Batch 70/88 - Loss: 2.4452\n",
            "Model 24 - Epoch 3 - Batch 80/88 - Loss: 2.3822\n",
            "Model 24 - Finished Epoch 3\n",
            "Model 24 - Starting Epoch 4\n",
            "Model 24 - Epoch 4 - Batch 10/88 - Loss: 2.3013\n",
            "Model 24 - Epoch 4 - Batch 20/88 - Loss: 2.3965\n",
            "Model 24 - Epoch 4 - Batch 30/88 - Loss: 2.2446\n",
            "Model 24 - Epoch 4 - Batch 40/88 - Loss: 2.2978\n",
            "Model 24 - Epoch 4 - Batch 50/88 - Loss: 2.3913\n",
            "Model 24 - Epoch 4 - Batch 60/88 - Loss: 2.3927\n",
            "Model 24 - Epoch 4 - Batch 70/88 - Loss: 2.3039\n",
            "Model 24 - Epoch 4 - Batch 80/88 - Loss: 2.3192\n",
            "Model 24 - Finished Epoch 4\n",
            "Model 24 - Starting Epoch 5\n",
            "Model 24 - Epoch 5 - Batch 10/88 - Loss: 2.2485\n",
            "Model 24 - Epoch 5 - Batch 20/88 - Loss: 2.1865\n",
            "Model 24 - Epoch 5 - Batch 30/88 - Loss: 2.4419\n",
            "Model 24 - Epoch 5 - Batch 40/88 - Loss: 2.2486\n",
            "Model 24 - Epoch 5 - Batch 50/88 - Loss: 2.3369\n",
            "Model 24 - Epoch 5 - Batch 60/88 - Loss: 2.2697\n",
            "Model 24 - Epoch 5 - Batch 70/88 - Loss: 2.4765\n",
            "Model 24 - Epoch 5 - Batch 80/88 - Loss: 2.3632\n",
            "Model 24 - Finished Epoch 5\n",
            "Model 24 - Starting Epoch 6\n",
            "Model 24 - Epoch 6 - Batch 10/88 - Loss: 2.2620\n",
            "Model 24 - Epoch 6 - Batch 20/88 - Loss: 2.3626\n",
            "Model 24 - Epoch 6 - Batch 30/88 - Loss: 2.1673\n",
            "Model 24 - Epoch 6 - Batch 40/88 - Loss: 2.3766\n",
            "Model 24 - Epoch 6 - Batch 50/88 - Loss: 2.2277\n",
            "Model 24 - Epoch 6 - Batch 60/88 - Loss: 2.3412\n",
            "Model 24 - Epoch 6 - Batch 70/88 - Loss: 2.2413\n",
            "Model 24 - Epoch 6 - Batch 80/88 - Loss: 2.2108\n",
            "Model 24 - Finished Epoch 6\n",
            "Model 24 - Starting Epoch 7\n",
            "Model 24 - Epoch 7 - Batch 10/88 - Loss: 2.3904\n",
            "Model 24 - Epoch 7 - Batch 20/88 - Loss: 2.1556\n",
            "Model 24 - Epoch 7 - Batch 30/88 - Loss: 2.2027\n",
            "Model 24 - Epoch 7 - Batch 40/88 - Loss: 2.3906\n",
            "Model 24 - Epoch 7 - Batch 50/88 - Loss: 2.3340\n",
            "Model 24 - Epoch 7 - Batch 60/88 - Loss: 2.4252\n",
            "Model 24 - Epoch 7 - Batch 70/88 - Loss: 2.3348\n",
            "Model 24 - Epoch 7 - Batch 80/88 - Loss: 2.3787\n",
            "Model 24 - Finished Epoch 7\n",
            "Model 24 - Starting Epoch 8\n",
            "Model 24 - Epoch 8 - Batch 10/88 - Loss: 2.2292\n",
            "Model 24 - Epoch 8 - Batch 20/88 - Loss: 2.2285\n",
            "Model 24 - Epoch 8 - Batch 30/88 - Loss: 2.2853\n",
            "Model 24 - Epoch 8 - Batch 40/88 - Loss: 2.1950\n",
            "Model 24 - Epoch 8 - Batch 50/88 - Loss: 2.2193\n",
            "Model 24 - Epoch 8 - Batch 60/88 - Loss: 2.3565\n",
            "Model 24 - Epoch 8 - Batch 70/88 - Loss: 2.1526\n",
            "Model 24 - Epoch 8 - Batch 80/88 - Loss: 2.4022\n",
            "Model 24 - Finished Epoch 8\n",
            "Model 24 - Starting Epoch 9\n",
            "Model 24 - Epoch 9 - Batch 10/88 - Loss: 2.1631\n",
            "Model 24 - Epoch 9 - Batch 20/88 - Loss: 2.1256\n",
            "Model 24 - Epoch 9 - Batch 30/88 - Loss: 2.1787\n",
            "Model 24 - Epoch 9 - Batch 40/88 - Loss: 2.3760\n",
            "Model 24 - Epoch 9 - Batch 50/88 - Loss: 2.2279\n",
            "Model 24 - Epoch 9 - Batch 60/88 - Loss: 2.2534\n",
            "Model 24 - Epoch 9 - Batch 70/88 - Loss: 2.2873\n",
            "Model 24 - Epoch 9 - Batch 80/88 - Loss: 2.3709\n",
            "Model 24 - Finished Epoch 9\n",
            "Model 24 - Starting Epoch 10\n",
            "Model 24 - Epoch 10 - Batch 10/88 - Loss: 2.2891\n",
            "Model 24 - Epoch 10 - Batch 20/88 - Loss: 2.2378\n",
            "Model 24 - Epoch 10 - Batch 30/88 - Loss: 2.1121\n",
            "Model 24 - Epoch 10 - Batch 40/88 - Loss: 2.2246\n",
            "Model 24 - Epoch 10 - Batch 50/88 - Loss: 2.2826\n",
            "Model 24 - Epoch 10 - Batch 60/88 - Loss: 2.2045\n",
            "Model 24 - Epoch 10 - Batch 70/88 - Loss: 2.1412\n",
            "Model 24 - Epoch 10 - Batch 80/88 - Loss: 2.3007\n",
            "Model 24 - Finished Epoch 10\n",
            "Model 24 done training for 10 epochs in generation 5\n",
            "Model 25 - Starting Epoch 1\n",
            "Model 25 - Epoch 1 - Batch 10/88 - Loss: 2.3029\n",
            "Model 25 - Epoch 1 - Batch 20/88 - Loss: 2.3542\n",
            "Model 25 - Epoch 1 - Batch 30/88 - Loss: 2.3369\n",
            "Model 25 - Epoch 1 - Batch 40/88 - Loss: 2.4476\n",
            "Model 25 - Epoch 1 - Batch 50/88 - Loss: 2.3672\n",
            "Model 25 - Epoch 1 - Batch 60/88 - Loss: 2.4268\n",
            "Model 25 - Epoch 1 - Batch 70/88 - Loss: 2.3437\n",
            "Model 25 - Epoch 1 - Batch 80/88 - Loss: 2.4345\n",
            "Model 25 - Finished Epoch 1\n",
            "Model 25 - Starting Epoch 2\n",
            "Model 25 - Epoch 2 - Batch 10/88 - Loss: 2.3159\n",
            "Model 25 - Epoch 2 - Batch 20/88 - Loss: 2.3232\n",
            "Model 25 - Epoch 2 - Batch 30/88 - Loss: 2.2800\n",
            "Model 25 - Epoch 2 - Batch 40/88 - Loss: 2.3546\n",
            "Model 25 - Epoch 2 - Batch 50/88 - Loss: 2.4479\n",
            "Model 25 - Epoch 2 - Batch 60/88 - Loss: 2.2597\n",
            "Model 25 - Epoch 2 - Batch 70/88 - Loss: 2.3295\n",
            "Model 25 - Epoch 2 - Batch 80/88 - Loss: 2.4246\n",
            "Model 25 - Finished Epoch 2\n",
            "Model 25 - Starting Epoch 3\n",
            "Model 25 - Epoch 3 - Batch 10/88 - Loss: 2.2996\n",
            "Model 25 - Epoch 3 - Batch 20/88 - Loss: 2.3160\n",
            "Model 25 - Epoch 3 - Batch 30/88 - Loss: 2.4375\n",
            "Model 25 - Epoch 3 - Batch 40/88 - Loss: 2.3300\n",
            "Model 25 - Epoch 3 - Batch 50/88 - Loss: 2.3277\n",
            "Model 25 - Epoch 3 - Batch 60/88 - Loss: 2.3274\n",
            "Model 25 - Epoch 3 - Batch 70/88 - Loss: 2.3186\n",
            "Model 25 - Epoch 3 - Batch 80/88 - Loss: 2.3881\n",
            "Model 25 - Finished Epoch 3\n",
            "Model 25 - Starting Epoch 4\n",
            "Model 25 - Epoch 4 - Batch 10/88 - Loss: 2.3935\n",
            "Model 25 - Epoch 4 - Batch 20/88 - Loss: 2.1255\n",
            "Model 25 - Epoch 4 - Batch 30/88 - Loss: 2.3750\n",
            "Model 25 - Epoch 4 - Batch 40/88 - Loss: 2.3188\n",
            "Model 25 - Epoch 4 - Batch 50/88 - Loss: 2.3755\n",
            "Model 25 - Epoch 4 - Batch 60/88 - Loss: 2.3510\n",
            "Model 25 - Epoch 4 - Batch 70/88 - Loss: 2.3785\n",
            "Model 25 - Epoch 4 - Batch 80/88 - Loss: 2.2208\n",
            "Model 25 - Finished Epoch 4\n",
            "Model 25 - Starting Epoch 5\n",
            "Model 25 - Epoch 5 - Batch 10/88 - Loss: 2.3414\n",
            "Model 25 - Epoch 5 - Batch 20/88 - Loss: 2.3042\n",
            "Model 25 - Epoch 5 - Batch 30/88 - Loss: 2.2965\n",
            "Model 25 - Epoch 5 - Batch 40/88 - Loss: 2.3163\n",
            "Model 25 - Epoch 5 - Batch 50/88 - Loss: 2.2462\n",
            "Model 25 - Epoch 5 - Batch 60/88 - Loss: 2.3883\n",
            "Model 25 - Epoch 5 - Batch 70/88 - Loss: 2.1600\n",
            "Model 25 - Epoch 5 - Batch 80/88 - Loss: 2.2831\n",
            "Model 25 - Finished Epoch 5\n",
            "Model 25 - Starting Epoch 6\n",
            "Model 25 - Epoch 6 - Batch 10/88 - Loss: 2.3446\n",
            "Model 25 - Epoch 6 - Batch 20/88 - Loss: 2.2704\n",
            "Model 25 - Epoch 6 - Batch 30/88 - Loss: 2.4089\n",
            "Model 25 - Epoch 6 - Batch 40/88 - Loss: 2.1976\n",
            "Model 25 - Epoch 6 - Batch 50/88 - Loss: 2.3039\n",
            "Model 25 - Epoch 6 - Batch 60/88 - Loss: 2.3167\n",
            "Model 25 - Epoch 6 - Batch 70/88 - Loss: 2.2610\n",
            "Model 25 - Epoch 6 - Batch 80/88 - Loss: 2.3176\n",
            "Model 25 - Finished Epoch 6\n",
            "Model 25 - Starting Epoch 7\n",
            "Model 25 - Epoch 7 - Batch 10/88 - Loss: 2.2068\n",
            "Model 25 - Epoch 7 - Batch 20/88 - Loss: 2.2751\n",
            "Model 25 - Epoch 7 - Batch 30/88 - Loss: 2.2091\n",
            "Model 25 - Epoch 7 - Batch 40/88 - Loss: 2.2191\n",
            "Model 25 - Epoch 7 - Batch 50/88 - Loss: 2.2996\n",
            "Model 25 - Epoch 7 - Batch 60/88 - Loss: 2.3929\n",
            "Model 25 - Epoch 7 - Batch 70/88 - Loss: 2.3531\n",
            "Model 25 - Epoch 7 - Batch 80/88 - Loss: 2.2520\n",
            "Model 25 - Finished Epoch 7\n",
            "Model 25 - Starting Epoch 8\n",
            "Model 25 - Epoch 8 - Batch 10/88 - Loss: 2.1771\n",
            "Model 25 - Epoch 8 - Batch 20/88 - Loss: 2.3733\n",
            "Model 25 - Epoch 8 - Batch 30/88 - Loss: 2.1722\n",
            "Model 25 - Epoch 8 - Batch 40/88 - Loss: 2.3024\n",
            "Model 25 - Epoch 8 - Batch 50/88 - Loss: 2.2889\n",
            "Model 25 - Epoch 8 - Batch 60/88 - Loss: 2.2080\n",
            "Model 25 - Epoch 8 - Batch 70/88 - Loss: 2.3274\n",
            "Model 25 - Epoch 8 - Batch 80/88 - Loss: 2.3630\n",
            "Model 25 - Finished Epoch 8\n",
            "Model 25 - Starting Epoch 9\n",
            "Model 25 - Epoch 9 - Batch 10/88 - Loss: 2.2882\n",
            "Model 25 - Epoch 9 - Batch 20/88 - Loss: 2.2654\n",
            "Model 25 - Epoch 9 - Batch 30/88 - Loss: 2.1600\n",
            "Model 25 - Epoch 9 - Batch 40/88 - Loss: 2.2575\n",
            "Model 25 - Epoch 9 - Batch 50/88 - Loss: 2.3663\n",
            "Model 25 - Epoch 9 - Batch 60/88 - Loss: 2.2594\n",
            "Model 25 - Epoch 9 - Batch 70/88 - Loss: 2.3391\n",
            "Model 25 - Epoch 9 - Batch 80/88 - Loss: 2.2368\n",
            "Model 25 - Finished Epoch 9\n",
            "Model 25 - Starting Epoch 10\n",
            "Model 25 - Epoch 10 - Batch 10/88 - Loss: 2.1467\n",
            "Model 25 - Epoch 10 - Batch 20/88 - Loss: 2.2279\n",
            "Model 25 - Epoch 10 - Batch 30/88 - Loss: 2.0759\n",
            "Model 25 - Epoch 10 - Batch 40/88 - Loss: 2.3384\n",
            "Model 25 - Epoch 10 - Batch 50/88 - Loss: 2.2388\n",
            "Model 25 - Epoch 10 - Batch 60/88 - Loss: 2.2547\n",
            "Model 25 - Epoch 10 - Batch 70/88 - Loss: 2.2837\n",
            "Model 25 - Epoch 10 - Batch 80/88 - Loss: 2.3055\n",
            "Model 25 - Finished Epoch 10\n",
            "Model 25 done training for 10 epochs in generation 5\n",
            "  🎉 New best model with accuracy: 0.6503\n",
            "  ✅ Best Acc This Gen: 0.6503 | Worst: 0.5545\n",
            "\n",
            "🌱 Generation 6 | Population size: 15\n",
            "Model 1 - Starting Epoch 1\n",
            "Model 1 - Epoch 1 - Batch 10/88 - Loss: 1.6686\n",
            "Model 1 - Epoch 1 - Batch 20/88 - Loss: 1.7131\n",
            "Model 1 - Epoch 1 - Batch 30/88 - Loss: 1.7561\n",
            "Model 1 - Epoch 1 - Batch 40/88 - Loss: 1.7791\n",
            "Model 1 - Epoch 1 - Batch 50/88 - Loss: 1.8284\n",
            "Model 1 - Epoch 1 - Batch 60/88 - Loss: 1.7281\n",
            "Model 1 - Epoch 1 - Batch 70/88 - Loss: 1.7568\n",
            "Model 1 - Epoch 1 - Batch 80/88 - Loss: 1.8708\n",
            "Model 1 - Finished Epoch 1\n",
            "Model 1 - Starting Epoch 2\n",
            "Model 1 - Epoch 2 - Batch 10/88 - Loss: 1.6541\n",
            "Model 1 - Epoch 2 - Batch 20/88 - Loss: 1.7726\n",
            "Model 1 - Epoch 2 - Batch 30/88 - Loss: 1.7340\n",
            "Model 1 - Epoch 2 - Batch 40/88 - Loss: 1.7270\n",
            "Model 1 - Epoch 2 - Batch 50/88 - Loss: 1.7261\n",
            "Model 1 - Epoch 2 - Batch 60/88 - Loss: 1.7690\n",
            "Model 1 - Epoch 2 - Batch 70/88 - Loss: 1.7791\n",
            "Model 1 - Epoch 2 - Batch 80/88 - Loss: 1.7546\n",
            "Model 1 - Finished Epoch 2\n",
            "Model 1 - Starting Epoch 3\n",
            "Model 1 - Epoch 3 - Batch 10/88 - Loss: 1.7301\n",
            "Model 1 - Epoch 3 - Batch 20/88 - Loss: 1.7437\n",
            "Model 1 - Epoch 3 - Batch 30/88 - Loss: 1.7356\n",
            "Model 1 - Epoch 3 - Batch 40/88 - Loss: 1.7654\n",
            "Model 1 - Epoch 3 - Batch 50/88 - Loss: 1.6829\n",
            "Model 1 - Epoch 3 - Batch 60/88 - Loss: 1.7155\n",
            "Model 1 - Epoch 3 - Batch 70/88 - Loss: 1.7331\n",
            "Model 1 - Epoch 3 - Batch 80/88 - Loss: 1.8304\n",
            "Model 1 - Finished Epoch 3\n",
            "Model 1 - Starting Epoch 4\n",
            "Model 1 - Epoch 4 - Batch 10/88 - Loss: 1.7292\n",
            "Model 1 - Epoch 4 - Batch 20/88 - Loss: 1.7500\n",
            "Model 1 - Epoch 4 - Batch 30/88 - Loss: 1.6784\n",
            "Model 1 - Epoch 4 - Batch 40/88 - Loss: 1.8057\n",
            "Model 1 - Epoch 4 - Batch 50/88 - Loss: 1.8127\n",
            "Model 1 - Epoch 4 - Batch 60/88 - Loss: 1.7915\n",
            "Model 1 - Epoch 4 - Batch 70/88 - Loss: 1.7185\n",
            "Model 1 - Epoch 4 - Batch 80/88 - Loss: 1.7469\n",
            "Model 1 - Finished Epoch 4\n",
            "Model 1 - Starting Epoch 5\n",
            "Model 1 - Epoch 5 - Batch 10/88 - Loss: 1.5929\n",
            "Model 1 - Epoch 5 - Batch 20/88 - Loss: 1.6137\n",
            "Model 1 - Epoch 5 - Batch 30/88 - Loss: 1.6280\n",
            "Model 1 - Epoch 5 - Batch 40/88 - Loss: 1.7605\n",
            "Model 1 - Epoch 5 - Batch 50/88 - Loss: 1.6674\n",
            "Model 1 - Epoch 5 - Batch 60/88 - Loss: 1.7005\n",
            "Model 1 - Epoch 5 - Batch 70/88 - Loss: 1.7864\n",
            "Model 1 - Epoch 5 - Batch 80/88 - Loss: 1.7125\n",
            "Model 1 - Finished Epoch 5\n",
            "Model 1 - Starting Epoch 6\n",
            "Model 1 - Epoch 6 - Batch 10/88 - Loss: 1.6136\n",
            "Model 1 - Epoch 6 - Batch 20/88 - Loss: 1.6015\n",
            "Model 1 - Epoch 6 - Batch 30/88 - Loss: 1.6101\n",
            "Model 1 - Epoch 6 - Batch 40/88 - Loss: 1.6198\n",
            "Model 1 - Epoch 6 - Batch 50/88 - Loss: 1.7064\n",
            "Model 1 - Epoch 6 - Batch 60/88 - Loss: 1.7757\n",
            "Model 1 - Epoch 6 - Batch 70/88 - Loss: 1.7444\n",
            "Model 1 - Epoch 6 - Batch 80/88 - Loss: 1.7182\n",
            "Model 1 - Finished Epoch 6\n",
            "Model 1 - Starting Epoch 7\n",
            "Model 1 - Epoch 7 - Batch 10/88 - Loss: 1.6859\n",
            "Model 1 - Epoch 7 - Batch 20/88 - Loss: 1.5626\n",
            "Model 1 - Epoch 7 - Batch 30/88 - Loss: 1.6659\n",
            "Model 1 - Epoch 7 - Batch 40/88 - Loss: 1.7116\n",
            "Model 1 - Epoch 7 - Batch 50/88 - Loss: 1.6468\n",
            "Model 1 - Epoch 7 - Batch 60/88 - Loss: 1.6493\n",
            "Model 1 - Epoch 7 - Batch 70/88 - Loss: 1.7022\n",
            "Model 1 - Epoch 7 - Batch 80/88 - Loss: 1.5684\n",
            "Model 1 - Finished Epoch 7\n",
            "Model 1 - Starting Epoch 8\n",
            "Model 1 - Epoch 8 - Batch 10/88 - Loss: 1.5801\n",
            "Model 1 - Epoch 8 - Batch 20/88 - Loss: 1.6217\n",
            "Model 1 - Epoch 8 - Batch 30/88 - Loss: 1.5627\n",
            "Model 1 - Epoch 8 - Batch 40/88 - Loss: 1.7533\n",
            "Model 1 - Epoch 8 - Batch 50/88 - Loss: 1.5820\n",
            "Model 1 - Epoch 8 - Batch 60/88 - Loss: 1.6337\n",
            "Model 1 - Epoch 8 - Batch 70/88 - Loss: 1.6617\n",
            "Model 1 - Epoch 8 - Batch 80/88 - Loss: 1.6766\n",
            "Model 1 - Finished Epoch 8\n",
            "Model 1 - Starting Epoch 9\n",
            "Model 1 - Epoch 9 - Batch 10/88 - Loss: 1.7004\n",
            "Model 1 - Epoch 9 - Batch 20/88 - Loss: 1.6882\n",
            "Model 1 - Epoch 9 - Batch 30/88 - Loss: 1.6156\n",
            "Model 1 - Epoch 9 - Batch 40/88 - Loss: 1.6177\n",
            "Model 1 - Epoch 9 - Batch 50/88 - Loss: 1.6479\n",
            "Model 1 - Epoch 9 - Batch 60/88 - Loss: 1.6613\n",
            "Model 1 - Epoch 9 - Batch 70/88 - Loss: 1.5475\n",
            "Model 1 - Epoch 9 - Batch 80/88 - Loss: 1.7118\n",
            "Model 1 - Finished Epoch 9\n",
            "Model 1 - Starting Epoch 10\n",
            "Model 1 - Epoch 10 - Batch 10/88 - Loss: 1.5952\n",
            "Model 1 - Epoch 10 - Batch 20/88 - Loss: 1.6572\n",
            "Model 1 - Epoch 10 - Batch 30/88 - Loss: 1.6472\n",
            "Model 1 - Epoch 10 - Batch 40/88 - Loss: 1.6635\n",
            "Model 1 - Epoch 10 - Batch 50/88 - Loss: 1.6210\n",
            "Model 1 - Epoch 10 - Batch 60/88 - Loss: 1.6164\n",
            "Model 1 - Epoch 10 - Batch 70/88 - Loss: 1.7447\n",
            "Model 1 - Epoch 10 - Batch 80/88 - Loss: 1.7102\n",
            "Model 1 - Finished Epoch 10\n",
            "Model 1 done training for 10 epochs in generation 6\n",
            "Model 2 - Starting Epoch 1\n",
            "Model 2 - Epoch 1 - Batch 10/88 - Loss: 2.0061\n",
            "Model 2 - Epoch 1 - Batch 20/88 - Loss: 2.1334\n",
            "Model 2 - Epoch 1 - Batch 30/88 - Loss: 2.0466\n",
            "Model 2 - Epoch 1 - Batch 40/88 - Loss: 1.9957\n",
            "Model 2 - Epoch 1 - Batch 50/88 - Loss: 2.1644\n",
            "Model 2 - Epoch 1 - Batch 60/88 - Loss: 2.0271\n",
            "Model 2 - Epoch 1 - Batch 70/88 - Loss: 2.2200\n",
            "Model 2 - Epoch 1 - Batch 80/88 - Loss: 2.1590\n",
            "Model 2 - Finished Epoch 1\n",
            "Model 2 - Starting Epoch 2\n",
            "Model 2 - Epoch 2 - Batch 10/88 - Loss: 1.9787\n",
            "Model 2 - Epoch 2 - Batch 20/88 - Loss: 2.1309\n",
            "Model 2 - Epoch 2 - Batch 30/88 - Loss: 1.9905\n",
            "Model 2 - Epoch 2 - Batch 40/88 - Loss: 2.0440\n",
            "Model 2 - Epoch 2 - Batch 50/88 - Loss: 2.1312\n",
            "Model 2 - Epoch 2 - Batch 60/88 - Loss: 2.0194\n",
            "Model 2 - Epoch 2 - Batch 70/88 - Loss: 2.0748\n",
            "Model 2 - Epoch 2 - Batch 80/88 - Loss: 1.9821\n",
            "Model 2 - Finished Epoch 2\n",
            "Model 2 - Starting Epoch 3\n",
            "Model 2 - Epoch 3 - Batch 10/88 - Loss: 2.0360\n",
            "Model 2 - Epoch 3 - Batch 20/88 - Loss: 2.0357\n",
            "Model 2 - Epoch 3 - Batch 30/88 - Loss: 2.0357\n",
            "Model 2 - Epoch 3 - Batch 40/88 - Loss: 1.9331\n",
            "Model 2 - Epoch 3 - Batch 50/88 - Loss: 1.9807\n",
            "Model 2 - Epoch 3 - Batch 60/88 - Loss: 2.0670\n",
            "Model 2 - Epoch 3 - Batch 70/88 - Loss: 2.0501\n",
            "Model 2 - Epoch 3 - Batch 80/88 - Loss: 2.1045\n",
            "Model 2 - Finished Epoch 3\n",
            "Model 2 - Starting Epoch 4\n",
            "Model 2 - Epoch 4 - Batch 10/88 - Loss: 2.0422\n",
            "Model 2 - Epoch 4 - Batch 20/88 - Loss: 1.9497\n",
            "Model 2 - Epoch 4 - Batch 30/88 - Loss: 2.0026\n",
            "Model 2 - Epoch 4 - Batch 40/88 - Loss: 1.9614\n",
            "Model 2 - Epoch 4 - Batch 50/88 - Loss: 2.1528\n",
            "Model 2 - Epoch 4 - Batch 60/88 - Loss: 2.0368\n",
            "Model 2 - Epoch 4 - Batch 70/88 - Loss: 2.0111\n",
            "Model 2 - Epoch 4 - Batch 80/88 - Loss: 2.0786\n",
            "Model 2 - Finished Epoch 4\n",
            "Model 2 - Starting Epoch 5\n",
            "Model 2 - Epoch 5 - Batch 10/88 - Loss: 1.9843\n",
            "Model 2 - Epoch 5 - Batch 20/88 - Loss: 2.0304\n",
            "Model 2 - Epoch 5 - Batch 30/88 - Loss: 1.9962\n",
            "Model 2 - Epoch 5 - Batch 40/88 - Loss: 2.0482\n",
            "Model 2 - Epoch 5 - Batch 50/88 - Loss: 2.0546\n",
            "Model 2 - Epoch 5 - Batch 60/88 - Loss: 2.0289\n",
            "Model 2 - Epoch 5 - Batch 70/88 - Loss: 2.0079\n",
            "Model 2 - Epoch 5 - Batch 80/88 - Loss: 1.9504\n",
            "Model 2 - Finished Epoch 5\n",
            "Model 2 - Starting Epoch 6\n",
            "Model 2 - Epoch 6 - Batch 10/88 - Loss: 1.8727\n",
            "Model 2 - Epoch 6 - Batch 20/88 - Loss: 1.9564\n",
            "Model 2 - Epoch 6 - Batch 30/88 - Loss: 2.0183\n",
            "Model 2 - Epoch 6 - Batch 40/88 - Loss: 2.0104\n",
            "Model 2 - Epoch 6 - Batch 50/88 - Loss: 2.1111\n",
            "Model 2 - Epoch 6 - Batch 60/88 - Loss: 2.0347\n",
            "Model 2 - Epoch 6 - Batch 70/88 - Loss: 1.9605\n",
            "Model 2 - Epoch 6 - Batch 80/88 - Loss: 2.1651\n",
            "Model 2 - Finished Epoch 6\n",
            "Model 2 - Starting Epoch 7\n",
            "Model 2 - Epoch 7 - Batch 10/88 - Loss: 1.9588\n",
            "Model 2 - Epoch 7 - Batch 20/88 - Loss: 2.0163\n",
            "Model 2 - Epoch 7 - Batch 30/88 - Loss: 1.8859\n",
            "Model 2 - Epoch 7 - Batch 40/88 - Loss: 2.0198\n",
            "Model 2 - Epoch 7 - Batch 50/88 - Loss: 2.0652\n",
            "Model 2 - Epoch 7 - Batch 60/88 - Loss: 2.0845\n",
            "Model 2 - Epoch 7 - Batch 70/88 - Loss: 2.0809\n",
            "Model 2 - Epoch 7 - Batch 80/88 - Loss: 2.1340\n",
            "Model 2 - Finished Epoch 7\n",
            "Model 2 - Starting Epoch 8\n",
            "Model 2 - Epoch 8 - Batch 10/88 - Loss: 2.0086\n",
            "Model 2 - Epoch 8 - Batch 20/88 - Loss: 2.1258\n",
            "Model 2 - Epoch 8 - Batch 30/88 - Loss: 1.9983\n",
            "Model 2 - Epoch 8 - Batch 40/88 - Loss: 2.0047\n",
            "Model 2 - Epoch 8 - Batch 50/88 - Loss: 2.1232\n",
            "Model 2 - Epoch 8 - Batch 60/88 - Loss: 1.9430\n",
            "Model 2 - Epoch 8 - Batch 70/88 - Loss: 2.0314\n",
            "Model 2 - Epoch 8 - Batch 80/88 - Loss: 1.9820\n",
            "Model 2 - Finished Epoch 8\n",
            "Model 2 - Starting Epoch 9\n",
            "Model 2 - Epoch 9 - Batch 10/88 - Loss: 1.8185\n",
            "Model 2 - Epoch 9 - Batch 20/88 - Loss: 1.8266\n",
            "Model 2 - Epoch 9 - Batch 30/88 - Loss: 1.8978\n",
            "Model 2 - Epoch 9 - Batch 40/88 - Loss: 1.9186\n",
            "Model 2 - Epoch 9 - Batch 50/88 - Loss: 1.9826\n",
            "Model 2 - Epoch 9 - Batch 60/88 - Loss: 1.9949\n",
            "Model 2 - Epoch 9 - Batch 70/88 - Loss: 2.0866\n",
            "Model 2 - Epoch 9 - Batch 80/88 - Loss: 2.0004\n",
            "Model 2 - Finished Epoch 9\n",
            "Model 2 - Starting Epoch 10\n",
            "Model 2 - Epoch 10 - Batch 10/88 - Loss: 1.9694\n",
            "Model 2 - Epoch 10 - Batch 20/88 - Loss: 1.9683\n",
            "Model 2 - Epoch 10 - Batch 30/88 - Loss: 2.0953\n",
            "Model 2 - Epoch 10 - Batch 40/88 - Loss: 2.0621\n",
            "Model 2 - Epoch 10 - Batch 50/88 - Loss: 1.9963\n",
            "Model 2 - Epoch 10 - Batch 60/88 - Loss: 1.8995\n",
            "Model 2 - Epoch 10 - Batch 70/88 - Loss: 1.9852\n",
            "Model 2 - Epoch 10 - Batch 80/88 - Loss: 1.9787\n",
            "Model 2 - Finished Epoch 10\n",
            "Model 2 done training for 10 epochs in generation 6\n",
            "Model 3 - Starting Epoch 1\n",
            "Model 3 - Epoch 1 - Batch 10/88 - Loss: 2.0774\n",
            "Model 3 - Epoch 1 - Batch 20/88 - Loss: 1.8649\n",
            "Model 3 - Epoch 1 - Batch 30/88 - Loss: 1.9849\n",
            "Model 3 - Epoch 1 - Batch 40/88 - Loss: 2.0555\n",
            "Model 3 - Epoch 1 - Batch 50/88 - Loss: 2.0937\n",
            "Model 3 - Epoch 1 - Batch 60/88 - Loss: 2.0173\n",
            "Model 3 - Epoch 1 - Batch 70/88 - Loss: 2.0421\n",
            "Model 3 - Epoch 1 - Batch 80/88 - Loss: 2.1086\n",
            "Model 3 - Finished Epoch 1\n",
            "Model 3 - Starting Epoch 2\n",
            "Model 3 - Epoch 2 - Batch 10/88 - Loss: 2.0385\n",
            "Model 3 - Epoch 2 - Batch 20/88 - Loss: 2.0091\n",
            "Model 3 - Epoch 2 - Batch 30/88 - Loss: 1.9806\n",
            "Model 3 - Epoch 2 - Batch 40/88 - Loss: 2.0331\n",
            "Model 3 - Epoch 2 - Batch 50/88 - Loss: 2.1364\n",
            "Model 3 - Epoch 2 - Batch 60/88 - Loss: 2.0285\n",
            "Model 3 - Epoch 2 - Batch 70/88 - Loss: 2.1867\n",
            "Model 3 - Epoch 2 - Batch 80/88 - Loss: 2.0201\n",
            "Model 3 - Finished Epoch 2\n",
            "Model 3 - Starting Epoch 3\n",
            "Model 3 - Epoch 3 - Batch 10/88 - Loss: 2.0588\n",
            "Model 3 - Epoch 3 - Batch 20/88 - Loss: 2.0034\n",
            "Model 3 - Epoch 3 - Batch 30/88 - Loss: 1.9489\n",
            "Model 3 - Epoch 3 - Batch 40/88 - Loss: 2.0578\n",
            "Model 3 - Epoch 3 - Batch 50/88 - Loss: 1.9261\n",
            "Model 3 - Epoch 3 - Batch 60/88 - Loss: 2.1026\n",
            "Model 3 - Epoch 3 - Batch 70/88 - Loss: 2.1178\n",
            "Model 3 - Epoch 3 - Batch 80/88 - Loss: 2.0599\n",
            "Model 3 - Finished Epoch 3\n",
            "Model 3 - Starting Epoch 4\n",
            "Model 3 - Epoch 4 - Batch 10/88 - Loss: 1.9396\n",
            "Model 3 - Epoch 4 - Batch 20/88 - Loss: 1.9088\n",
            "Model 3 - Epoch 4 - Batch 30/88 - Loss: 2.0126\n",
            "Model 3 - Epoch 4 - Batch 40/88 - Loss: 2.0500\n",
            "Model 3 - Epoch 4 - Batch 50/88 - Loss: 1.9762\n",
            "Model 3 - Epoch 4 - Batch 60/88 - Loss: 1.9490\n",
            "Model 3 - Epoch 4 - Batch 70/88 - Loss: 1.9950\n",
            "Model 3 - Epoch 4 - Batch 80/88 - Loss: 2.0281\n",
            "Model 3 - Finished Epoch 4\n",
            "Model 3 - Starting Epoch 5\n",
            "Model 3 - Epoch 5 - Batch 10/88 - Loss: 1.9441\n",
            "Model 3 - Epoch 5 - Batch 20/88 - Loss: 1.9078\n",
            "Model 3 - Epoch 5 - Batch 30/88 - Loss: 2.0248\n",
            "Model 3 - Epoch 5 - Batch 40/88 - Loss: 2.0045\n",
            "Model 3 - Epoch 5 - Batch 50/88 - Loss: 1.9454\n",
            "Model 3 - Epoch 5 - Batch 60/88 - Loss: 2.0059\n",
            "Model 3 - Epoch 5 - Batch 70/88 - Loss: 2.0244\n",
            "Model 3 - Epoch 5 - Batch 80/88 - Loss: 1.9907\n",
            "Model 3 - Finished Epoch 5\n",
            "Model 3 - Starting Epoch 6\n",
            "Model 3 - Epoch 6 - Batch 10/88 - Loss: 1.9636\n",
            "Model 3 - Epoch 6 - Batch 20/88 - Loss: 2.0772\n",
            "Model 3 - Epoch 6 - Batch 30/88 - Loss: 2.0220\n",
            "Model 3 - Epoch 6 - Batch 40/88 - Loss: 2.0323\n",
            "Model 3 - Epoch 6 - Batch 50/88 - Loss: 2.0042\n",
            "Model 3 - Epoch 6 - Batch 60/88 - Loss: 1.8823\n",
            "Model 3 - Epoch 6 - Batch 70/88 - Loss: 2.0084\n",
            "Model 3 - Epoch 6 - Batch 80/88 - Loss: 1.9897\n",
            "Model 3 - Finished Epoch 6\n",
            "Model 3 - Starting Epoch 7\n",
            "Model 3 - Epoch 7 - Batch 10/88 - Loss: 1.9351\n",
            "Model 3 - Epoch 7 - Batch 20/88 - Loss: 1.9481\n",
            "Model 3 - Epoch 7 - Batch 30/88 - Loss: 1.8802\n",
            "Model 3 - Epoch 7 - Batch 40/88 - Loss: 1.9019\n",
            "Model 3 - Epoch 7 - Batch 50/88 - Loss: 1.9515\n",
            "Model 3 - Epoch 7 - Batch 60/88 - Loss: 1.9264\n",
            "Model 3 - Epoch 7 - Batch 70/88 - Loss: 2.0109\n",
            "Model 3 - Epoch 7 - Batch 80/88 - Loss: 1.8900\n",
            "Model 3 - Finished Epoch 7\n",
            "Model 3 - Starting Epoch 8\n",
            "Model 3 - Epoch 8 - Batch 10/88 - Loss: 1.8409\n",
            "Model 3 - Epoch 8 - Batch 20/88 - Loss: 1.9657\n",
            "Model 3 - Epoch 8 - Batch 30/88 - Loss: 1.8881\n",
            "Model 3 - Epoch 8 - Batch 40/88 - Loss: 1.8710\n",
            "Model 3 - Epoch 8 - Batch 50/88 - Loss: 1.9176\n",
            "Model 3 - Epoch 8 - Batch 60/88 - Loss: 2.0533\n",
            "Model 3 - Epoch 8 - Batch 70/88 - Loss: 1.9450\n",
            "Model 3 - Epoch 8 - Batch 80/88 - Loss: 1.9569\n",
            "Model 3 - Finished Epoch 8\n",
            "Model 3 - Starting Epoch 9\n",
            "Model 3 - Epoch 9 - Batch 10/88 - Loss: 1.9310\n",
            "Model 3 - Epoch 9 - Batch 20/88 - Loss: 1.9546\n",
            "Model 3 - Epoch 9 - Batch 30/88 - Loss: 1.9397\n",
            "Model 3 - Epoch 9 - Batch 40/88 - Loss: 1.8031\n",
            "Model 3 - Epoch 9 - Batch 50/88 - Loss: 1.8515\n",
            "Model 3 - Epoch 9 - Batch 60/88 - Loss: 1.8923\n",
            "Model 3 - Epoch 9 - Batch 70/88 - Loss: 2.0292\n",
            "Model 3 - Epoch 9 - Batch 80/88 - Loss: 1.9803\n",
            "Model 3 - Finished Epoch 9\n",
            "Model 3 - Starting Epoch 10\n",
            "Model 3 - Epoch 10 - Batch 10/88 - Loss: 2.0371\n",
            "Model 3 - Epoch 10 - Batch 20/88 - Loss: 2.0878\n",
            "Model 3 - Epoch 10 - Batch 30/88 - Loss: 1.8662\n",
            "Model 3 - Epoch 10 - Batch 40/88 - Loss: 1.9087\n",
            "Model 3 - Epoch 10 - Batch 50/88 - Loss: 1.9429\n",
            "Model 3 - Epoch 10 - Batch 60/88 - Loss: 1.9527\n",
            "Model 3 - Epoch 10 - Batch 70/88 - Loss: 1.9781\n",
            "Model 3 - Epoch 10 - Batch 80/88 - Loss: 1.8043\n",
            "Model 3 - Finished Epoch 10\n",
            "Model 3 done training for 10 epochs in generation 6\n",
            "Model 4 - Starting Epoch 1\n",
            "Model 4 - Epoch 1 - Batch 10/88 - Loss: 2.0289\n",
            "Model 4 - Epoch 1 - Batch 20/88 - Loss: 2.1240\n",
            "Model 4 - Epoch 1 - Batch 30/88 - Loss: 2.0824\n",
            "Model 4 - Epoch 1 - Batch 40/88 - Loss: 2.1763\n",
            "Model 4 - Epoch 1 - Batch 50/88 - Loss: 2.1993\n",
            "Model 4 - Epoch 1 - Batch 60/88 - Loss: 2.0948\n",
            "Model 4 - Epoch 1 - Batch 70/88 - Loss: 2.2839\n",
            "Model 4 - Epoch 1 - Batch 80/88 - Loss: 2.2316\n",
            "Model 4 - Finished Epoch 1\n",
            "Model 4 - Starting Epoch 2\n",
            "Model 4 - Epoch 2 - Batch 10/88 - Loss: 2.1085\n",
            "Model 4 - Epoch 2 - Batch 20/88 - Loss: 2.1594\n",
            "Model 4 - Epoch 2 - Batch 30/88 - Loss: 2.0747\n",
            "Model 4 - Epoch 2 - Batch 40/88 - Loss: 2.0799\n",
            "Model 4 - Epoch 2 - Batch 50/88 - Loss: 2.0974\n",
            "Model 4 - Epoch 2 - Batch 60/88 - Loss: 2.2212\n",
            "Model 4 - Epoch 2 - Batch 70/88 - Loss: 2.1599\n",
            "Model 4 - Epoch 2 - Batch 80/88 - Loss: 2.0636\n",
            "Model 4 - Finished Epoch 2\n",
            "Model 4 - Starting Epoch 3\n",
            "Model 4 - Epoch 3 - Batch 10/88 - Loss: 1.9925\n",
            "Model 4 - Epoch 3 - Batch 20/88 - Loss: 2.1392\n",
            "Model 4 - Epoch 3 - Batch 30/88 - Loss: 2.0662\n",
            "Model 4 - Epoch 3 - Batch 40/88 - Loss: 2.2592\n",
            "Model 4 - Epoch 3 - Batch 50/88 - Loss: 2.0336\n",
            "Model 4 - Epoch 3 - Batch 60/88 - Loss: 2.1526\n",
            "Model 4 - Epoch 3 - Batch 70/88 - Loss: 2.1470\n",
            "Model 4 - Epoch 3 - Batch 80/88 - Loss: 2.1494\n",
            "Model 4 - Finished Epoch 3\n",
            "Model 4 - Starting Epoch 4\n",
            "Model 4 - Epoch 4 - Batch 10/88 - Loss: 2.1270\n",
            "Model 4 - Epoch 4 - Batch 20/88 - Loss: 2.0763\n",
            "Model 4 - Epoch 4 - Batch 30/88 - Loss: 2.1075\n",
            "Model 4 - Epoch 4 - Batch 40/88 - Loss: 2.1116\n",
            "Model 4 - Epoch 4 - Batch 50/88 - Loss: 2.0885\n",
            "Model 4 - Epoch 4 - Batch 60/88 - Loss: 2.1273\n",
            "Model 4 - Epoch 4 - Batch 70/88 - Loss: 2.1376\n",
            "Model 4 - Epoch 4 - Batch 80/88 - Loss: 2.1013\n",
            "Model 4 - Finished Epoch 4\n",
            "Model 4 - Starting Epoch 5\n",
            "Model 4 - Epoch 5 - Batch 10/88 - Loss: 2.0155\n",
            "Model 4 - Epoch 5 - Batch 20/88 - Loss: 1.9872\n",
            "Model 4 - Epoch 5 - Batch 30/88 - Loss: 2.1498\n",
            "Model 4 - Epoch 5 - Batch 40/88 - Loss: 2.0810\n",
            "Model 4 - Epoch 5 - Batch 50/88 - Loss: 2.0980\n",
            "Model 4 - Epoch 5 - Batch 60/88 - Loss: 2.1325\n",
            "Model 4 - Epoch 5 - Batch 70/88 - Loss: 2.0602\n",
            "Model 4 - Epoch 5 - Batch 80/88 - Loss: 2.1368\n",
            "Model 4 - Finished Epoch 5\n",
            "Model 4 - Starting Epoch 6\n",
            "Model 4 - Epoch 6 - Batch 10/88 - Loss: 2.0358\n",
            "Model 4 - Epoch 6 - Batch 20/88 - Loss: 2.0177\n",
            "Model 4 - Epoch 6 - Batch 30/88 - Loss: 2.1953\n",
            "Model 4 - Epoch 6 - Batch 40/88 - Loss: 2.1229\n",
            "Model 4 - Epoch 6 - Batch 50/88 - Loss: 2.0930\n",
            "Model 4 - Epoch 6 - Batch 60/88 - Loss: 2.1455\n",
            "Model 4 - Epoch 6 - Batch 70/88 - Loss: 2.0913\n",
            "Model 4 - Epoch 6 - Batch 80/88 - Loss: 2.0102\n",
            "Model 4 - Finished Epoch 6\n",
            "Model 4 - Starting Epoch 7\n",
            "Model 4 - Epoch 7 - Batch 10/88 - Loss: 2.2147\n",
            "Model 4 - Epoch 7 - Batch 20/88 - Loss: 2.0896\n",
            "Model 4 - Epoch 7 - Batch 30/88 - Loss: 2.0530\n",
            "Model 4 - Epoch 7 - Batch 40/88 - Loss: 2.0355\n",
            "Model 4 - Epoch 7 - Batch 50/88 - Loss: 2.0457\n",
            "Model 4 - Epoch 7 - Batch 60/88 - Loss: 2.0965\n",
            "Model 4 - Epoch 7 - Batch 70/88 - Loss: 2.0711\n",
            "Model 4 - Epoch 7 - Batch 80/88 - Loss: 2.0913\n",
            "Model 4 - Finished Epoch 7\n",
            "Model 4 - Starting Epoch 8\n",
            "Model 4 - Epoch 8 - Batch 10/88 - Loss: 2.0578\n",
            "Model 4 - Epoch 8 - Batch 20/88 - Loss: 2.0997\n",
            "Model 4 - Epoch 8 - Batch 30/88 - Loss: 2.0289\n",
            "Model 4 - Epoch 8 - Batch 40/88 - Loss: 2.0559\n",
            "Model 4 - Epoch 8 - Batch 50/88 - Loss: 2.0660\n",
            "Model 4 - Epoch 8 - Batch 60/88 - Loss: 2.0944\n",
            "Model 4 - Epoch 8 - Batch 70/88 - Loss: 2.0011\n",
            "Model 4 - Epoch 8 - Batch 80/88 - Loss: 2.0724\n",
            "Model 4 - Finished Epoch 8\n",
            "Model 4 - Starting Epoch 9\n",
            "Model 4 - Epoch 9 - Batch 10/88 - Loss: 2.0683\n",
            "Model 4 - Epoch 9 - Batch 20/88 - Loss: 2.0730\n",
            "Model 4 - Epoch 9 - Batch 30/88 - Loss: 1.9408\n",
            "Model 4 - Epoch 9 - Batch 40/88 - Loss: 2.0060\n",
            "Model 4 - Epoch 9 - Batch 50/88 - Loss: 2.1982\n",
            "Model 4 - Epoch 9 - Batch 60/88 - Loss: 2.0975\n",
            "Model 4 - Epoch 9 - Batch 70/88 - Loss: 2.0031\n",
            "Model 4 - Epoch 9 - Batch 80/88 - Loss: 2.0343\n",
            "Model 4 - Finished Epoch 9\n",
            "Model 4 - Starting Epoch 10\n",
            "Model 4 - Epoch 10 - Batch 10/88 - Loss: 1.9777\n",
            "Model 4 - Epoch 10 - Batch 20/88 - Loss: 1.9473\n",
            "Model 4 - Epoch 10 - Batch 30/88 - Loss: 2.0998\n",
            "Model 4 - Epoch 10 - Batch 40/88 - Loss: 2.0896\n",
            "Model 4 - Epoch 10 - Batch 50/88 - Loss: 1.9914\n",
            "Model 4 - Epoch 10 - Batch 60/88 - Loss: 1.9821\n",
            "Model 4 - Epoch 10 - Batch 70/88 - Loss: 2.0488\n",
            "Model 4 - Epoch 10 - Batch 80/88 - Loss: 2.1754\n",
            "Model 4 - Finished Epoch 10\n",
            "Model 4 done training for 10 epochs in generation 6\n",
            "Model 5 - Starting Epoch 1\n",
            "Model 5 - Epoch 1 - Batch 10/88 - Loss: 2.0247\n",
            "Model 5 - Epoch 1 - Batch 20/88 - Loss: 1.9826\n",
            "Model 5 - Epoch 1 - Batch 30/88 - Loss: 2.0652\n",
            "Model 5 - Epoch 1 - Batch 40/88 - Loss: 2.1301\n",
            "Model 5 - Epoch 1 - Batch 50/88 - Loss: 2.1612\n",
            "Model 5 - Epoch 1 - Batch 60/88 - Loss: 2.0914\n",
            "Model 5 - Epoch 1 - Batch 70/88 - Loss: 2.1805\n",
            "Model 5 - Epoch 1 - Batch 80/88 - Loss: 2.1559\n",
            "Model 5 - Finished Epoch 1\n",
            "Model 5 - Starting Epoch 2\n",
            "Model 5 - Epoch 2 - Batch 10/88 - Loss: 2.0827\n",
            "Model 5 - Epoch 2 - Batch 20/88 - Loss: 2.1065\n",
            "Model 5 - Epoch 2 - Batch 30/88 - Loss: 2.0577\n",
            "Model 5 - Epoch 2 - Batch 40/88 - Loss: 1.9762\n",
            "Model 5 - Epoch 2 - Batch 50/88 - Loss: 2.1177\n",
            "Model 5 - Epoch 2 - Batch 60/88 - Loss: 2.1759\n",
            "Model 5 - Epoch 2 - Batch 70/88 - Loss: 2.0104\n",
            "Model 5 - Epoch 2 - Batch 80/88 - Loss: 2.0756\n",
            "Model 5 - Finished Epoch 2\n",
            "Model 5 - Starting Epoch 3\n",
            "Model 5 - Epoch 3 - Batch 10/88 - Loss: 2.0466\n",
            "Model 5 - Epoch 3 - Batch 20/88 - Loss: 2.0761\n",
            "Model 5 - Epoch 3 - Batch 30/88 - Loss: 2.1695\n",
            "Model 5 - Epoch 3 - Batch 40/88 - Loss: 2.1556\n",
            "Model 5 - Epoch 3 - Batch 50/88 - Loss: 2.0319\n",
            "Model 5 - Epoch 3 - Batch 60/88 - Loss: 2.1073\n",
            "Model 5 - Epoch 3 - Batch 70/88 - Loss: 2.0511\n",
            "Model 5 - Epoch 3 - Batch 80/88 - Loss: 2.0800\n",
            "Model 5 - Finished Epoch 3\n",
            "Model 5 - Starting Epoch 4\n",
            "Model 5 - Epoch 4 - Batch 10/88 - Loss: 2.0087\n",
            "Model 5 - Epoch 4 - Batch 20/88 - Loss: 2.0699\n",
            "Model 5 - Epoch 4 - Batch 30/88 - Loss: 2.1363\n",
            "Model 5 - Epoch 4 - Batch 40/88 - Loss: 2.0415\n",
            "Model 5 - Epoch 4 - Batch 50/88 - Loss: 2.0490\n",
            "Model 5 - Epoch 4 - Batch 60/88 - Loss: 2.0750\n",
            "Model 5 - Epoch 4 - Batch 70/88 - Loss: 2.1783\n",
            "Model 5 - Epoch 4 - Batch 80/88 - Loss: 2.1257\n",
            "Model 5 - Finished Epoch 4\n",
            "Model 5 - Starting Epoch 5\n",
            "Model 5 - Epoch 5 - Batch 10/88 - Loss: 2.0867\n",
            "Model 5 - Epoch 5 - Batch 20/88 - Loss: 2.0654\n",
            "Model 5 - Epoch 5 - Batch 30/88 - Loss: 2.1384\n",
            "Model 5 - Epoch 5 - Batch 40/88 - Loss: 2.1770\n",
            "Model 5 - Epoch 5 - Batch 50/88 - Loss: 2.0539\n",
            "Model 5 - Epoch 5 - Batch 60/88 - Loss: 2.0677\n",
            "Model 5 - Epoch 5 - Batch 70/88 - Loss: 2.0079\n",
            "Model 5 - Epoch 5 - Batch 80/88 - Loss: 2.1476\n",
            "Model 5 - Finished Epoch 5\n",
            "Model 5 - Starting Epoch 6\n",
            "Model 5 - Epoch 6 - Batch 10/88 - Loss: 2.0147\n",
            "Model 5 - Epoch 6 - Batch 20/88 - Loss: 1.9495\n",
            "Model 5 - Epoch 6 - Batch 30/88 - Loss: 1.9897\n",
            "Model 5 - Epoch 6 - Batch 40/88 - Loss: 2.1269\n",
            "Model 5 - Epoch 6 - Batch 50/88 - Loss: 2.0916\n",
            "Model 5 - Epoch 6 - Batch 60/88 - Loss: 1.9475\n",
            "Model 5 - Epoch 6 - Batch 70/88 - Loss: 2.0177\n",
            "Model 5 - Epoch 6 - Batch 80/88 - Loss: 2.0445\n",
            "Model 5 - Finished Epoch 6\n",
            "Model 5 - Starting Epoch 7\n",
            "Model 5 - Epoch 7 - Batch 10/88 - Loss: 1.9597\n",
            "Model 5 - Epoch 7 - Batch 20/88 - Loss: 1.9933\n",
            "Model 5 - Epoch 7 - Batch 30/88 - Loss: 2.0546\n",
            "Model 5 - Epoch 7 - Batch 40/88 - Loss: 2.0233\n",
            "Model 5 - Epoch 7 - Batch 50/88 - Loss: 2.0819\n",
            "Model 5 - Epoch 7 - Batch 60/88 - Loss: 2.1675\n",
            "Model 5 - Epoch 7 - Batch 70/88 - Loss: 2.1423\n",
            "Model 5 - Epoch 7 - Batch 80/88 - Loss: 2.0750\n",
            "Model 5 - Finished Epoch 7\n",
            "Model 5 - Starting Epoch 8\n",
            "Model 5 - Epoch 8 - Batch 10/88 - Loss: 1.9932\n",
            "Model 5 - Epoch 8 - Batch 20/88 - Loss: 2.0076\n",
            "Model 5 - Epoch 8 - Batch 30/88 - Loss: 2.1069\n",
            "Model 5 - Epoch 8 - Batch 40/88 - Loss: 2.0487\n",
            "Model 5 - Epoch 8 - Batch 50/88 - Loss: 2.0744\n",
            "Model 5 - Epoch 8 - Batch 60/88 - Loss: 2.0788\n",
            "Model 5 - Epoch 8 - Batch 70/88 - Loss: 1.9677\n",
            "Model 5 - Epoch 8 - Batch 80/88 - Loss: 2.1674\n",
            "Model 5 - Finished Epoch 8\n",
            "Model 5 - Starting Epoch 9\n",
            "Model 5 - Epoch 9 - Batch 10/88 - Loss: 2.0063\n",
            "Model 5 - Epoch 9 - Batch 20/88 - Loss: 2.0567\n",
            "Model 5 - Epoch 9 - Batch 30/88 - Loss: 2.1495\n",
            "Model 5 - Epoch 9 - Batch 40/88 - Loss: 1.9999\n",
            "Model 5 - Epoch 9 - Batch 50/88 - Loss: 2.0376\n",
            "Model 5 - Epoch 9 - Batch 60/88 - Loss: 2.0936\n",
            "Model 5 - Epoch 9 - Batch 70/88 - Loss: 2.0943\n",
            "Model 5 - Epoch 9 - Batch 80/88 - Loss: 1.9304\n",
            "Model 5 - Finished Epoch 9\n",
            "Model 5 - Starting Epoch 10\n",
            "Model 5 - Epoch 10 - Batch 10/88 - Loss: 2.0123\n",
            "Model 5 - Epoch 10 - Batch 20/88 - Loss: 1.9888\n",
            "Model 5 - Epoch 10 - Batch 30/88 - Loss: 2.0698\n",
            "Model 5 - Epoch 10 - Batch 40/88 - Loss: 2.0729\n",
            "Model 5 - Epoch 10 - Batch 50/88 - Loss: 1.9439\n",
            "Model 5 - Epoch 10 - Batch 60/88 - Loss: 1.9818\n",
            "Model 5 - Epoch 10 - Batch 70/88 - Loss: 2.0581\n",
            "Model 5 - Epoch 10 - Batch 80/88 - Loss: 1.8551\n",
            "Model 5 - Finished Epoch 10\n",
            "Model 5 done training for 10 epochs in generation 6\n",
            "Model 6 - Starting Epoch 1\n",
            "Model 6 - Epoch 1 - Batch 10/88 - Loss: 2.1815\n",
            "Model 6 - Epoch 1 - Batch 20/88 - Loss: 2.1997\n",
            "Model 6 - Epoch 1 - Batch 30/88 - Loss: 2.2086\n",
            "Model 6 - Epoch 1 - Batch 40/88 - Loss: 2.1293\n",
            "Model 6 - Epoch 1 - Batch 50/88 - Loss: 2.2435\n",
            "Model 6 - Epoch 1 - Batch 60/88 - Loss: 2.2082\n",
            "Model 6 - Epoch 1 - Batch 70/88 - Loss: 2.2566\n",
            "Model 6 - Epoch 1 - Batch 80/88 - Loss: 2.3339\n",
            "Model 6 - Finished Epoch 1\n",
            "Model 6 - Starting Epoch 2\n",
            "Model 6 - Epoch 2 - Batch 10/88 - Loss: 2.1217\n",
            "Model 6 - Epoch 2 - Batch 20/88 - Loss: 2.1099\n",
            "Model 6 - Epoch 2 - Batch 30/88 - Loss: 2.1994\n",
            "Model 6 - Epoch 2 - Batch 40/88 - Loss: 2.1556\n",
            "Model 6 - Epoch 2 - Batch 50/88 - Loss: 2.2442\n",
            "Model 6 - Epoch 2 - Batch 60/88 - Loss: 2.2604\n",
            "Model 6 - Epoch 2 - Batch 70/88 - Loss: 2.2580\n",
            "Model 6 - Epoch 2 - Batch 80/88 - Loss: 2.1471\n",
            "Model 6 - Finished Epoch 2\n",
            "Model 6 - Starting Epoch 3\n",
            "Model 6 - Epoch 3 - Batch 10/88 - Loss: 2.2315\n",
            "Model 6 - Epoch 3 - Batch 20/88 - Loss: 2.0522\n",
            "Model 6 - Epoch 3 - Batch 30/88 - Loss: 2.1466\n",
            "Model 6 - Epoch 3 - Batch 40/88 - Loss: 2.1628\n",
            "Model 6 - Epoch 3 - Batch 50/88 - Loss: 2.1999\n",
            "Model 6 - Epoch 3 - Batch 60/88 - Loss: 2.2659\n",
            "Model 6 - Epoch 3 - Batch 70/88 - Loss: 2.2243\n",
            "Model 6 - Epoch 3 - Batch 80/88 - Loss: 2.2333\n",
            "Model 6 - Finished Epoch 3\n",
            "Model 6 - Starting Epoch 4\n",
            "Model 6 - Epoch 4 - Batch 10/88 - Loss: 2.1242\n",
            "Model 6 - Epoch 4 - Batch 20/88 - Loss: 2.1887\n",
            "Model 6 - Epoch 4 - Batch 30/88 - Loss: 2.0579\n",
            "Model 6 - Epoch 4 - Batch 40/88 - Loss: 2.2341\n",
            "Model 6 - Epoch 4 - Batch 50/88 - Loss: 2.1041\n",
            "Model 6 - Epoch 4 - Batch 60/88 - Loss: 2.1526\n",
            "Model 6 - Epoch 4 - Batch 70/88 - Loss: 2.1619\n",
            "Model 6 - Epoch 4 - Batch 80/88 - Loss: 2.1143\n",
            "Model 6 - Finished Epoch 4\n",
            "Model 6 - Starting Epoch 5\n",
            "Model 6 - Epoch 5 - Batch 10/88 - Loss: 2.1179\n",
            "Model 6 - Epoch 5 - Batch 20/88 - Loss: 2.0589\n",
            "Model 6 - Epoch 5 - Batch 30/88 - Loss: 2.2085\n",
            "Model 6 - Epoch 5 - Batch 40/88 - Loss: 2.3099\n",
            "Model 6 - Epoch 5 - Batch 50/88 - Loss: 2.1698\n",
            "Model 6 - Epoch 5 - Batch 60/88 - Loss: 2.1485\n",
            "Model 6 - Epoch 5 - Batch 70/88 - Loss: 2.0389\n",
            "Model 6 - Epoch 5 - Batch 80/88 - Loss: 2.1592\n",
            "Model 6 - Finished Epoch 5\n",
            "Model 6 - Starting Epoch 6\n",
            "Model 6 - Epoch 6 - Batch 10/88 - Loss: 2.2125\n",
            "Model 6 - Epoch 6 - Batch 20/88 - Loss: 2.1562\n",
            "Model 6 - Epoch 6 - Batch 30/88 - Loss: 2.1538\n",
            "Model 6 - Epoch 6 - Batch 40/88 - Loss: 2.1576\n",
            "Model 6 - Epoch 6 - Batch 50/88 - Loss: 2.2431\n",
            "Model 6 - Epoch 6 - Batch 60/88 - Loss: 2.2095\n",
            "Model 6 - Epoch 6 - Batch 70/88 - Loss: 2.1951\n",
            "Model 6 - Epoch 6 - Batch 80/88 - Loss: 2.1133\n",
            "Model 6 - Finished Epoch 6\n",
            "Model 6 - Starting Epoch 7\n",
            "Model 6 - Epoch 7 - Batch 10/88 - Loss: 2.1110\n",
            "Model 6 - Epoch 7 - Batch 20/88 - Loss: 2.1023\n",
            "Model 6 - Epoch 7 - Batch 30/88 - Loss: 2.0713\n",
            "Model 6 - Epoch 7 - Batch 40/88 - Loss: 2.1610\n",
            "Model 6 - Epoch 7 - Batch 50/88 - Loss: 2.2278\n",
            "Model 6 - Epoch 7 - Batch 60/88 - Loss: 2.1083\n",
            "Model 6 - Epoch 7 - Batch 70/88 - Loss: 2.1318\n",
            "Model 6 - Epoch 7 - Batch 80/88 - Loss: 2.1518\n",
            "Model 6 - Finished Epoch 7\n",
            "Model 6 - Starting Epoch 8\n",
            "Model 6 - Epoch 8 - Batch 10/88 - Loss: 2.0940\n",
            "Model 6 - Epoch 8 - Batch 20/88 - Loss: 2.1387\n",
            "Model 6 - Epoch 8 - Batch 30/88 - Loss: 1.9871\n",
            "Model 6 - Epoch 8 - Batch 40/88 - Loss: 2.1121\n",
            "Model 6 - Epoch 8 - Batch 50/88 - Loss: 2.1667\n",
            "Model 6 - Epoch 8 - Batch 60/88 - Loss: 2.2182\n",
            "Model 6 - Epoch 8 - Batch 70/88 - Loss: 2.1867\n",
            "Model 6 - Epoch 8 - Batch 80/88 - Loss: 2.0645\n",
            "Model 6 - Finished Epoch 8\n",
            "Model 6 - Starting Epoch 9\n",
            "Model 6 - Epoch 9 - Batch 10/88 - Loss: 2.0842\n",
            "Model 6 - Epoch 9 - Batch 20/88 - Loss: 2.1395\n",
            "Model 6 - Epoch 9 - Batch 30/88 - Loss: 2.0038\n",
            "Model 6 - Epoch 9 - Batch 40/88 - Loss: 2.0197\n",
            "Model 6 - Epoch 9 - Batch 50/88 - Loss: 2.0794\n",
            "Model 6 - Epoch 9 - Batch 60/88 - Loss: 2.1522\n",
            "Model 6 - Epoch 9 - Batch 70/88 - Loss: 2.2428\n",
            "Model 6 - Epoch 9 - Batch 80/88 - Loss: 2.1037\n",
            "Model 6 - Finished Epoch 9\n",
            "Model 6 - Starting Epoch 10\n",
            "Model 6 - Epoch 10 - Batch 10/88 - Loss: 1.9504\n",
            "Model 6 - Epoch 10 - Batch 20/88 - Loss: 2.1683\n",
            "Model 6 - Epoch 10 - Batch 30/88 - Loss: 2.0619\n",
            "Model 6 - Epoch 10 - Batch 40/88 - Loss: 2.1514\n",
            "Model 6 - Epoch 10 - Batch 50/88 - Loss: 2.2009\n",
            "Model 6 - Epoch 10 - Batch 60/88 - Loss: 2.0502\n",
            "Model 6 - Epoch 10 - Batch 70/88 - Loss: 2.1196\n",
            "Model 6 - Epoch 10 - Batch 80/88 - Loss: 2.0589\n",
            "Model 6 - Finished Epoch 10\n",
            "Model 6 done training for 10 epochs in generation 6\n",
            "Model 7 - Starting Epoch 1\n",
            "Model 7 - Epoch 1 - Batch 10/88 - Loss: 2.1242\n",
            "Model 7 - Epoch 1 - Batch 20/88 - Loss: 2.0394\n",
            "Model 7 - Epoch 1 - Batch 30/88 - Loss: 1.9975\n",
            "Model 7 - Epoch 1 - Batch 40/88 - Loss: 2.1208\n",
            "Model 7 - Epoch 1 - Batch 50/88 - Loss: 2.1002\n",
            "Model 7 - Epoch 1 - Batch 60/88 - Loss: 2.1125\n",
            "Model 7 - Epoch 1 - Batch 70/88 - Loss: 2.0316\n",
            "Model 7 - Epoch 1 - Batch 80/88 - Loss: 2.1816\n",
            "Model 7 - Finished Epoch 1\n",
            "Model 7 - Starting Epoch 2\n",
            "Model 7 - Epoch 2 - Batch 10/88 - Loss: 1.9578\n",
            "Model 7 - Epoch 2 - Batch 20/88 - Loss: 1.9656\n",
            "Model 7 - Epoch 2 - Batch 30/88 - Loss: 1.9329\n",
            "Model 7 - Epoch 2 - Batch 40/88 - Loss: 2.0884\n",
            "Model 7 - Epoch 2 - Batch 50/88 - Loss: 2.0073\n",
            "Model 7 - Epoch 2 - Batch 60/88 - Loss: 2.1953\n",
            "Model 7 - Epoch 2 - Batch 70/88 - Loss: 2.0183\n",
            "Model 7 - Epoch 2 - Batch 80/88 - Loss: 2.1719\n",
            "Model 7 - Finished Epoch 2\n",
            "Model 7 - Starting Epoch 3\n",
            "Model 7 - Epoch 3 - Batch 10/88 - Loss: 2.0173\n",
            "Model 7 - Epoch 3 - Batch 20/88 - Loss: 1.8578\n",
            "Model 7 - Epoch 3 - Batch 30/88 - Loss: 2.0345\n",
            "Model 7 - Epoch 3 - Batch 40/88 - Loss: 2.0164\n",
            "Model 7 - Epoch 3 - Batch 50/88 - Loss: 1.9811\n",
            "Model 7 - Epoch 3 - Batch 60/88 - Loss: 2.0414\n",
            "Model 7 - Epoch 3 - Batch 70/88 - Loss: 2.1478\n",
            "Model 7 - Epoch 3 - Batch 80/88 - Loss: 2.0622\n",
            "Model 7 - Finished Epoch 3\n",
            "Model 7 - Starting Epoch 4\n",
            "Model 7 - Epoch 4 - Batch 10/88 - Loss: 2.0275\n",
            "Model 7 - Epoch 4 - Batch 20/88 - Loss: 2.0098\n",
            "Model 7 - Epoch 4 - Batch 30/88 - Loss: 1.9492\n",
            "Model 7 - Epoch 4 - Batch 40/88 - Loss: 1.9830\n",
            "Model 7 - Epoch 4 - Batch 50/88 - Loss: 1.9637\n",
            "Model 7 - Epoch 4 - Batch 60/88 - Loss: 2.1396\n",
            "Model 7 - Epoch 4 - Batch 70/88 - Loss: 2.0560\n",
            "Model 7 - Epoch 4 - Batch 80/88 - Loss: 1.9678\n",
            "Model 7 - Finished Epoch 4\n",
            "Model 7 - Starting Epoch 5\n",
            "Model 7 - Epoch 5 - Batch 10/88 - Loss: 2.0978\n",
            "Model 7 - Epoch 5 - Batch 20/88 - Loss: 2.0220\n",
            "Model 7 - Epoch 5 - Batch 30/88 - Loss: 2.1365\n",
            "Model 7 - Epoch 5 - Batch 40/88 - Loss: 2.0190\n",
            "Model 7 - Epoch 5 - Batch 50/88 - Loss: 2.0762\n",
            "Model 7 - Epoch 5 - Batch 60/88 - Loss: 2.0754\n",
            "Model 7 - Epoch 5 - Batch 70/88 - Loss: 2.0550\n",
            "Model 7 - Epoch 5 - Batch 80/88 - Loss: 2.0194\n",
            "Model 7 - Finished Epoch 5\n",
            "Model 7 - Starting Epoch 6\n",
            "Model 7 - Epoch 6 - Batch 10/88 - Loss: 1.9326\n",
            "Model 7 - Epoch 6 - Batch 20/88 - Loss: 1.9609\n",
            "Model 7 - Epoch 6 - Batch 30/88 - Loss: 1.9737\n",
            "Model 7 - Epoch 6 - Batch 40/88 - Loss: 2.0591\n",
            "Model 7 - Epoch 6 - Batch 50/88 - Loss: 2.0406\n",
            "Model 7 - Epoch 6 - Batch 60/88 - Loss: 2.0781\n",
            "Model 7 - Epoch 6 - Batch 70/88 - Loss: 2.0137\n",
            "Model 7 - Epoch 6 - Batch 80/88 - Loss: 2.1617\n",
            "Model 7 - Finished Epoch 6\n",
            "Model 7 - Starting Epoch 7\n",
            "Model 7 - Epoch 7 - Batch 10/88 - Loss: 1.8650\n",
            "Model 7 - Epoch 7 - Batch 20/88 - Loss: 2.0739\n",
            "Model 7 - Epoch 7 - Batch 30/88 - Loss: 1.9535\n",
            "Model 7 - Epoch 7 - Batch 40/88 - Loss: 1.9582\n",
            "Model 7 - Epoch 7 - Batch 50/88 - Loss: 2.0146\n",
            "Model 7 - Epoch 7 - Batch 60/88 - Loss: 1.9649\n",
            "Model 7 - Epoch 7 - Batch 70/88 - Loss: 2.1118\n",
            "Model 7 - Epoch 7 - Batch 80/88 - Loss: 2.1499\n",
            "Model 7 - Finished Epoch 7\n",
            "Model 7 - Starting Epoch 8\n",
            "Model 7 - Epoch 8 - Batch 10/88 - Loss: 2.0447\n",
            "Model 7 - Epoch 8 - Batch 20/88 - Loss: 2.0758\n",
            "Model 7 - Epoch 8 - Batch 30/88 - Loss: 1.9875\n",
            "Model 7 - Epoch 8 - Batch 40/88 - Loss: 2.0220\n",
            "Model 7 - Epoch 8 - Batch 50/88 - Loss: 2.0567\n",
            "Model 7 - Epoch 8 - Batch 60/88 - Loss: 1.9576\n",
            "Model 7 - Epoch 8 - Batch 70/88 - Loss: 1.9392\n",
            "Model 7 - Epoch 8 - Batch 80/88 - Loss: 2.0317\n",
            "Model 7 - Finished Epoch 8\n",
            "Model 7 - Starting Epoch 9\n",
            "Model 7 - Epoch 9 - Batch 10/88 - Loss: 1.9365\n",
            "Model 7 - Epoch 9 - Batch 20/88 - Loss: 1.9078\n",
            "Model 7 - Epoch 9 - Batch 30/88 - Loss: 1.9176\n",
            "Model 7 - Epoch 9 - Batch 40/88 - Loss: 2.0150\n",
            "Model 7 - Epoch 9 - Batch 50/88 - Loss: 2.0023\n",
            "Model 7 - Epoch 9 - Batch 60/88 - Loss: 2.0699\n",
            "Model 7 - Epoch 9 - Batch 70/88 - Loss: 1.9836\n",
            "Model 7 - Epoch 9 - Batch 80/88 - Loss: 2.0567\n",
            "Model 7 - Finished Epoch 9\n",
            "Model 7 - Starting Epoch 10\n",
            "Model 7 - Epoch 10 - Batch 10/88 - Loss: 1.9655\n",
            "Model 7 - Epoch 10 - Batch 20/88 - Loss: 2.0318\n",
            "Model 7 - Epoch 10 - Batch 30/88 - Loss: 1.9474\n",
            "Model 7 - Epoch 10 - Batch 40/88 - Loss: 2.0478\n",
            "Model 7 - Epoch 10 - Batch 50/88 - Loss: 1.9629\n",
            "Model 7 - Epoch 10 - Batch 60/88 - Loss: 2.0627\n",
            "Model 7 - Epoch 10 - Batch 70/88 - Loss: 1.9487\n",
            "Model 7 - Epoch 10 - Batch 80/88 - Loss: 1.9893\n",
            "Model 7 - Finished Epoch 10\n",
            "Model 7 done training for 10 epochs in generation 6\n",
            "Model 8 - Starting Epoch 1\n",
            "Model 8 - Epoch 1 - Batch 10/88 - Loss: 2.2991\n",
            "Model 8 - Epoch 1 - Batch 20/88 - Loss: 2.2900\n",
            "Model 8 - Epoch 1 - Batch 30/88 - Loss: 2.2560\n",
            "Model 8 - Epoch 1 - Batch 40/88 - Loss: 2.1526\n",
            "Model 8 - Epoch 1 - Batch 50/88 - Loss: 2.2783\n",
            "Model 8 - Epoch 1 - Batch 60/88 - Loss: 2.2983\n",
            "Model 8 - Epoch 1 - Batch 70/88 - Loss: 2.2830\n",
            "Model 8 - Epoch 1 - Batch 80/88 - Loss: 2.2039\n",
            "Model 8 - Finished Epoch 1\n",
            "Model 8 - Starting Epoch 2\n",
            "Model 8 - Epoch 2 - Batch 10/88 - Loss: 2.2424\n",
            "Model 8 - Epoch 2 - Batch 20/88 - Loss: 2.2273\n",
            "Model 8 - Epoch 2 - Batch 30/88 - Loss: 2.1220\n",
            "Model 8 - Epoch 2 - Batch 40/88 - Loss: 2.3529\n",
            "Model 8 - Epoch 2 - Batch 50/88 - Loss: 2.2269\n",
            "Model 8 - Epoch 2 - Batch 60/88 - Loss: 2.1612\n",
            "Model 8 - Epoch 2 - Batch 70/88 - Loss: 2.2956\n",
            "Model 8 - Epoch 2 - Batch 80/88 - Loss: 2.1269\n",
            "Model 8 - Finished Epoch 2\n",
            "Model 8 - Starting Epoch 3\n",
            "Model 8 - Epoch 3 - Batch 10/88 - Loss: 2.1789\n",
            "Model 8 - Epoch 3 - Batch 20/88 - Loss: 2.1979\n",
            "Model 8 - Epoch 3 - Batch 30/88 - Loss: 2.2871\n",
            "Model 8 - Epoch 3 - Batch 40/88 - Loss: 2.2119\n",
            "Model 8 - Epoch 3 - Batch 50/88 - Loss: 2.2712\n",
            "Model 8 - Epoch 3 - Batch 60/88 - Loss: 2.3474\n",
            "Model 8 - Epoch 3 - Batch 70/88 - Loss: 2.2161\n",
            "Model 8 - Epoch 3 - Batch 80/88 - Loss: 2.2237\n",
            "Model 8 - Finished Epoch 3\n",
            "Model 8 - Starting Epoch 4\n",
            "Model 8 - Epoch 4 - Batch 10/88 - Loss: 2.1940\n",
            "Model 8 - Epoch 4 - Batch 20/88 - Loss: 2.2790\n",
            "Model 8 - Epoch 4 - Batch 30/88 - Loss: 2.1302\n",
            "Model 8 - Epoch 4 - Batch 40/88 - Loss: 2.2405\n",
            "Model 8 - Epoch 4 - Batch 50/88 - Loss: 2.2687\n",
            "Model 8 - Epoch 4 - Batch 60/88 - Loss: 2.2116\n",
            "Model 8 - Epoch 4 - Batch 70/88 - Loss: 2.2359\n",
            "Model 8 - Epoch 4 - Batch 80/88 - Loss: 2.3641\n",
            "Model 8 - Finished Epoch 4\n",
            "Model 8 - Starting Epoch 5\n",
            "Model 8 - Epoch 5 - Batch 10/88 - Loss: 2.1694\n",
            "Model 8 - Epoch 5 - Batch 20/88 - Loss: 2.0977\n",
            "Model 8 - Epoch 5 - Batch 30/88 - Loss: 2.2907\n",
            "Model 8 - Epoch 5 - Batch 40/88 - Loss: 2.1914\n",
            "Model 8 - Epoch 5 - Batch 50/88 - Loss: 2.2376\n",
            "Model 8 - Epoch 5 - Batch 60/88 - Loss: 2.2037\n",
            "Model 8 - Epoch 5 - Batch 70/88 - Loss: 2.2460\n",
            "Model 8 - Epoch 5 - Batch 80/88 - Loss: 2.3370\n",
            "Model 8 - Finished Epoch 5\n",
            "Model 8 - Starting Epoch 6\n",
            "Model 8 - Epoch 6 - Batch 10/88 - Loss: 2.1473\n",
            "Model 8 - Epoch 6 - Batch 20/88 - Loss: 2.0885\n",
            "Model 8 - Epoch 6 - Batch 30/88 - Loss: 2.2467\n",
            "Model 8 - Epoch 6 - Batch 40/88 - Loss: 2.0819\n",
            "Model 8 - Epoch 6 - Batch 50/88 - Loss: 2.2689\n",
            "Model 8 - Epoch 6 - Batch 60/88 - Loss: 2.1584\n",
            "Model 8 - Epoch 6 - Batch 70/88 - Loss: 2.3788\n",
            "Model 8 - Epoch 6 - Batch 80/88 - Loss: 2.1766\n",
            "Model 8 - Finished Epoch 6\n",
            "Model 8 - Starting Epoch 7\n",
            "Model 8 - Epoch 7 - Batch 10/88 - Loss: 2.1021\n",
            "Model 8 - Epoch 7 - Batch 20/88 - Loss: 2.1790\n",
            "Model 8 - Epoch 7 - Batch 30/88 - Loss: 2.2149\n",
            "Model 8 - Epoch 7 - Batch 40/88 - Loss: 2.0979\n",
            "Model 8 - Epoch 7 - Batch 50/88 - Loss: 2.2350\n",
            "Model 8 - Epoch 7 - Batch 60/88 - Loss: 2.2190\n",
            "Model 8 - Epoch 7 - Batch 70/88 - Loss: 2.2563\n",
            "Model 8 - Epoch 7 - Batch 80/88 - Loss: 2.2656\n",
            "Model 8 - Finished Epoch 7\n",
            "Model 8 - Starting Epoch 8\n",
            "Model 8 - Epoch 8 - Batch 10/88 - Loss: 2.1931\n",
            "Model 8 - Epoch 8 - Batch 20/88 - Loss: 2.0808\n",
            "Model 8 - Epoch 8 - Batch 30/88 - Loss: 2.1817\n",
            "Model 8 - Epoch 8 - Batch 40/88 - Loss: 2.3438\n",
            "Model 8 - Epoch 8 - Batch 50/88 - Loss: 2.1957\n",
            "Model 8 - Epoch 8 - Batch 60/88 - Loss: 2.1944\n",
            "Model 8 - Epoch 8 - Batch 70/88 - Loss: 2.2092\n",
            "Model 8 - Epoch 8 - Batch 80/88 - Loss: 2.2406\n",
            "Model 8 - Finished Epoch 8\n",
            "Model 8 - Starting Epoch 9\n",
            "Model 8 - Epoch 9 - Batch 10/88 - Loss: 2.0615\n",
            "Model 8 - Epoch 9 - Batch 20/88 - Loss: 2.1209\n",
            "Model 8 - Epoch 9 - Batch 30/88 - Loss: 2.2532\n",
            "Model 8 - Epoch 9 - Batch 40/88 - Loss: 2.1675\n",
            "Model 8 - Epoch 9 - Batch 50/88 - Loss: 2.2424\n",
            "Model 8 - Epoch 9 - Batch 60/88 - Loss: 2.0619\n",
            "Model 8 - Epoch 9 - Batch 70/88 - Loss: 2.3220\n",
            "Model 8 - Epoch 9 - Batch 80/88 - Loss: 2.1876\n",
            "Model 8 - Finished Epoch 9\n",
            "Model 8 - Starting Epoch 10\n",
            "Model 8 - Epoch 10 - Batch 10/88 - Loss: 2.0990\n",
            "Model 8 - Epoch 10 - Batch 20/88 - Loss: 2.0752\n",
            "Model 8 - Epoch 10 - Batch 30/88 - Loss: 2.1049\n",
            "Model 8 - Epoch 10 - Batch 40/88 - Loss: 2.3128\n",
            "Model 8 - Epoch 10 - Batch 50/88 - Loss: 2.0523\n",
            "Model 8 - Epoch 10 - Batch 60/88 - Loss: 2.2167\n",
            "Model 8 - Epoch 10 - Batch 70/88 - Loss: 2.1550\n",
            "Model 8 - Epoch 10 - Batch 80/88 - Loss: 2.1560\n",
            "Model 8 - Finished Epoch 10\n",
            "Model 8 done training for 10 epochs in generation 6\n",
            "Model 9 - Starting Epoch 1\n",
            "Model 9 - Epoch 1 - Batch 10/88 - Loss: 2.0993\n",
            "Model 9 - Epoch 1 - Batch 20/88 - Loss: 2.0993\n",
            "Model 9 - Epoch 1 - Batch 30/88 - Loss: 2.1544\n",
            "Model 9 - Epoch 1 - Batch 40/88 - Loss: 2.1227\n",
            "Model 9 - Epoch 1 - Batch 50/88 - Loss: 1.9689\n",
            "Model 9 - Epoch 1 - Batch 60/88 - Loss: 1.9469\n",
            "Model 9 - Epoch 1 - Batch 70/88 - Loss: 2.0293\n",
            "Model 9 - Epoch 1 - Batch 80/88 - Loss: 2.0588\n",
            "Model 9 - Finished Epoch 1\n",
            "Model 9 - Starting Epoch 2\n",
            "Model 9 - Epoch 2 - Batch 10/88 - Loss: 2.0263\n",
            "Model 9 - Epoch 2 - Batch 20/88 - Loss: 2.0564\n",
            "Model 9 - Epoch 2 - Batch 30/88 - Loss: 2.0045\n",
            "Model 9 - Epoch 2 - Batch 40/88 - Loss: 2.0401\n",
            "Model 9 - Epoch 2 - Batch 50/88 - Loss: 2.0345\n",
            "Model 9 - Epoch 2 - Batch 60/88 - Loss: 2.0409\n",
            "Model 9 - Epoch 2 - Batch 70/88 - Loss: 2.1471\n",
            "Model 9 - Epoch 2 - Batch 80/88 - Loss: 1.9981\n",
            "Model 9 - Finished Epoch 2\n",
            "Model 9 - Starting Epoch 3\n",
            "Model 9 - Epoch 3 - Batch 10/88 - Loss: 1.9924\n",
            "Model 9 - Epoch 3 - Batch 20/88 - Loss: 1.8652\n",
            "Model 9 - Epoch 3 - Batch 30/88 - Loss: 2.1179\n",
            "Model 9 - Epoch 3 - Batch 40/88 - Loss: 2.0559\n",
            "Model 9 - Epoch 3 - Batch 50/88 - Loss: 1.9966\n",
            "Model 9 - Epoch 3 - Batch 60/88 - Loss: 2.0430\n",
            "Model 9 - Epoch 3 - Batch 70/88 - Loss: 1.9596\n",
            "Model 9 - Epoch 3 - Batch 80/88 - Loss: 2.0024\n",
            "Model 9 - Finished Epoch 3\n",
            "Model 9 - Starting Epoch 4\n",
            "Model 9 - Epoch 4 - Batch 10/88 - Loss: 2.0324\n",
            "Model 9 - Epoch 4 - Batch 20/88 - Loss: 2.0330\n",
            "Model 9 - Epoch 4 - Batch 30/88 - Loss: 1.9830\n",
            "Model 9 - Epoch 4 - Batch 40/88 - Loss: 2.0059\n",
            "Model 9 - Epoch 4 - Batch 50/88 - Loss: 1.9342\n",
            "Model 9 - Epoch 4 - Batch 60/88 - Loss: 2.0369\n",
            "Model 9 - Epoch 4 - Batch 70/88 - Loss: 2.1393\n",
            "Model 9 - Epoch 4 - Batch 80/88 - Loss: 2.0316\n",
            "Model 9 - Finished Epoch 4\n",
            "Model 9 - Starting Epoch 5\n",
            "Model 9 - Epoch 5 - Batch 10/88 - Loss: 1.9195\n",
            "Model 9 - Epoch 5 - Batch 20/88 - Loss: 2.0158\n",
            "Model 9 - Epoch 5 - Batch 30/88 - Loss: 1.8699\n",
            "Model 9 - Epoch 5 - Batch 40/88 - Loss: 2.1024\n",
            "Model 9 - Epoch 5 - Batch 50/88 - Loss: 1.8555\n",
            "Model 9 - Epoch 5 - Batch 60/88 - Loss: 1.9510\n",
            "Model 9 - Epoch 5 - Batch 70/88 - Loss: 2.0484\n",
            "Model 9 - Epoch 5 - Batch 80/88 - Loss: 2.0677\n",
            "Model 9 - Finished Epoch 5\n",
            "Model 9 - Starting Epoch 6\n",
            "Model 9 - Epoch 6 - Batch 10/88 - Loss: 2.1439\n",
            "Model 9 - Epoch 6 - Batch 20/88 - Loss: 1.9068\n",
            "Model 9 - Epoch 6 - Batch 30/88 - Loss: 1.9309\n",
            "Model 9 - Epoch 6 - Batch 40/88 - Loss: 1.8926\n",
            "Model 9 - Epoch 6 - Batch 50/88 - Loss: 2.0364\n",
            "Model 9 - Epoch 6 - Batch 60/88 - Loss: 2.0600\n",
            "Model 9 - Epoch 6 - Batch 70/88 - Loss: 1.8797\n",
            "Model 9 - Epoch 6 - Batch 80/88 - Loss: 2.0291\n",
            "Model 9 - Finished Epoch 6\n",
            "Model 9 - Starting Epoch 7\n",
            "Model 9 - Epoch 7 - Batch 10/88 - Loss: 1.9220\n",
            "Model 9 - Epoch 7 - Batch 20/88 - Loss: 1.7609\n",
            "Model 9 - Epoch 7 - Batch 30/88 - Loss: 1.9255\n",
            "Model 9 - Epoch 7 - Batch 40/88 - Loss: 1.8723\n",
            "Model 9 - Epoch 7 - Batch 50/88 - Loss: 2.0403\n",
            "Model 9 - Epoch 7 - Batch 60/88 - Loss: 1.9046\n",
            "Model 9 - Epoch 7 - Batch 70/88 - Loss: 1.9437\n",
            "Model 9 - Epoch 7 - Batch 80/88 - Loss: 1.9987\n",
            "Model 9 - Finished Epoch 7\n",
            "Model 9 - Starting Epoch 8\n",
            "Model 9 - Epoch 8 - Batch 10/88 - Loss: 1.9058\n",
            "Model 9 - Epoch 8 - Batch 20/88 - Loss: 1.9527\n",
            "Model 9 - Epoch 8 - Batch 30/88 - Loss: 1.9349\n",
            "Model 9 - Epoch 8 - Batch 40/88 - Loss: 1.9054\n",
            "Model 9 - Epoch 8 - Batch 50/88 - Loss: 1.9295\n",
            "Model 9 - Epoch 8 - Batch 60/88 - Loss: 2.0106\n",
            "Model 9 - Epoch 8 - Batch 70/88 - Loss: 1.9175\n",
            "Model 9 - Epoch 8 - Batch 80/88 - Loss: 1.9156\n",
            "Model 9 - Finished Epoch 8\n",
            "Model 9 - Starting Epoch 9\n",
            "Model 9 - Epoch 9 - Batch 10/88 - Loss: 1.9944\n",
            "Model 9 - Epoch 9 - Batch 20/88 - Loss: 1.8432\n",
            "Model 9 - Epoch 9 - Batch 30/88 - Loss: 1.9550\n",
            "Model 9 - Epoch 9 - Batch 40/88 - Loss: 1.8942\n",
            "Model 9 - Epoch 9 - Batch 50/88 - Loss: 1.9208\n",
            "Model 9 - Epoch 9 - Batch 60/88 - Loss: 1.9926\n",
            "Model 9 - Epoch 9 - Batch 70/88 - Loss: 1.9787\n",
            "Model 9 - Epoch 9 - Batch 80/88 - Loss: 1.9668\n",
            "Model 9 - Finished Epoch 9\n",
            "Model 9 - Starting Epoch 10\n",
            "Model 9 - Epoch 10 - Batch 10/88 - Loss: 1.8738\n",
            "Model 9 - Epoch 10 - Batch 20/88 - Loss: 2.0290\n",
            "Model 9 - Epoch 10 - Batch 30/88 - Loss: 1.9248\n",
            "Model 9 - Epoch 10 - Batch 40/88 - Loss: 1.9722\n",
            "Model 9 - Epoch 10 - Batch 50/88 - Loss: 1.8987\n",
            "Model 9 - Epoch 10 - Batch 60/88 - Loss: 1.9228\n",
            "Model 9 - Epoch 10 - Batch 70/88 - Loss: 1.8336\n",
            "Model 9 - Epoch 10 - Batch 80/88 - Loss: 1.9736\n",
            "Model 9 - Finished Epoch 10\n",
            "Model 9 done training for 10 epochs in generation 6\n",
            "Model 10 - Starting Epoch 1\n",
            "Model 10 - Epoch 1 - Batch 10/88 - Loss: 2.0393\n",
            "Model 10 - Epoch 1 - Batch 20/88 - Loss: 2.0836\n",
            "Model 10 - Epoch 1 - Batch 30/88 - Loss: 2.2549\n",
            "Model 10 - Epoch 1 - Batch 40/88 - Loss: 2.1971\n",
            "Model 10 - Epoch 1 - Batch 50/88 - Loss: 2.0943\n",
            "Model 10 - Epoch 1 - Batch 60/88 - Loss: 2.0930\n",
            "Model 10 - Epoch 1 - Batch 70/88 - Loss: 2.0068\n",
            "Model 10 - Epoch 1 - Batch 80/88 - Loss: 2.1271\n",
            "Model 10 - Finished Epoch 1\n",
            "Model 10 - Starting Epoch 2\n",
            "Model 10 - Epoch 2 - Batch 10/88 - Loss: 1.9980\n",
            "Model 10 - Epoch 2 - Batch 20/88 - Loss: 2.0939\n",
            "Model 10 - Epoch 2 - Batch 30/88 - Loss: 1.9724\n",
            "Model 10 - Epoch 2 - Batch 40/88 - Loss: 2.1132\n",
            "Model 10 - Epoch 2 - Batch 50/88 - Loss: 1.9882\n",
            "Model 10 - Epoch 2 - Batch 60/88 - Loss: 2.0987\n",
            "Model 10 - Epoch 2 - Batch 70/88 - Loss: 2.1900\n",
            "Model 10 - Epoch 2 - Batch 80/88 - Loss: 2.2000\n",
            "Model 10 - Finished Epoch 2\n",
            "Model 10 - Starting Epoch 3\n",
            "Model 10 - Epoch 3 - Batch 10/88 - Loss: 1.9645\n",
            "Model 10 - Epoch 3 - Batch 20/88 - Loss: 2.1606\n",
            "Model 10 - Epoch 3 - Batch 30/88 - Loss: 2.1001\n",
            "Model 10 - Epoch 3 - Batch 40/88 - Loss: 2.0721\n",
            "Model 10 - Epoch 3 - Batch 50/88 - Loss: 2.1327\n",
            "Model 10 - Epoch 3 - Batch 60/88 - Loss: 2.1814\n",
            "Model 10 - Epoch 3 - Batch 70/88 - Loss: 2.1263\n",
            "Model 10 - Epoch 3 - Batch 80/88 - Loss: 2.0772\n",
            "Model 10 - Finished Epoch 3\n",
            "Model 10 - Starting Epoch 4\n",
            "Model 10 - Epoch 4 - Batch 10/88 - Loss: 2.0708\n",
            "Model 10 - Epoch 4 - Batch 20/88 - Loss: 2.0664\n",
            "Model 10 - Epoch 4 - Batch 30/88 - Loss: 2.0865\n",
            "Model 10 - Epoch 4 - Batch 40/88 - Loss: 2.1509\n",
            "Model 10 - Epoch 4 - Batch 50/88 - Loss: 2.0178\n",
            "Model 10 - Epoch 4 - Batch 60/88 - Loss: 2.0743\n",
            "Model 10 - Epoch 4 - Batch 70/88 - Loss: 2.1915\n",
            "Model 10 - Epoch 4 - Batch 80/88 - Loss: 2.1566\n",
            "Model 10 - Finished Epoch 4\n",
            "Model 10 - Starting Epoch 5\n",
            "Model 10 - Epoch 5 - Batch 10/88 - Loss: 2.0065\n",
            "Model 10 - Epoch 5 - Batch 20/88 - Loss: 2.1300\n",
            "Model 10 - Epoch 5 - Batch 30/88 - Loss: 2.0483\n",
            "Model 10 - Epoch 5 - Batch 40/88 - Loss: 2.0763\n",
            "Model 10 - Epoch 5 - Batch 50/88 - Loss: 2.1272\n",
            "Model 10 - Epoch 5 - Batch 60/88 - Loss: 2.1467\n",
            "Model 10 - Epoch 5 - Batch 70/88 - Loss: 2.1952\n",
            "Model 10 - Epoch 5 - Batch 80/88 - Loss: 2.0559\n",
            "Model 10 - Finished Epoch 5\n",
            "Model 10 - Starting Epoch 6\n",
            "Model 10 - Epoch 6 - Batch 10/88 - Loss: 2.0414\n",
            "Model 10 - Epoch 6 - Batch 20/88 - Loss: 2.0363\n",
            "Model 10 - Epoch 6 - Batch 30/88 - Loss: 2.0445\n",
            "Model 10 - Epoch 6 - Batch 40/88 - Loss: 2.0380\n",
            "Model 10 - Epoch 6 - Batch 50/88 - Loss: 2.0891\n",
            "Model 10 - Epoch 6 - Batch 60/88 - Loss: 2.1559\n",
            "Model 10 - Epoch 6 - Batch 70/88 - Loss: 2.1757\n",
            "Model 10 - Epoch 6 - Batch 80/88 - Loss: 2.0566\n",
            "Model 10 - Finished Epoch 6\n",
            "Model 10 - Starting Epoch 7\n",
            "Model 10 - Epoch 7 - Batch 10/88 - Loss: 2.0966\n",
            "Model 10 - Epoch 7 - Batch 20/88 - Loss: 2.0085\n",
            "Model 10 - Epoch 7 - Batch 30/88 - Loss: 2.0749\n",
            "Model 10 - Epoch 7 - Batch 40/88 - Loss: 2.0571\n",
            "Model 10 - Epoch 7 - Batch 50/88 - Loss: 2.0593\n",
            "Model 10 - Epoch 7 - Batch 60/88 - Loss: 2.0307\n",
            "Model 10 - Epoch 7 - Batch 70/88 - Loss: 2.0646\n",
            "Model 10 - Epoch 7 - Batch 80/88 - Loss: 2.1331\n",
            "Model 10 - Finished Epoch 7\n",
            "Model 10 - Starting Epoch 8\n",
            "Model 10 - Epoch 8 - Batch 10/88 - Loss: 1.9921\n",
            "Model 10 - Epoch 8 - Batch 20/88 - Loss: 1.9076\n",
            "Model 10 - Epoch 8 - Batch 30/88 - Loss: 2.0233\n",
            "Model 10 - Epoch 8 - Batch 40/88 - Loss: 1.9314\n",
            "Model 10 - Epoch 8 - Batch 50/88 - Loss: 2.0258\n",
            "Model 10 - Epoch 8 - Batch 60/88 - Loss: 2.0517\n",
            "Model 10 - Epoch 8 - Batch 70/88 - Loss: 2.1719\n",
            "Model 10 - Epoch 8 - Batch 80/88 - Loss: 2.0468\n",
            "Model 10 - Finished Epoch 8\n",
            "Model 10 - Starting Epoch 9\n",
            "Model 10 - Epoch 9 - Batch 10/88 - Loss: 2.0934\n",
            "Model 10 - Epoch 9 - Batch 20/88 - Loss: 2.0976\n",
            "Model 10 - Epoch 9 - Batch 30/88 - Loss: 2.0760\n",
            "Model 10 - Epoch 9 - Batch 40/88 - Loss: 1.9802\n",
            "Model 10 - Epoch 9 - Batch 50/88 - Loss: 1.9591\n",
            "Model 10 - Epoch 9 - Batch 60/88 - Loss: 2.0288\n",
            "Model 10 - Epoch 9 - Batch 70/88 - Loss: 2.0925\n",
            "Model 10 - Epoch 9 - Batch 80/88 - Loss: 2.0386\n",
            "Model 10 - Finished Epoch 9\n",
            "Model 10 - Starting Epoch 10\n",
            "Model 10 - Epoch 10 - Batch 10/88 - Loss: 2.0230\n",
            "Model 10 - Epoch 10 - Batch 20/88 - Loss: 2.1286\n",
            "Model 10 - Epoch 10 - Batch 30/88 - Loss: 1.9635\n",
            "Model 10 - Epoch 10 - Batch 40/88 - Loss: 2.0541\n",
            "Model 10 - Epoch 10 - Batch 50/88 - Loss: 2.0806\n",
            "Model 10 - Epoch 10 - Batch 60/88 - Loss: 2.0073\n",
            "Model 10 - Epoch 10 - Batch 70/88 - Loss: 2.0656\n",
            "Model 10 - Epoch 10 - Batch 80/88 - Loss: 2.0598\n",
            "Model 10 - Finished Epoch 10\n",
            "Model 10 done training for 10 epochs in generation 6\n",
            "Model 11 - Starting Epoch 1\n",
            "Model 11 - Epoch 1 - Batch 10/88 - Loss: 2.0966\n",
            "Model 11 - Epoch 1 - Batch 20/88 - Loss: 2.2210\n",
            "Model 11 - Epoch 1 - Batch 30/88 - Loss: 2.2710\n",
            "Model 11 - Epoch 1 - Batch 40/88 - Loss: 2.2841\n",
            "Model 11 - Epoch 1 - Batch 50/88 - Loss: 2.2426\n",
            "Model 11 - Epoch 1 - Batch 60/88 - Loss: 2.1859\n",
            "Model 11 - Epoch 1 - Batch 70/88 - Loss: 2.1450\n",
            "Model 11 - Epoch 1 - Batch 80/88 - Loss: 2.1929\n",
            "Model 11 - Finished Epoch 1\n",
            "Model 11 - Starting Epoch 2\n",
            "Model 11 - Epoch 2 - Batch 10/88 - Loss: 2.2066\n",
            "Model 11 - Epoch 2 - Batch 20/88 - Loss: 2.2307\n",
            "Model 11 - Epoch 2 - Batch 30/88 - Loss: 2.2519\n",
            "Model 11 - Epoch 2 - Batch 40/88 - Loss: 2.0623\n",
            "Model 11 - Epoch 2 - Batch 50/88 - Loss: 2.2447\n",
            "Model 11 - Epoch 2 - Batch 60/88 - Loss: 2.2722\n",
            "Model 11 - Epoch 2 - Batch 70/88 - Loss: 2.0923\n",
            "Model 11 - Epoch 2 - Batch 80/88 - Loss: 2.1150\n",
            "Model 11 - Finished Epoch 2\n",
            "Model 11 - Starting Epoch 3\n",
            "Model 11 - Epoch 3 - Batch 10/88 - Loss: 2.1339\n",
            "Model 11 - Epoch 3 - Batch 20/88 - Loss: 2.1167\n",
            "Model 11 - Epoch 3 - Batch 30/88 - Loss: 2.1560\n",
            "Model 11 - Epoch 3 - Batch 40/88 - Loss: 2.1370\n",
            "Model 11 - Epoch 3 - Batch 50/88 - Loss: 2.1429\n",
            "Model 11 - Epoch 3 - Batch 60/88 - Loss: 2.0477\n",
            "Model 11 - Epoch 3 - Batch 70/88 - Loss: 2.2995\n",
            "Model 11 - Epoch 3 - Batch 80/88 - Loss: 2.2243\n",
            "Model 11 - Finished Epoch 3\n",
            "Model 11 - Starting Epoch 4\n",
            "Model 11 - Epoch 4 - Batch 10/88 - Loss: 2.0452\n",
            "Model 11 - Epoch 4 - Batch 20/88 - Loss: 2.0527\n",
            "Model 11 - Epoch 4 - Batch 30/88 - Loss: 2.2010\n",
            "Model 11 - Epoch 4 - Batch 40/88 - Loss: 2.2107\n",
            "Model 11 - Epoch 4 - Batch 50/88 - Loss: 2.1967\n",
            "Model 11 - Epoch 4 - Batch 60/88 - Loss: 2.2882\n",
            "Model 11 - Epoch 4 - Batch 70/88 - Loss: 2.3077\n",
            "Model 11 - Epoch 4 - Batch 80/88 - Loss: 2.1915\n",
            "Model 11 - Finished Epoch 4\n",
            "Model 11 - Starting Epoch 5\n",
            "Model 11 - Epoch 5 - Batch 10/88 - Loss: 2.2022\n",
            "Model 11 - Epoch 5 - Batch 20/88 - Loss: 2.0558\n",
            "Model 11 - Epoch 5 - Batch 30/88 - Loss: 2.1384\n",
            "Model 11 - Epoch 5 - Batch 40/88 - Loss: 2.1484\n",
            "Model 11 - Epoch 5 - Batch 50/88 - Loss: 2.0959\n",
            "Model 11 - Epoch 5 - Batch 60/88 - Loss: 2.1503\n",
            "Model 11 - Epoch 5 - Batch 70/88 - Loss: 2.2428\n",
            "Model 11 - Epoch 5 - Batch 80/88 - Loss: 2.2737\n",
            "Model 11 - Finished Epoch 5\n",
            "Model 11 - Starting Epoch 6\n",
            "Model 11 - Epoch 6 - Batch 10/88 - Loss: 2.1102\n",
            "Model 11 - Epoch 6 - Batch 20/88 - Loss: 2.1760\n",
            "Model 11 - Epoch 6 - Batch 30/88 - Loss: 2.0423\n",
            "Model 11 - Epoch 6 - Batch 40/88 - Loss: 2.1041\n",
            "Model 11 - Epoch 6 - Batch 50/88 - Loss: 2.1927\n",
            "Model 11 - Epoch 6 - Batch 60/88 - Loss: 2.1135\n",
            "Model 11 - Epoch 6 - Batch 70/88 - Loss: 2.1442\n",
            "Model 11 - Epoch 6 - Batch 80/88 - Loss: 2.1986\n",
            "Model 11 - Finished Epoch 6\n",
            "Model 11 - Starting Epoch 7\n",
            "Model 11 - Epoch 7 - Batch 10/88 - Loss: 2.1088\n",
            "Model 11 - Epoch 7 - Batch 20/88 - Loss: 2.0242\n",
            "Model 11 - Epoch 7 - Batch 30/88 - Loss: 2.0701\n",
            "Model 11 - Epoch 7 - Batch 40/88 - Loss: 2.0520\n",
            "Model 11 - Epoch 7 - Batch 50/88 - Loss: 2.1823\n",
            "Model 11 - Epoch 7 - Batch 60/88 - Loss: 2.1071\n",
            "Model 11 - Epoch 7 - Batch 70/88 - Loss: 2.0903\n",
            "Model 11 - Epoch 7 - Batch 80/88 - Loss: 2.1786\n",
            "Model 11 - Finished Epoch 7\n",
            "Model 11 - Starting Epoch 8\n",
            "Model 11 - Epoch 8 - Batch 10/88 - Loss: 2.0417\n",
            "Model 11 - Epoch 8 - Batch 20/88 - Loss: 2.1631\n",
            "Model 11 - Epoch 8 - Batch 30/88 - Loss: 2.1459\n",
            "Model 11 - Epoch 8 - Batch 40/88 - Loss: 2.1446\n",
            "Model 11 - Epoch 8 - Batch 50/88 - Loss: 2.1081\n",
            "Model 11 - Epoch 8 - Batch 60/88 - Loss: 2.0521\n",
            "Model 11 - Epoch 8 - Batch 70/88 - Loss: 2.0954\n",
            "Model 11 - Epoch 8 - Batch 80/88 - Loss: 2.1284\n",
            "Model 11 - Finished Epoch 8\n",
            "Model 11 - Starting Epoch 9\n",
            "Model 11 - Epoch 9 - Batch 10/88 - Loss: 2.0467\n",
            "Model 11 - Epoch 9 - Batch 20/88 - Loss: 2.2273\n",
            "Model 11 - Epoch 9 - Batch 30/88 - Loss: 2.0836\n",
            "Model 11 - Epoch 9 - Batch 40/88 - Loss: 2.0986\n",
            "Model 11 - Epoch 9 - Batch 50/88 - Loss: 2.1308\n",
            "Model 11 - Epoch 9 - Batch 60/88 - Loss: 2.1670\n",
            "Model 11 - Epoch 9 - Batch 70/88 - Loss: 2.1113\n",
            "Model 11 - Epoch 9 - Batch 80/88 - Loss: 2.1828\n",
            "Model 11 - Finished Epoch 9\n",
            "Model 11 - Starting Epoch 10\n",
            "Model 11 - Epoch 10 - Batch 10/88 - Loss: 2.1558\n",
            "Model 11 - Epoch 10 - Batch 20/88 - Loss: 1.8879\n",
            "Model 11 - Epoch 10 - Batch 30/88 - Loss: 2.0764\n",
            "Model 11 - Epoch 10 - Batch 40/88 - Loss: 2.1179\n",
            "Model 11 - Epoch 10 - Batch 50/88 - Loss: 2.1252\n",
            "Model 11 - Epoch 10 - Batch 60/88 - Loss: 2.2414\n",
            "Model 11 - Epoch 10 - Batch 70/88 - Loss: 2.0913\n",
            "Model 11 - Epoch 10 - Batch 80/88 - Loss: 2.1074\n",
            "Model 11 - Finished Epoch 10\n",
            "Model 11 done training for 10 epochs in generation 6\n",
            "Model 12 - Starting Epoch 1\n",
            "Model 12 - Epoch 1 - Batch 10/88 - Loss: 2.1948\n",
            "Model 12 - Epoch 1 - Batch 20/88 - Loss: 2.0274\n",
            "Model 12 - Epoch 1 - Batch 30/88 - Loss: 2.1391\n",
            "Model 12 - Epoch 1 - Batch 40/88 - Loss: 2.1035\n",
            "Model 12 - Epoch 1 - Batch 50/88 - Loss: 2.0723\n",
            "Model 12 - Epoch 1 - Batch 60/88 - Loss: 2.2377\n",
            "Model 12 - Epoch 1 - Batch 70/88 - Loss: 2.2157\n",
            "Model 12 - Epoch 1 - Batch 80/88 - Loss: 2.0976\n",
            "Model 12 - Finished Epoch 1\n",
            "Model 12 - Starting Epoch 2\n",
            "Model 12 - Epoch 2 - Batch 10/88 - Loss: 2.0625\n",
            "Model 12 - Epoch 2 - Batch 20/88 - Loss: 2.1004\n",
            "Model 12 - Epoch 2 - Batch 30/88 - Loss: 2.1107\n",
            "Model 12 - Epoch 2 - Batch 40/88 - Loss: 2.0542\n",
            "Model 12 - Epoch 2 - Batch 50/88 - Loss: 2.0646\n",
            "Model 12 - Epoch 2 - Batch 60/88 - Loss: 2.1680\n",
            "Model 12 - Epoch 2 - Batch 70/88 - Loss: 2.1484\n",
            "Model 12 - Epoch 2 - Batch 80/88 - Loss: 2.0241\n",
            "Model 12 - Finished Epoch 2\n",
            "Model 12 - Starting Epoch 3\n",
            "Model 12 - Epoch 3 - Batch 10/88 - Loss: 2.2451\n",
            "Model 12 - Epoch 3 - Batch 20/88 - Loss: 2.1296\n",
            "Model 12 - Epoch 3 - Batch 30/88 - Loss: 2.0862\n",
            "Model 12 - Epoch 3 - Batch 40/88 - Loss: 2.1002\n",
            "Model 12 - Epoch 3 - Batch 50/88 - Loss: 2.0868\n",
            "Model 12 - Epoch 3 - Batch 60/88 - Loss: 2.1523\n",
            "Model 12 - Epoch 3 - Batch 70/88 - Loss: 2.1695\n",
            "Model 12 - Epoch 3 - Batch 80/88 - Loss: 2.1173\n",
            "Model 12 - Finished Epoch 3\n",
            "Model 12 - Starting Epoch 4\n",
            "Model 12 - Epoch 4 - Batch 10/88 - Loss: 2.0237\n",
            "Model 12 - Epoch 4 - Batch 20/88 - Loss: 2.1415\n",
            "Model 12 - Epoch 4 - Batch 30/88 - Loss: 2.0208\n",
            "Model 12 - Epoch 4 - Batch 40/88 - Loss: 2.0858\n",
            "Model 12 - Epoch 4 - Batch 50/88 - Loss: 2.0817\n",
            "Model 12 - Epoch 4 - Batch 60/88 - Loss: 2.1607\n",
            "Model 12 - Epoch 4 - Batch 70/88 - Loss: 2.1280\n",
            "Model 12 - Epoch 4 - Batch 80/88 - Loss: 2.1477\n",
            "Model 12 - Finished Epoch 4\n",
            "Model 12 - Starting Epoch 5\n",
            "Model 12 - Epoch 5 - Batch 10/88 - Loss: 1.9322\n",
            "Model 12 - Epoch 5 - Batch 20/88 - Loss: 2.0765\n",
            "Model 12 - Epoch 5 - Batch 30/88 - Loss: 2.0768\n",
            "Model 12 - Epoch 5 - Batch 40/88 - Loss: 2.2095\n",
            "Model 12 - Epoch 5 - Batch 50/88 - Loss: 2.0813\n",
            "Model 12 - Epoch 5 - Batch 60/88 - Loss: 2.0616\n",
            "Model 12 - Epoch 5 - Batch 70/88 - Loss: 2.1049\n",
            "Model 12 - Epoch 5 - Batch 80/88 - Loss: 2.1045\n",
            "Model 12 - Finished Epoch 5\n",
            "Model 12 - Starting Epoch 6\n",
            "Model 12 - Epoch 6 - Batch 10/88 - Loss: 2.0331\n",
            "Model 12 - Epoch 6 - Batch 20/88 - Loss: 2.1109\n",
            "Model 12 - Epoch 6 - Batch 30/88 - Loss: 2.0871\n",
            "Model 12 - Epoch 6 - Batch 40/88 - Loss: 2.0049\n",
            "Model 12 - Epoch 6 - Batch 50/88 - Loss: 1.9808\n",
            "Model 12 - Epoch 6 - Batch 60/88 - Loss: 2.0795\n",
            "Model 12 - Epoch 6 - Batch 70/88 - Loss: 2.1413\n",
            "Model 12 - Epoch 6 - Batch 80/88 - Loss: 2.1431\n",
            "Model 12 - Finished Epoch 6\n",
            "Model 12 - Starting Epoch 7\n",
            "Model 12 - Epoch 7 - Batch 10/88 - Loss: 2.1069\n",
            "Model 12 - Epoch 7 - Batch 20/88 - Loss: 2.0161\n",
            "Model 12 - Epoch 7 - Batch 30/88 - Loss: 2.1342\n",
            "Model 12 - Epoch 7 - Batch 40/88 - Loss: 2.0625\n",
            "Model 12 - Epoch 7 - Batch 50/88 - Loss: 2.1287\n",
            "Model 12 - Epoch 7 - Batch 60/88 - Loss: 1.9910\n",
            "Model 12 - Epoch 7 - Batch 70/88 - Loss: 2.1159\n",
            "Model 12 - Epoch 7 - Batch 80/88 - Loss: 2.0610\n",
            "Model 12 - Finished Epoch 7\n",
            "Model 12 - Starting Epoch 8\n",
            "Model 12 - Epoch 8 - Batch 10/88 - Loss: 1.8992\n",
            "Model 12 - Epoch 8 - Batch 20/88 - Loss: 1.9049\n",
            "Model 12 - Epoch 8 - Batch 30/88 - Loss: 2.0376\n",
            "Model 12 - Epoch 8 - Batch 40/88 - Loss: 2.0001\n",
            "Model 12 - Epoch 8 - Batch 50/88 - Loss: 2.1026\n",
            "Model 12 - Epoch 8 - Batch 60/88 - Loss: 2.0603\n",
            "Model 12 - Epoch 8 - Batch 70/88 - Loss: 2.1363\n",
            "Model 12 - Epoch 8 - Batch 80/88 - Loss: 2.0558\n",
            "Model 12 - Finished Epoch 8\n",
            "Model 12 - Starting Epoch 9\n",
            "Model 12 - Epoch 9 - Batch 10/88 - Loss: 1.9698\n",
            "Model 12 - Epoch 9 - Batch 20/88 - Loss: 2.0739\n",
            "Model 12 - Epoch 9 - Batch 30/88 - Loss: 2.0375\n",
            "Model 12 - Epoch 9 - Batch 40/88 - Loss: 2.1879\n",
            "Model 12 - Epoch 9 - Batch 50/88 - Loss: 2.0730\n",
            "Model 12 - Epoch 9 - Batch 60/88 - Loss: 2.1066\n",
            "Model 12 - Epoch 9 - Batch 70/88 - Loss: 1.9447\n",
            "Model 12 - Epoch 9 - Batch 80/88 - Loss: 2.0777\n",
            "Model 12 - Finished Epoch 9\n",
            "Model 12 - Starting Epoch 10\n",
            "Model 12 - Epoch 10 - Batch 10/88 - Loss: 1.9632\n",
            "Model 12 - Epoch 10 - Batch 20/88 - Loss: 2.0662\n",
            "Model 12 - Epoch 10 - Batch 30/88 - Loss: 2.0460\n",
            "Model 12 - Epoch 10 - Batch 40/88 - Loss: 2.0748\n",
            "Model 12 - Epoch 10 - Batch 50/88 - Loss: 2.0756\n",
            "Model 12 - Epoch 10 - Batch 60/88 - Loss: 2.0695\n",
            "Model 12 - Epoch 10 - Batch 70/88 - Loss: 2.0126\n",
            "Model 12 - Epoch 10 - Batch 80/88 - Loss: 2.0309\n",
            "Model 12 - Finished Epoch 10\n",
            "Model 12 done training for 10 epochs in generation 6\n",
            "Model 13 - Starting Epoch 1\n",
            "Model 13 - Epoch 1 - Batch 10/88 - Loss: 2.0289\n",
            "Model 13 - Epoch 1 - Batch 20/88 - Loss: 1.8139\n",
            "Model 13 - Epoch 1 - Batch 30/88 - Loss: 2.0791\n",
            "Model 13 - Epoch 1 - Batch 40/88 - Loss: 2.0021\n",
            "Model 13 - Epoch 1 - Batch 50/88 - Loss: 2.0688\n",
            "Model 13 - Epoch 1 - Batch 60/88 - Loss: 1.9718\n",
            "Model 13 - Epoch 1 - Batch 70/88 - Loss: 2.0058\n",
            "Model 13 - Epoch 1 - Batch 80/88 - Loss: 2.0404\n",
            "Model 13 - Finished Epoch 1\n",
            "Model 13 - Starting Epoch 2\n",
            "Model 13 - Epoch 2 - Batch 10/88 - Loss: 1.9145\n",
            "Model 13 - Epoch 2 - Batch 20/88 - Loss: 2.0010\n",
            "Model 13 - Epoch 2 - Batch 30/88 - Loss: 1.9199\n",
            "Model 13 - Epoch 2 - Batch 40/88 - Loss: 1.9465\n",
            "Model 13 - Epoch 2 - Batch 50/88 - Loss: 1.8243\n",
            "Model 13 - Epoch 2 - Batch 60/88 - Loss: 1.9534\n",
            "Model 13 - Epoch 2 - Batch 70/88 - Loss: 1.9849\n",
            "Model 13 - Epoch 2 - Batch 80/88 - Loss: 2.1317\n",
            "Model 13 - Finished Epoch 2\n",
            "Model 13 - Starting Epoch 3\n",
            "Model 13 - Epoch 3 - Batch 10/88 - Loss: 1.9554\n",
            "Model 13 - Epoch 3 - Batch 20/88 - Loss: 1.8844\n",
            "Model 13 - Epoch 3 - Batch 30/88 - Loss: 1.8962\n",
            "Model 13 - Epoch 3 - Batch 40/88 - Loss: 1.9900\n",
            "Model 13 - Epoch 3 - Batch 50/88 - Loss: 1.9443\n",
            "Model 13 - Epoch 3 - Batch 60/88 - Loss: 1.9995\n",
            "Model 13 - Epoch 3 - Batch 70/88 - Loss: 1.9595\n",
            "Model 13 - Epoch 3 - Batch 80/88 - Loss: 1.9539\n",
            "Model 13 - Finished Epoch 3\n",
            "Model 13 - Starting Epoch 4\n",
            "Model 13 - Epoch 4 - Batch 10/88 - Loss: 1.8294\n",
            "Model 13 - Epoch 4 - Batch 20/88 - Loss: 1.9558\n",
            "Model 13 - Epoch 4 - Batch 30/88 - Loss: 1.8343\n",
            "Model 13 - Epoch 4 - Batch 40/88 - Loss: 1.9461\n",
            "Model 13 - Epoch 4 - Batch 50/88 - Loss: 2.0107\n",
            "Model 13 - Epoch 4 - Batch 60/88 - Loss: 1.9548\n",
            "Model 13 - Epoch 4 - Batch 70/88 - Loss: 1.8899\n",
            "Model 13 - Epoch 4 - Batch 80/88 - Loss: 2.1381\n",
            "Model 13 - Finished Epoch 4\n",
            "Model 13 - Starting Epoch 5\n",
            "Model 13 - Epoch 5 - Batch 10/88 - Loss: 1.9575\n",
            "Model 13 - Epoch 5 - Batch 20/88 - Loss: 1.9212\n",
            "Model 13 - Epoch 5 - Batch 30/88 - Loss: 1.8132\n",
            "Model 13 - Epoch 5 - Batch 40/88 - Loss: 2.0236\n",
            "Model 13 - Epoch 5 - Batch 50/88 - Loss: 1.9368\n",
            "Model 13 - Epoch 5 - Batch 60/88 - Loss: 1.9054\n",
            "Model 13 - Epoch 5 - Batch 70/88 - Loss: 1.9490\n",
            "Model 13 - Epoch 5 - Batch 80/88 - Loss: 1.9865\n",
            "Model 13 - Finished Epoch 5\n",
            "Model 13 - Starting Epoch 6\n",
            "Model 13 - Epoch 6 - Batch 10/88 - Loss: 1.8162\n",
            "Model 13 - Epoch 6 - Batch 20/88 - Loss: 2.0377\n",
            "Model 13 - Epoch 6 - Batch 30/88 - Loss: 1.9367\n",
            "Model 13 - Epoch 6 - Batch 40/88 - Loss: 1.8098\n",
            "Model 13 - Epoch 6 - Batch 50/88 - Loss: 1.8991\n",
            "Model 13 - Epoch 6 - Batch 60/88 - Loss: 1.8947\n",
            "Model 13 - Epoch 6 - Batch 70/88 - Loss: 1.9439\n",
            "Model 13 - Epoch 6 - Batch 80/88 - Loss: 1.8373\n",
            "Model 13 - Finished Epoch 6\n",
            "Model 13 - Starting Epoch 7\n",
            "Model 13 - Epoch 7 - Batch 10/88 - Loss: 1.7587\n",
            "Model 13 - Epoch 7 - Batch 20/88 - Loss: 1.7915\n",
            "Model 13 - Epoch 7 - Batch 30/88 - Loss: 1.8086\n",
            "Model 13 - Epoch 7 - Batch 40/88 - Loss: 1.7313\n",
            "Model 13 - Epoch 7 - Batch 50/88 - Loss: 1.8501\n",
            "Model 13 - Epoch 7 - Batch 60/88 - Loss: 1.8819\n",
            "Model 13 - Epoch 7 - Batch 70/88 - Loss: 1.8580\n",
            "Model 13 - Epoch 7 - Batch 80/88 - Loss: 1.8424\n",
            "Model 13 - Finished Epoch 7\n",
            "Model 13 - Starting Epoch 8\n",
            "Model 13 - Epoch 8 - Batch 10/88 - Loss: 1.8514\n",
            "Model 13 - Epoch 8 - Batch 20/88 - Loss: 1.8304\n",
            "Model 13 - Epoch 8 - Batch 30/88 - Loss: 1.8728\n",
            "Model 13 - Epoch 8 - Batch 40/88 - Loss: 1.9724\n",
            "Model 13 - Epoch 8 - Batch 50/88 - Loss: 1.9159\n",
            "Model 13 - Epoch 8 - Batch 60/88 - Loss: 1.9211\n",
            "Model 13 - Epoch 8 - Batch 70/88 - Loss: 1.9830\n",
            "Model 13 - Epoch 8 - Batch 80/88 - Loss: 1.9414\n",
            "Model 13 - Finished Epoch 8\n",
            "Model 13 - Starting Epoch 9\n",
            "Model 13 - Epoch 9 - Batch 10/88 - Loss: 1.8412\n",
            "Model 13 - Epoch 9 - Batch 20/88 - Loss: 1.8601\n",
            "Model 13 - Epoch 9 - Batch 30/88 - Loss: 1.8656\n",
            "Model 13 - Epoch 9 - Batch 40/88 - Loss: 1.8188\n",
            "Model 13 - Epoch 9 - Batch 50/88 - Loss: 1.8454\n",
            "Model 13 - Epoch 9 - Batch 60/88 - Loss: 1.8197\n",
            "Model 13 - Epoch 9 - Batch 70/88 - Loss: 1.8971\n",
            "Model 13 - Epoch 9 - Batch 80/88 - Loss: 1.8286\n",
            "Model 13 - Finished Epoch 9\n",
            "Model 13 - Starting Epoch 10\n",
            "Model 13 - Epoch 10 - Batch 10/88 - Loss: 1.8567\n",
            "Model 13 - Epoch 10 - Batch 20/88 - Loss: 1.7577\n",
            "Model 13 - Epoch 10 - Batch 30/88 - Loss: 1.8345\n",
            "Model 13 - Epoch 10 - Batch 40/88 - Loss: 1.9755\n",
            "Model 13 - Epoch 10 - Batch 50/88 - Loss: 1.8128\n",
            "Model 13 - Epoch 10 - Batch 60/88 - Loss: 1.9361\n",
            "Model 13 - Epoch 10 - Batch 70/88 - Loss: 1.9788\n",
            "Model 13 - Epoch 10 - Batch 80/88 - Loss: 1.8779\n",
            "Model 13 - Finished Epoch 10\n",
            "Model 13 done training for 10 epochs in generation 6\n",
            "Model 14 - Starting Epoch 1\n",
            "Model 14 - Epoch 1 - Batch 10/88 - Loss: 2.1306\n",
            "Model 14 - Epoch 1 - Batch 20/88 - Loss: 2.0868\n",
            "Model 14 - Epoch 1 - Batch 30/88 - Loss: 2.2736\n",
            "Model 14 - Epoch 1 - Batch 40/88 - Loss: 2.1095\n",
            "Model 14 - Epoch 1 - Batch 50/88 - Loss: 2.2948\n",
            "Model 14 - Epoch 1 - Batch 60/88 - Loss: 2.1932\n",
            "Model 14 - Epoch 1 - Batch 70/88 - Loss: 2.1722\n",
            "Model 14 - Epoch 1 - Batch 80/88 - Loss: 2.1924\n",
            "Model 14 - Finished Epoch 1\n",
            "Model 14 - Starting Epoch 2\n",
            "Model 14 - Epoch 2 - Batch 10/88 - Loss: 2.0785\n",
            "Model 14 - Epoch 2 - Batch 20/88 - Loss: 2.1453\n",
            "Model 14 - Epoch 2 - Batch 30/88 - Loss: 2.1731\n",
            "Model 14 - Epoch 2 - Batch 40/88 - Loss: 2.1919\n",
            "Model 14 - Epoch 2 - Batch 50/88 - Loss: 2.1619\n",
            "Model 14 - Epoch 2 - Batch 60/88 - Loss: 2.1063\n",
            "Model 14 - Epoch 2 - Batch 70/88 - Loss: 2.1094\n",
            "Model 14 - Epoch 2 - Batch 80/88 - Loss: 2.1196\n",
            "Model 14 - Finished Epoch 2\n",
            "Model 14 - Starting Epoch 3\n",
            "Model 14 - Epoch 3 - Batch 10/88 - Loss: 2.1538\n",
            "Model 14 - Epoch 3 - Batch 20/88 - Loss: 2.1311\n",
            "Model 14 - Epoch 3 - Batch 30/88 - Loss: 2.1065\n",
            "Model 14 - Epoch 3 - Batch 40/88 - Loss: 2.2455\n",
            "Model 14 - Epoch 3 - Batch 50/88 - Loss: 2.1577\n",
            "Model 14 - Epoch 3 - Batch 60/88 - Loss: 2.1018\n",
            "Model 14 - Epoch 3 - Batch 70/88 - Loss: 2.1402\n",
            "Model 14 - Epoch 3 - Batch 80/88 - Loss: 2.1517\n",
            "Model 14 - Finished Epoch 3\n",
            "Model 14 - Starting Epoch 4\n",
            "Model 14 - Epoch 4 - Batch 10/88 - Loss: 2.0483\n",
            "Model 14 - Epoch 4 - Batch 20/88 - Loss: 2.0738\n",
            "Model 14 - Epoch 4 - Batch 30/88 - Loss: 2.0207\n",
            "Model 14 - Epoch 4 - Batch 40/88 - Loss: 2.1934\n",
            "Model 14 - Epoch 4 - Batch 50/88 - Loss: 2.1240\n",
            "Model 14 - Epoch 4 - Batch 60/88 - Loss: 2.0843\n",
            "Model 14 - Epoch 4 - Batch 70/88 - Loss: 2.0986\n",
            "Model 14 - Epoch 4 - Batch 80/88 - Loss: 2.0910\n",
            "Model 14 - Finished Epoch 4\n",
            "Model 14 - Starting Epoch 5\n",
            "Model 14 - Epoch 5 - Batch 10/88 - Loss: 2.0525\n",
            "Model 14 - Epoch 5 - Batch 20/88 - Loss: 2.1424\n",
            "Model 14 - Epoch 5 - Batch 30/88 - Loss: 2.0536\n",
            "Model 14 - Epoch 5 - Batch 40/88 - Loss: 2.0614\n",
            "Model 14 - Epoch 5 - Batch 50/88 - Loss: 2.1232\n",
            "Model 14 - Epoch 5 - Batch 60/88 - Loss: 2.1099\n",
            "Model 14 - Epoch 5 - Batch 70/88 - Loss: 2.1720\n",
            "Model 14 - Epoch 5 - Batch 80/88 - Loss: 2.1398\n",
            "Model 14 - Finished Epoch 5\n",
            "Model 14 - Starting Epoch 6\n",
            "Model 14 - Epoch 6 - Batch 10/88 - Loss: 2.0403\n",
            "Model 14 - Epoch 6 - Batch 20/88 - Loss: 1.9516\n",
            "Model 14 - Epoch 6 - Batch 30/88 - Loss: 2.0246\n",
            "Model 14 - Epoch 6 - Batch 40/88 - Loss: 1.9221\n",
            "Model 14 - Epoch 6 - Batch 50/88 - Loss: 2.1202\n",
            "Model 14 - Epoch 6 - Batch 60/88 - Loss: 1.9859\n",
            "Model 14 - Epoch 6 - Batch 70/88 - Loss: 2.1503\n",
            "Model 14 - Epoch 6 - Batch 80/88 - Loss: 2.1872\n",
            "Model 14 - Finished Epoch 6\n",
            "Model 14 - Starting Epoch 7\n",
            "Model 14 - Epoch 7 - Batch 10/88 - Loss: 1.9952\n",
            "Model 14 - Epoch 7 - Batch 20/88 - Loss: 2.0280\n",
            "Model 14 - Epoch 7 - Batch 30/88 - Loss: 2.1376\n",
            "Model 14 - Epoch 7 - Batch 40/88 - Loss: 2.0962\n",
            "Model 14 - Epoch 7 - Batch 50/88 - Loss: 2.0683\n",
            "Model 14 - Epoch 7 - Batch 60/88 - Loss: 2.1281\n",
            "Model 14 - Epoch 7 - Batch 70/88 - Loss: 2.0655\n",
            "Model 14 - Epoch 7 - Batch 80/88 - Loss: 2.0558\n",
            "Model 14 - Finished Epoch 7\n",
            "Model 14 - Starting Epoch 8\n",
            "Model 14 - Epoch 8 - Batch 10/88 - Loss: 2.0034\n",
            "Model 14 - Epoch 8 - Batch 20/88 - Loss: 2.0022\n",
            "Model 14 - Epoch 8 - Batch 30/88 - Loss: 1.9698\n",
            "Model 14 - Epoch 8 - Batch 40/88 - Loss: 2.0857\n",
            "Model 14 - Epoch 8 - Batch 50/88 - Loss: 2.0570\n",
            "Model 14 - Epoch 8 - Batch 60/88 - Loss: 2.0929\n",
            "Model 14 - Epoch 8 - Batch 70/88 - Loss: 2.0690\n",
            "Model 14 - Epoch 8 - Batch 80/88 - Loss: 2.1250\n",
            "Model 14 - Finished Epoch 8\n",
            "Model 14 - Starting Epoch 9\n",
            "Model 14 - Epoch 9 - Batch 10/88 - Loss: 2.0927\n",
            "Model 14 - Epoch 9 - Batch 20/88 - Loss: 2.0827\n",
            "Model 14 - Epoch 9 - Batch 30/88 - Loss: 2.0650\n",
            "Model 14 - Epoch 9 - Batch 40/88 - Loss: 2.0297\n",
            "Model 14 - Epoch 9 - Batch 50/88 - Loss: 2.0129\n",
            "Model 14 - Epoch 9 - Batch 60/88 - Loss: 2.0034\n",
            "Model 14 - Epoch 9 - Batch 70/88 - Loss: 1.9777\n",
            "Model 14 - Epoch 9 - Batch 80/88 - Loss: 2.1020\n",
            "Model 14 - Finished Epoch 9\n",
            "Model 14 - Starting Epoch 10\n",
            "Model 14 - Epoch 10 - Batch 10/88 - Loss: 1.9259\n",
            "Model 14 - Epoch 10 - Batch 20/88 - Loss: 1.9102\n",
            "Model 14 - Epoch 10 - Batch 30/88 - Loss: 2.0524\n",
            "Model 14 - Epoch 10 - Batch 40/88 - Loss: 1.9809\n",
            "Model 14 - Epoch 10 - Batch 50/88 - Loss: 2.0007\n",
            "Model 14 - Epoch 10 - Batch 60/88 - Loss: 2.0493\n",
            "Model 14 - Epoch 10 - Batch 70/88 - Loss: 2.0447\n",
            "Model 14 - Epoch 10 - Batch 80/88 - Loss: 1.9400\n",
            "Model 14 - Finished Epoch 10\n",
            "Model 14 done training for 10 epochs in generation 6\n",
            "Model 15 - Starting Epoch 1\n",
            "Model 15 - Epoch 1 - Batch 10/88 - Loss: 2.2822\n",
            "Model 15 - Epoch 1 - Batch 20/88 - Loss: 2.3132\n",
            "Model 15 - Epoch 1 - Batch 30/88 - Loss: 2.4538\n",
            "Model 15 - Epoch 1 - Batch 40/88 - Loss: 2.3459\n",
            "Model 15 - Epoch 1 - Batch 50/88 - Loss: 2.2556\n",
            "Model 15 - Epoch 1 - Batch 60/88 - Loss: 2.3132\n",
            "Model 15 - Epoch 1 - Batch 70/88 - Loss: 2.3136\n",
            "Model 15 - Epoch 1 - Batch 80/88 - Loss: 2.3089\n",
            "Model 15 - Finished Epoch 1\n",
            "Model 15 - Starting Epoch 2\n",
            "Model 15 - Epoch 2 - Batch 10/88 - Loss: 2.0723\n",
            "Model 15 - Epoch 2 - Batch 20/88 - Loss: 2.2291\n",
            "Model 15 - Epoch 2 - Batch 30/88 - Loss: 2.2830\n",
            "Model 15 - Epoch 2 - Batch 40/88 - Loss: 2.3977\n",
            "Model 15 - Epoch 2 - Batch 50/88 - Loss: 2.3566\n",
            "Model 15 - Epoch 2 - Batch 60/88 - Loss: 2.2991\n",
            "Model 15 - Epoch 2 - Batch 70/88 - Loss: 2.3919\n",
            "Model 15 - Epoch 2 - Batch 80/88 - Loss: 2.2846\n",
            "Model 15 - Finished Epoch 2\n",
            "Model 15 - Starting Epoch 3\n",
            "Model 15 - Epoch 3 - Batch 10/88 - Loss: 2.2560\n",
            "Model 15 - Epoch 3 - Batch 20/88 - Loss: 2.2329\n",
            "Model 15 - Epoch 3 - Batch 30/88 - Loss: 2.4005\n",
            "Model 15 - Epoch 3 - Batch 40/88 - Loss: 2.3618\n",
            "Model 15 - Epoch 3 - Batch 50/88 - Loss: 2.3060\n",
            "Model 15 - Epoch 3 - Batch 60/88 - Loss: 2.3211\n",
            "Model 15 - Epoch 3 - Batch 70/88 - Loss: 2.4258\n",
            "Model 15 - Epoch 3 - Batch 80/88 - Loss: 2.1805\n",
            "Model 15 - Finished Epoch 3\n",
            "Model 15 - Starting Epoch 4\n",
            "Model 15 - Epoch 4 - Batch 10/88 - Loss: 2.2410\n",
            "Model 15 - Epoch 4 - Batch 20/88 - Loss: 2.2464\n",
            "Model 15 - Epoch 4 - Batch 30/88 - Loss: 2.2865\n",
            "Model 15 - Epoch 4 - Batch 40/88 - Loss: 2.1589\n",
            "Model 15 - Epoch 4 - Batch 50/88 - Loss: 2.3281\n",
            "Model 15 - Epoch 4 - Batch 60/88 - Loss: 2.2816\n",
            "Model 15 - Epoch 4 - Batch 70/88 - Loss: 2.4785\n",
            "Model 15 - Epoch 4 - Batch 80/88 - Loss: 2.2392\n",
            "Model 15 - Finished Epoch 4\n",
            "Model 15 - Starting Epoch 5\n",
            "Model 15 - Epoch 5 - Batch 10/88 - Loss: 2.2852\n",
            "Model 15 - Epoch 5 - Batch 20/88 - Loss: 2.3170\n",
            "Model 15 - Epoch 5 - Batch 30/88 - Loss: 2.1328\n",
            "Model 15 - Epoch 5 - Batch 40/88 - Loss: 2.3073\n",
            "Model 15 - Epoch 5 - Batch 50/88 - Loss: 2.3613\n",
            "Model 15 - Epoch 5 - Batch 60/88 - Loss: 2.3024\n",
            "Model 15 - Epoch 5 - Batch 70/88 - Loss: 2.2779\n",
            "Model 15 - Epoch 5 - Batch 80/88 - Loss: 2.2131\n",
            "Model 15 - Finished Epoch 5\n",
            "Model 15 - Starting Epoch 6\n",
            "Model 15 - Epoch 6 - Batch 10/88 - Loss: 2.1830\n",
            "Model 15 - Epoch 6 - Batch 20/88 - Loss: 2.2810\n",
            "Model 15 - Epoch 6 - Batch 30/88 - Loss: 2.1802\n",
            "Model 15 - Epoch 6 - Batch 40/88 - Loss: 2.1155\n",
            "Model 15 - Epoch 6 - Batch 50/88 - Loss: 2.3243\n",
            "Model 15 - Epoch 6 - Batch 60/88 - Loss: 2.2333\n",
            "Model 15 - Epoch 6 - Batch 70/88 - Loss: 2.2928\n",
            "Model 15 - Epoch 6 - Batch 80/88 - Loss: 2.3809\n",
            "Model 15 - Finished Epoch 6\n",
            "Model 15 - Starting Epoch 7\n",
            "Model 15 - Epoch 7 - Batch 10/88 - Loss: 2.1846\n",
            "Model 15 - Epoch 7 - Batch 20/88 - Loss: 2.2720\n",
            "Model 15 - Epoch 7 - Batch 30/88 - Loss: 2.2835\n",
            "Model 15 - Epoch 7 - Batch 40/88 - Loss: 2.2842\n",
            "Model 15 - Epoch 7 - Batch 50/88 - Loss: 2.3059\n",
            "Model 15 - Epoch 7 - Batch 60/88 - Loss: 2.2333\n",
            "Model 15 - Epoch 7 - Batch 70/88 - Loss: 2.1326\n",
            "Model 15 - Epoch 7 - Batch 80/88 - Loss: 2.2673\n",
            "Model 15 - Finished Epoch 7\n",
            "Model 15 - Starting Epoch 8\n",
            "Model 15 - Epoch 8 - Batch 10/88 - Loss: 2.1815\n",
            "Model 15 - Epoch 8 - Batch 20/88 - Loss: 2.1145\n",
            "Model 15 - Epoch 8 - Batch 30/88 - Loss: 2.2869\n",
            "Model 15 - Epoch 8 - Batch 40/88 - Loss: 2.2846\n",
            "Model 15 - Epoch 8 - Batch 50/88 - Loss: 2.2783\n",
            "Model 15 - Epoch 8 - Batch 60/88 - Loss: 2.1788\n",
            "Model 15 - Epoch 8 - Batch 70/88 - Loss: 2.1195\n",
            "Model 15 - Epoch 8 - Batch 80/88 - Loss: 2.2149\n",
            "Model 15 - Finished Epoch 8\n",
            "Model 15 - Starting Epoch 9\n",
            "Model 15 - Epoch 9 - Batch 10/88 - Loss: 2.1588\n",
            "Model 15 - Epoch 9 - Batch 20/88 - Loss: 2.1793\n",
            "Model 15 - Epoch 9 - Batch 30/88 - Loss: 2.2900\n",
            "Model 15 - Epoch 9 - Batch 40/88 - Loss: 2.2504\n",
            "Model 15 - Epoch 9 - Batch 50/88 - Loss: 2.2367\n",
            "Model 15 - Epoch 9 - Batch 60/88 - Loss: 2.1531\n",
            "Model 15 - Epoch 9 - Batch 70/88 - Loss: 2.0849\n",
            "Model 15 - Epoch 9 - Batch 80/88 - Loss: 2.3655\n",
            "Model 15 - Finished Epoch 9\n",
            "Model 15 - Starting Epoch 10\n",
            "Model 15 - Epoch 10 - Batch 10/88 - Loss: 2.1925\n",
            "Model 15 - Epoch 10 - Batch 20/88 - Loss: 2.1259\n",
            "Model 15 - Epoch 10 - Batch 30/88 - Loss: 2.1955\n",
            "Model 15 - Epoch 10 - Batch 40/88 - Loss: 2.1785\n",
            "Model 15 - Epoch 10 - Batch 50/88 - Loss: 2.2017\n",
            "Model 15 - Epoch 10 - Batch 60/88 - Loss: 2.0812\n",
            "Model 15 - Epoch 10 - Batch 70/88 - Loss: 2.1878\n",
            "Model 15 - Epoch 10 - Batch 80/88 - Loss: 2.3276\n",
            "Model 15 - Finished Epoch 10\n",
            "Model 15 done training for 10 epochs in generation 6\n",
            "  🎉 New best model with accuracy: 0.6520\n",
            "  ✅ Best Acc This Gen: 0.6520 | Worst: 0.5895\n",
            "\n",
            "🌱 Generation 7 | Population size: 9\n",
            "Model 1 - Starting Epoch 1\n",
            "Model 1 - Epoch 1 - Batch 10/88 - Loss: 1.6296\n",
            "Model 1 - Epoch 1 - Batch 20/88 - Loss: 1.6064\n",
            "Model 1 - Epoch 1 - Batch 30/88 - Loss: 1.7152\n",
            "Model 1 - Epoch 1 - Batch 40/88 - Loss: 1.6383\n",
            "Model 1 - Epoch 1 - Batch 50/88 - Loss: 1.7045\n",
            "Model 1 - Epoch 1 - Batch 60/88 - Loss: 1.6343\n",
            "Model 1 - Epoch 1 - Batch 70/88 - Loss: 1.5848\n",
            "Model 1 - Epoch 1 - Batch 80/88 - Loss: 1.7686\n",
            "Model 1 - Finished Epoch 1\n",
            "Model 1 - Starting Epoch 2\n",
            "Model 1 - Epoch 2 - Batch 10/88 - Loss: 1.6473\n",
            "Model 1 - Epoch 2 - Batch 20/88 - Loss: 1.7059\n",
            "Model 1 - Epoch 2 - Batch 30/88 - Loss: 1.5762\n",
            "Model 1 - Epoch 2 - Batch 40/88 - Loss: 1.6298\n",
            "Model 1 - Epoch 2 - Batch 50/88 - Loss: 1.6875\n",
            "Model 1 - Epoch 2 - Batch 60/88 - Loss: 1.6938\n",
            "Model 1 - Epoch 2 - Batch 70/88 - Loss: 1.6549\n",
            "Model 1 - Epoch 2 - Batch 80/88 - Loss: 1.6830\n",
            "Model 1 - Finished Epoch 2\n",
            "Model 1 - Starting Epoch 3\n",
            "Model 1 - Epoch 3 - Batch 10/88 - Loss: 1.6144\n",
            "Model 1 - Epoch 3 - Batch 20/88 - Loss: 1.6423\n",
            "Model 1 - Epoch 3 - Batch 30/88 - Loss: 1.5539\n",
            "Model 1 - Epoch 3 - Batch 40/88 - Loss: 1.5563\n",
            "Model 1 - Epoch 3 - Batch 50/88 - Loss: 1.6942\n",
            "Model 1 - Epoch 3 - Batch 60/88 - Loss: 1.5823\n",
            "Model 1 - Epoch 3 - Batch 70/88 - Loss: 1.5897\n",
            "Model 1 - Epoch 3 - Batch 80/88 - Loss: 1.6265\n",
            "Model 1 - Finished Epoch 3\n",
            "Model 1 - Starting Epoch 4\n",
            "Model 1 - Epoch 4 - Batch 10/88 - Loss: 1.6507\n",
            "Model 1 - Epoch 4 - Batch 20/88 - Loss: 1.5845\n",
            "Model 1 - Epoch 4 - Batch 30/88 - Loss: 1.5701\n",
            "Model 1 - Epoch 4 - Batch 40/88 - Loss: 1.5595\n",
            "Model 1 - Epoch 4 - Batch 50/88 - Loss: 1.5294\n",
            "Model 1 - Epoch 4 - Batch 60/88 - Loss: 1.6336\n",
            "Model 1 - Epoch 4 - Batch 70/88 - Loss: 1.7115\n",
            "Model 1 - Epoch 4 - Batch 80/88 - Loss: 1.6073\n",
            "Model 1 - Finished Epoch 4\n",
            "Model 1 - Starting Epoch 5\n",
            "Model 1 - Epoch 5 - Batch 10/88 - Loss: 1.6268\n",
            "Model 1 - Epoch 5 - Batch 20/88 - Loss: 1.5987\n",
            "Model 1 - Epoch 5 - Batch 30/88 - Loss: 1.5787\n",
            "Model 1 - Epoch 5 - Batch 40/88 - Loss: 1.5984\n",
            "Model 1 - Epoch 5 - Batch 50/88 - Loss: 1.5989\n",
            "Model 1 - Epoch 5 - Batch 60/88 - Loss: 1.5603\n",
            "Model 1 - Epoch 5 - Batch 70/88 - Loss: 1.5010\n",
            "Model 1 - Epoch 5 - Batch 80/88 - Loss: 1.5162\n",
            "Model 1 - Finished Epoch 5\n",
            "Model 1 - Starting Epoch 6\n",
            "Model 1 - Epoch 6 - Batch 10/88 - Loss: 1.5523\n",
            "Model 1 - Epoch 6 - Batch 20/88 - Loss: 1.6118\n",
            "Model 1 - Epoch 6 - Batch 30/88 - Loss: 1.6719\n",
            "Model 1 - Epoch 6 - Batch 40/88 - Loss: 1.5623\n",
            "Model 1 - Epoch 6 - Batch 50/88 - Loss: 1.6174\n",
            "Model 1 - Epoch 6 - Batch 60/88 - Loss: 1.6139\n",
            "Model 1 - Epoch 6 - Batch 70/88 - Loss: 1.6281\n",
            "Model 1 - Epoch 6 - Batch 80/88 - Loss: 1.6713\n",
            "Model 1 - Finished Epoch 6\n",
            "Model 1 - Starting Epoch 7\n",
            "Model 1 - Epoch 7 - Batch 10/88 - Loss: 1.5883\n",
            "Model 1 - Epoch 7 - Batch 20/88 - Loss: 1.6460\n",
            "Model 1 - Epoch 7 - Batch 30/88 - Loss: 1.4505\n",
            "Model 1 - Epoch 7 - Batch 40/88 - Loss: 1.5345\n",
            "Model 1 - Epoch 7 - Batch 50/88 - Loss: 1.4741\n",
            "Model 1 - Epoch 7 - Batch 60/88 - Loss: 1.6157\n",
            "Model 1 - Epoch 7 - Batch 70/88 - Loss: 1.6128\n",
            "Model 1 - Epoch 7 - Batch 80/88 - Loss: 1.6812\n",
            "Model 1 - Finished Epoch 7\n",
            "Model 1 - Starting Epoch 8\n",
            "Model 1 - Epoch 8 - Batch 10/88 - Loss: 1.5338\n",
            "Model 1 - Epoch 8 - Batch 20/88 - Loss: 1.5122\n",
            "Model 1 - Epoch 8 - Batch 30/88 - Loss: 1.5551\n",
            "Model 1 - Epoch 8 - Batch 40/88 - Loss: 1.4969\n",
            "Model 1 - Epoch 8 - Batch 50/88 - Loss: 1.6833\n",
            "Model 1 - Epoch 8 - Batch 60/88 - Loss: 1.5721\n",
            "Model 1 - Epoch 8 - Batch 70/88 - Loss: 1.6170\n",
            "Model 1 - Epoch 8 - Batch 80/88 - Loss: 1.6997\n",
            "Model 1 - Finished Epoch 8\n",
            "Model 1 - Starting Epoch 9\n",
            "Model 1 - Epoch 9 - Batch 10/88 - Loss: 1.5670\n",
            "Model 1 - Epoch 9 - Batch 20/88 - Loss: 1.3958\n",
            "Model 1 - Epoch 9 - Batch 30/88 - Loss: 1.6330\n",
            "Model 1 - Epoch 9 - Batch 40/88 - Loss: 1.4558\n",
            "Model 1 - Epoch 9 - Batch 50/88 - Loss: 1.5320\n",
            "Model 1 - Epoch 9 - Batch 60/88 - Loss: 1.5824\n",
            "Model 1 - Epoch 9 - Batch 70/88 - Loss: 1.6551\n",
            "Model 1 - Epoch 9 - Batch 80/88 - Loss: 1.5698\n",
            "Model 1 - Finished Epoch 9\n",
            "Model 1 - Starting Epoch 10\n",
            "Model 1 - Epoch 10 - Batch 10/88 - Loss: 1.5816\n",
            "Model 1 - Epoch 10 - Batch 20/88 - Loss: 1.5343\n",
            "Model 1 - Epoch 10 - Batch 30/88 - Loss: 1.4241\n",
            "Model 1 - Epoch 10 - Batch 40/88 - Loss: 1.5356\n",
            "Model 1 - Epoch 10 - Batch 50/88 - Loss: 1.5737\n",
            "Model 1 - Epoch 10 - Batch 60/88 - Loss: 1.5726\n",
            "Model 1 - Epoch 10 - Batch 70/88 - Loss: 1.5832\n",
            "Model 1 - Epoch 10 - Batch 80/88 - Loss: 1.5739\n",
            "Model 1 - Finished Epoch 10\n",
            "Model 1 done training for 10 epochs in generation 7\n",
            "Model 2 - Starting Epoch 1\n",
            "Model 2 - Epoch 1 - Batch 10/88 - Loss: 1.8096\n",
            "Model 2 - Epoch 1 - Batch 20/88 - Loss: 1.8719\n",
            "Model 2 - Epoch 1 - Batch 30/88 - Loss: 1.8689\n",
            "Model 2 - Epoch 1 - Batch 40/88 - Loss: 1.8820\n",
            "Model 2 - Epoch 1 - Batch 50/88 - Loss: 2.0135\n",
            "Model 2 - Epoch 1 - Batch 60/88 - Loss: 1.9221\n",
            "Model 2 - Epoch 1 - Batch 70/88 - Loss: 1.8899\n",
            "Model 2 - Epoch 1 - Batch 80/88 - Loss: 1.8708\n",
            "Model 2 - Finished Epoch 1\n",
            "Model 2 - Starting Epoch 2\n",
            "Model 2 - Epoch 2 - Batch 10/88 - Loss: 1.8656\n",
            "Model 2 - Epoch 2 - Batch 20/88 - Loss: 1.8950\n",
            "Model 2 - Epoch 2 - Batch 30/88 - Loss: 1.8087\n",
            "Model 2 - Epoch 2 - Batch 40/88 - Loss: 1.9226\n",
            "Model 2 - Epoch 2 - Batch 50/88 - Loss: 1.9274\n",
            "Model 2 - Epoch 2 - Batch 60/88 - Loss: 1.9547\n",
            "Model 2 - Epoch 2 - Batch 70/88 - Loss: 2.0274\n",
            "Model 2 - Epoch 2 - Batch 80/88 - Loss: 1.9548\n",
            "Model 2 - Finished Epoch 2\n",
            "Model 2 - Starting Epoch 3\n",
            "Model 2 - Epoch 3 - Batch 10/88 - Loss: 1.8788\n",
            "Model 2 - Epoch 3 - Batch 20/88 - Loss: 1.8610\n",
            "Model 2 - Epoch 3 - Batch 30/88 - Loss: 1.8949\n",
            "Model 2 - Epoch 3 - Batch 40/88 - Loss: 1.9085\n",
            "Model 2 - Epoch 3 - Batch 50/88 - Loss: 1.8903\n",
            "Model 2 - Epoch 3 - Batch 60/88 - Loss: 1.8984\n",
            "Model 2 - Epoch 3 - Batch 70/88 - Loss: 1.9409\n",
            "Model 2 - Epoch 3 - Batch 80/88 - Loss: 1.9225\n",
            "Model 2 - Finished Epoch 3\n",
            "Model 2 - Starting Epoch 4\n",
            "Model 2 - Epoch 4 - Batch 10/88 - Loss: 1.8966\n",
            "Model 2 - Epoch 4 - Batch 20/88 - Loss: 1.8348\n",
            "Model 2 - Epoch 4 - Batch 30/88 - Loss: 1.8504\n",
            "Model 2 - Epoch 4 - Batch 40/88 - Loss: 1.8991\n",
            "Model 2 - Epoch 4 - Batch 50/88 - Loss: 1.7910\n",
            "Model 2 - Epoch 4 - Batch 60/88 - Loss: 1.9129\n",
            "Model 2 - Epoch 4 - Batch 70/88 - Loss: 1.8422\n",
            "Model 2 - Epoch 4 - Batch 80/88 - Loss: 1.8550\n",
            "Model 2 - Finished Epoch 4\n",
            "Model 2 - Starting Epoch 5\n",
            "Model 2 - Epoch 5 - Batch 10/88 - Loss: 1.9023\n",
            "Model 2 - Epoch 5 - Batch 20/88 - Loss: 1.8461\n",
            "Model 2 - Epoch 5 - Batch 30/88 - Loss: 1.8895\n",
            "Model 2 - Epoch 5 - Batch 40/88 - Loss: 1.8093\n",
            "Model 2 - Epoch 5 - Batch 50/88 - Loss: 1.8877\n",
            "Model 2 - Epoch 5 - Batch 60/88 - Loss: 1.8717\n",
            "Model 2 - Epoch 5 - Batch 70/88 - Loss: 1.8546\n",
            "Model 2 - Epoch 5 - Batch 80/88 - Loss: 1.8182\n",
            "Model 2 - Finished Epoch 5\n",
            "Model 2 - Starting Epoch 6\n",
            "Model 2 - Epoch 6 - Batch 10/88 - Loss: 1.8331\n",
            "Model 2 - Epoch 6 - Batch 20/88 - Loss: 1.7296\n",
            "Model 2 - Epoch 6 - Batch 30/88 - Loss: 1.8288\n",
            "Model 2 - Epoch 6 - Batch 40/88 - Loss: 1.8119\n",
            "Model 2 - Epoch 6 - Batch 50/88 - Loss: 1.8165\n",
            "Model 2 - Epoch 6 - Batch 60/88 - Loss: 1.9432\n",
            "Model 2 - Epoch 6 - Batch 70/88 - Loss: 1.8448\n",
            "Model 2 - Epoch 6 - Batch 80/88 - Loss: 1.8457\n",
            "Model 2 - Finished Epoch 6\n",
            "Model 2 - Starting Epoch 7\n",
            "Model 2 - Epoch 7 - Batch 10/88 - Loss: 1.9073\n",
            "Model 2 - Epoch 7 - Batch 20/88 - Loss: 1.8722\n",
            "Model 2 - Epoch 7 - Batch 30/88 - Loss: 1.6924\n",
            "Model 2 - Epoch 7 - Batch 40/88 - Loss: 1.9009\n",
            "Model 2 - Epoch 7 - Batch 50/88 - Loss: 1.7640\n",
            "Model 2 - Epoch 7 - Batch 60/88 - Loss: 1.8187\n",
            "Model 2 - Epoch 7 - Batch 70/88 - Loss: 1.8141\n",
            "Model 2 - Epoch 7 - Batch 80/88 - Loss: 1.8712\n",
            "Model 2 - Finished Epoch 7\n",
            "Model 2 - Starting Epoch 8\n",
            "Model 2 - Epoch 8 - Batch 10/88 - Loss: 1.6786\n",
            "Model 2 - Epoch 8 - Batch 20/88 - Loss: 1.8785\n",
            "Model 2 - Epoch 8 - Batch 30/88 - Loss: 1.7694\n",
            "Model 2 - Epoch 8 - Batch 40/88 - Loss: 1.8823\n",
            "Model 2 - Epoch 8 - Batch 50/88 - Loss: 1.7974\n",
            "Model 2 - Epoch 8 - Batch 60/88 - Loss: 1.9084\n",
            "Model 2 - Epoch 8 - Batch 70/88 - Loss: 1.8448\n",
            "Model 2 - Epoch 8 - Batch 80/88 - Loss: 1.8528\n",
            "Model 2 - Finished Epoch 8\n",
            "Model 2 - Starting Epoch 9\n",
            "Model 2 - Epoch 9 - Batch 10/88 - Loss: 1.8478\n",
            "Model 2 - Epoch 9 - Batch 20/88 - Loss: 1.7258\n",
            "Model 2 - Epoch 9 - Batch 30/88 - Loss: 1.7437\n",
            "Model 2 - Epoch 9 - Batch 40/88 - Loss: 1.8751\n",
            "Model 2 - Epoch 9 - Batch 50/88 - Loss: 1.9514\n",
            "Model 2 - Epoch 9 - Batch 60/88 - Loss: 1.9129\n",
            "Model 2 - Epoch 9 - Batch 70/88 - Loss: 1.8444\n",
            "Model 2 - Epoch 9 - Batch 80/88 - Loss: 1.8020\n",
            "Model 2 - Finished Epoch 9\n",
            "Model 2 - Starting Epoch 10\n",
            "Model 2 - Epoch 10 - Batch 10/88 - Loss: 1.7922\n",
            "Model 2 - Epoch 10 - Batch 20/88 - Loss: 1.7833\n",
            "Model 2 - Epoch 10 - Batch 30/88 - Loss: 1.7393\n",
            "Model 2 - Epoch 10 - Batch 40/88 - Loss: 1.7726\n",
            "Model 2 - Epoch 10 - Batch 50/88 - Loss: 1.9103\n",
            "Model 2 - Epoch 10 - Batch 60/88 - Loss: 1.8505\n",
            "Model 2 - Epoch 10 - Batch 70/88 - Loss: 1.9613\n",
            "Model 2 - Epoch 10 - Batch 80/88 - Loss: 1.8462\n",
            "Model 2 - Finished Epoch 10\n",
            "Model 2 done training for 10 epochs in generation 7\n",
            "Model 3 - Starting Epoch 1\n",
            "Model 3 - Epoch 1 - Batch 10/88 - Loss: 1.8835\n",
            "Model 3 - Epoch 1 - Batch 20/88 - Loss: 1.8888\n",
            "Model 3 - Epoch 1 - Batch 30/88 - Loss: 1.9542\n",
            "Model 3 - Epoch 1 - Batch 40/88 - Loss: 2.0091\n",
            "Model 3 - Epoch 1 - Batch 50/88 - Loss: 2.0563\n",
            "Model 3 - Epoch 1 - Batch 60/88 - Loss: 1.9177\n",
            "Model 3 - Epoch 1 - Batch 70/88 - Loss: 2.0661\n",
            "Model 3 - Epoch 1 - Batch 80/88 - Loss: 2.0378\n",
            "Model 3 - Finished Epoch 1\n",
            "Model 3 - Starting Epoch 2\n",
            "Model 3 - Epoch 2 - Batch 10/88 - Loss: 1.9256\n",
            "Model 3 - Epoch 2 - Batch 20/88 - Loss: 1.9837\n",
            "Model 3 - Epoch 2 - Batch 30/88 - Loss: 1.9626\n",
            "Model 3 - Epoch 2 - Batch 40/88 - Loss: 1.9420\n",
            "Model 3 - Epoch 2 - Batch 50/88 - Loss: 1.9210\n",
            "Model 3 - Epoch 2 - Batch 60/88 - Loss: 2.0204\n",
            "Model 3 - Epoch 2 - Batch 70/88 - Loss: 1.9957\n",
            "Model 3 - Epoch 2 - Batch 80/88 - Loss: 2.0719\n",
            "Model 3 - Finished Epoch 2\n",
            "Model 3 - Starting Epoch 3\n",
            "Model 3 - Epoch 3 - Batch 10/88 - Loss: 1.9320\n",
            "Model 3 - Epoch 3 - Batch 20/88 - Loss: 1.8173\n",
            "Model 3 - Epoch 3 - Batch 30/88 - Loss: 1.9676\n",
            "Model 3 - Epoch 3 - Batch 40/88 - Loss: 1.9118\n",
            "Model 3 - Epoch 3 - Batch 50/88 - Loss: 1.9761\n",
            "Model 3 - Epoch 3 - Batch 60/88 - Loss: 1.9484\n",
            "Model 3 - Epoch 3 - Batch 70/88 - Loss: 1.9540\n",
            "Model 3 - Epoch 3 - Batch 80/88 - Loss: 1.8972\n",
            "Model 3 - Finished Epoch 3\n",
            "Model 3 - Starting Epoch 4\n",
            "Model 3 - Epoch 4 - Batch 10/88 - Loss: 1.8526\n",
            "Model 3 - Epoch 4 - Batch 20/88 - Loss: 1.9148\n",
            "Model 3 - Epoch 4 - Batch 30/88 - Loss: 1.9037\n",
            "Model 3 - Epoch 4 - Batch 40/88 - Loss: 1.9413\n",
            "Model 3 - Epoch 4 - Batch 50/88 - Loss: 1.8909\n",
            "Model 3 - Epoch 4 - Batch 60/88 - Loss: 2.0209\n",
            "Model 3 - Epoch 4 - Batch 70/88 - Loss: 1.9371\n",
            "Model 3 - Epoch 4 - Batch 80/88 - Loss: 1.9324\n",
            "Model 3 - Finished Epoch 4\n",
            "Model 3 - Starting Epoch 5\n",
            "Model 3 - Epoch 5 - Batch 10/88 - Loss: 1.7850\n",
            "Model 3 - Epoch 5 - Batch 20/88 - Loss: 1.8560\n",
            "Model 3 - Epoch 5 - Batch 30/88 - Loss: 2.0282\n",
            "Model 3 - Epoch 5 - Batch 40/88 - Loss: 1.9145\n",
            "Model 3 - Epoch 5 - Batch 50/88 - Loss: 1.8667\n",
            "Model 3 - Epoch 5 - Batch 60/88 - Loss: 1.9868\n",
            "Model 3 - Epoch 5 - Batch 70/88 - Loss: 1.9647\n",
            "Model 3 - Epoch 5 - Batch 80/88 - Loss: 1.9202\n",
            "Model 3 - Finished Epoch 5\n",
            "Model 3 - Starting Epoch 6\n",
            "Model 3 - Epoch 6 - Batch 10/88 - Loss: 1.8257\n",
            "Model 3 - Epoch 6 - Batch 20/88 - Loss: 1.8792\n",
            "Model 3 - Epoch 6 - Batch 30/88 - Loss: 1.8779\n",
            "Model 3 - Epoch 6 - Batch 40/88 - Loss: 1.9436\n",
            "Model 3 - Epoch 6 - Batch 50/88 - Loss: 1.9969\n",
            "Model 3 - Epoch 6 - Batch 60/88 - Loss: 1.8753\n",
            "Model 3 - Epoch 6 - Batch 70/88 - Loss: 1.9930\n",
            "Model 3 - Epoch 6 - Batch 80/88 - Loss: 1.8235\n",
            "Model 3 - Finished Epoch 6\n",
            "Model 3 - Starting Epoch 7\n",
            "Model 3 - Epoch 7 - Batch 10/88 - Loss: 1.9470\n",
            "Model 3 - Epoch 7 - Batch 20/88 - Loss: 1.9240\n",
            "Model 3 - Epoch 7 - Batch 30/88 - Loss: 1.8823\n",
            "Model 3 - Epoch 7 - Batch 40/88 - Loss: 1.9807\n",
            "Model 3 - Epoch 7 - Batch 50/88 - Loss: 1.8999\n",
            "Model 3 - Epoch 7 - Batch 60/88 - Loss: 1.8867\n",
            "Model 3 - Epoch 7 - Batch 70/88 - Loss: 1.9109\n",
            "Model 3 - Epoch 7 - Batch 80/88 - Loss: 1.8878\n",
            "Model 3 - Finished Epoch 7\n",
            "Model 3 - Starting Epoch 8\n",
            "Model 3 - Epoch 8 - Batch 10/88 - Loss: 1.9192\n",
            "Model 3 - Epoch 8 - Batch 20/88 - Loss: 1.9301\n",
            "Model 3 - Epoch 8 - Batch 30/88 - Loss: 1.8701\n",
            "Model 3 - Epoch 8 - Batch 40/88 - Loss: 1.9581\n",
            "Model 3 - Epoch 8 - Batch 50/88 - Loss: 1.8906\n",
            "Model 3 - Epoch 8 - Batch 60/88 - Loss: 1.9128\n",
            "Model 3 - Epoch 8 - Batch 70/88 - Loss: 1.9553\n",
            "Model 3 - Epoch 8 - Batch 80/88 - Loss: 1.9349\n",
            "Model 3 - Finished Epoch 8\n",
            "Model 3 - Starting Epoch 9\n",
            "Model 3 - Epoch 9 - Batch 10/88 - Loss: 1.8987\n",
            "Model 3 - Epoch 9 - Batch 20/88 - Loss: 1.8750\n",
            "Model 3 - Epoch 9 - Batch 30/88 - Loss: 1.8175\n",
            "Model 3 - Epoch 9 - Batch 40/88 - Loss: 1.9551\n",
            "Model 3 - Epoch 9 - Batch 50/88 - Loss: 1.8820\n",
            "Model 3 - Epoch 9 - Batch 60/88 - Loss: 2.0126\n",
            "Model 3 - Epoch 9 - Batch 70/88 - Loss: 1.8487\n",
            "Model 3 - Epoch 9 - Batch 80/88 - Loss: 2.1534\n",
            "Model 3 - Finished Epoch 9\n",
            "Model 3 - Starting Epoch 10\n",
            "Model 3 - Epoch 10 - Batch 10/88 - Loss: 1.9008\n",
            "Model 3 - Epoch 10 - Batch 20/88 - Loss: 1.8877\n",
            "Model 3 - Epoch 10 - Batch 30/88 - Loss: 1.7872\n",
            "Model 3 - Epoch 10 - Batch 40/88 - Loss: 1.9103\n",
            "Model 3 - Epoch 10 - Batch 50/88 - Loss: 1.8062\n",
            "Model 3 - Epoch 10 - Batch 60/88 - Loss: 1.9357\n",
            "Model 3 - Epoch 10 - Batch 70/88 - Loss: 1.8558\n",
            "Model 3 - Epoch 10 - Batch 80/88 - Loss: 1.9535\n",
            "Model 3 - Finished Epoch 10\n",
            "Model 3 done training for 10 epochs in generation 7\n",
            "Model 4 - Starting Epoch 1\n",
            "Model 4 - Epoch 1 - Batch 10/88 - Loss: 2.0821\n",
            "Model 4 - Epoch 1 - Batch 20/88 - Loss: 1.8307\n",
            "Model 4 - Epoch 1 - Batch 30/88 - Loss: 1.9228\n",
            "Model 4 - Epoch 1 - Batch 40/88 - Loss: 1.9377\n",
            "Model 4 - Epoch 1 - Batch 50/88 - Loss: 2.0027\n",
            "Model 4 - Epoch 1 - Batch 60/88 - Loss: 2.0192\n",
            "Model 4 - Epoch 1 - Batch 70/88 - Loss: 2.1070\n",
            "Model 4 - Epoch 1 - Batch 80/88 - Loss: 1.9529\n",
            "Model 4 - Finished Epoch 1\n",
            "Model 4 - Starting Epoch 2\n",
            "Model 4 - Epoch 2 - Batch 10/88 - Loss: 1.9597\n",
            "Model 4 - Epoch 2 - Batch 20/88 - Loss: 1.9442\n",
            "Model 4 - Epoch 2 - Batch 30/88 - Loss: 1.9664\n",
            "Model 4 - Epoch 2 - Batch 40/88 - Loss: 1.9704\n",
            "Model 4 - Epoch 2 - Batch 50/88 - Loss: 2.0316\n",
            "Model 4 - Epoch 2 - Batch 60/88 - Loss: 2.1095\n",
            "Model 4 - Epoch 2 - Batch 70/88 - Loss: 1.9197\n",
            "Model 4 - Epoch 2 - Batch 80/88 - Loss: 2.0372\n",
            "Model 4 - Finished Epoch 2\n",
            "Model 4 - Starting Epoch 3\n",
            "Model 4 - Epoch 3 - Batch 10/88 - Loss: 1.9228\n",
            "Model 4 - Epoch 3 - Batch 20/88 - Loss: 1.9475\n",
            "Model 4 - Epoch 3 - Batch 30/88 - Loss: 1.9214\n",
            "Model 4 - Epoch 3 - Batch 40/88 - Loss: 1.9048\n",
            "Model 4 - Epoch 3 - Batch 50/88 - Loss: 1.9389\n",
            "Model 4 - Epoch 3 - Batch 60/88 - Loss: 1.9976\n",
            "Model 4 - Epoch 3 - Batch 70/88 - Loss: 1.8974\n",
            "Model 4 - Epoch 3 - Batch 80/88 - Loss: 2.0537\n",
            "Model 4 - Finished Epoch 3\n",
            "Model 4 - Starting Epoch 4\n",
            "Model 4 - Epoch 4 - Batch 10/88 - Loss: 1.8958\n",
            "Model 4 - Epoch 4 - Batch 20/88 - Loss: 1.8802\n",
            "Model 4 - Epoch 4 - Batch 30/88 - Loss: 1.9364\n",
            "Model 4 - Epoch 4 - Batch 40/88 - Loss: 1.9966\n",
            "Model 4 - Epoch 4 - Batch 50/88 - Loss: 1.9399\n",
            "Model 4 - Epoch 4 - Batch 60/88 - Loss: 1.9620\n",
            "Model 4 - Epoch 4 - Batch 70/88 - Loss: 2.0241\n",
            "Model 4 - Epoch 4 - Batch 80/88 - Loss: 2.0191\n",
            "Model 4 - Finished Epoch 4\n",
            "Model 4 - Starting Epoch 5\n",
            "Model 4 - Epoch 5 - Batch 10/88 - Loss: 1.9649\n",
            "Model 4 - Epoch 5 - Batch 20/88 - Loss: 1.8160\n",
            "Model 4 - Epoch 5 - Batch 30/88 - Loss: 1.9148\n",
            "Model 4 - Epoch 5 - Batch 40/88 - Loss: 1.8210\n",
            "Model 4 - Epoch 5 - Batch 50/88 - Loss: 1.8719\n",
            "Model 4 - Epoch 5 - Batch 60/88 - Loss: 1.9024\n",
            "Model 4 - Epoch 5 - Batch 70/88 - Loss: 1.9888\n",
            "Model 4 - Epoch 5 - Batch 80/88 - Loss: 1.9775\n",
            "Model 4 - Finished Epoch 5\n",
            "Model 4 - Starting Epoch 6\n",
            "Model 4 - Epoch 6 - Batch 10/88 - Loss: 1.8872\n",
            "Model 4 - Epoch 6 - Batch 20/88 - Loss: 1.8446\n",
            "Model 4 - Epoch 6 - Batch 30/88 - Loss: 1.8957\n",
            "Model 4 - Epoch 6 - Batch 40/88 - Loss: 1.8808\n",
            "Model 4 - Epoch 6 - Batch 50/88 - Loss: 1.9127\n",
            "Model 4 - Epoch 6 - Batch 60/88 - Loss: 1.8459\n",
            "Model 4 - Epoch 6 - Batch 70/88 - Loss: 1.8908\n",
            "Model 4 - Epoch 6 - Batch 80/88 - Loss: 1.9260\n",
            "Model 4 - Finished Epoch 6\n",
            "Model 4 - Starting Epoch 7\n",
            "Model 4 - Epoch 7 - Batch 10/88 - Loss: 1.8009\n",
            "Model 4 - Epoch 7 - Batch 20/88 - Loss: 1.8822\n",
            "Model 4 - Epoch 7 - Batch 30/88 - Loss: 1.9080\n",
            "Model 4 - Epoch 7 - Batch 40/88 - Loss: 1.7835\n",
            "Model 4 - Epoch 7 - Batch 50/88 - Loss: 1.8725\n",
            "Model 4 - Epoch 7 - Batch 60/88 - Loss: 1.9196\n",
            "Model 4 - Epoch 7 - Batch 70/88 - Loss: 1.9375\n",
            "Model 4 - Epoch 7 - Batch 80/88 - Loss: 1.9000\n",
            "Model 4 - Finished Epoch 7\n",
            "Model 4 - Starting Epoch 8\n",
            "Model 4 - Epoch 8 - Batch 10/88 - Loss: 1.9251\n",
            "Model 4 - Epoch 8 - Batch 20/88 - Loss: 1.8495\n",
            "Model 4 - Epoch 8 - Batch 30/88 - Loss: 1.8810\n",
            "Model 4 - Epoch 8 - Batch 40/88 - Loss: 1.8717\n",
            "Model 4 - Epoch 8 - Batch 50/88 - Loss: 1.8433\n",
            "Model 4 - Epoch 8 - Batch 60/88 - Loss: 1.9418\n",
            "Model 4 - Epoch 8 - Batch 70/88 - Loss: 2.0318\n",
            "Model 4 - Epoch 8 - Batch 80/88 - Loss: 1.9021\n",
            "Model 4 - Finished Epoch 8\n",
            "Model 4 - Starting Epoch 9\n",
            "Model 4 - Epoch 9 - Batch 10/88 - Loss: 1.8970\n",
            "Model 4 - Epoch 9 - Batch 20/88 - Loss: 1.8937\n",
            "Model 4 - Epoch 9 - Batch 30/88 - Loss: 1.9893\n",
            "Model 4 - Epoch 9 - Batch 40/88 - Loss: 1.8400\n",
            "Model 4 - Epoch 9 - Batch 50/88 - Loss: 1.8298\n",
            "Model 4 - Epoch 9 - Batch 60/88 - Loss: 1.8581\n",
            "Model 4 - Epoch 9 - Batch 70/88 - Loss: 1.8697\n",
            "Model 4 - Epoch 9 - Batch 80/88 - Loss: 1.9792\n",
            "Model 4 - Finished Epoch 9\n",
            "Model 4 - Starting Epoch 10\n",
            "Model 4 - Epoch 10 - Batch 10/88 - Loss: 1.7971\n",
            "Model 4 - Epoch 10 - Batch 20/88 - Loss: 1.8104\n",
            "Model 4 - Epoch 10 - Batch 30/88 - Loss: 1.7201\n",
            "Model 4 - Epoch 10 - Batch 40/88 - Loss: 1.9782\n",
            "Model 4 - Epoch 10 - Batch 50/88 - Loss: 1.8419\n",
            "Model 4 - Epoch 10 - Batch 60/88 - Loss: 1.9989\n",
            "Model 4 - Epoch 10 - Batch 70/88 - Loss: 1.8602\n",
            "Model 4 - Epoch 10 - Batch 80/88 - Loss: 2.0153\n",
            "Model 4 - Finished Epoch 10\n",
            "Model 4 done training for 10 epochs in generation 7\n",
            "Model 5 - Starting Epoch 1\n",
            "Model 5 - Epoch 1 - Batch 10/88 - Loss: 2.0309\n",
            "Model 5 - Epoch 1 - Batch 20/88 - Loss: 2.0875\n",
            "Model 5 - Epoch 1 - Batch 30/88 - Loss: 2.0204\n",
            "Model 5 - Epoch 1 - Batch 40/88 - Loss: 1.9506\n",
            "Model 5 - Epoch 1 - Batch 50/88 - Loss: 2.0720\n",
            "Model 5 - Epoch 1 - Batch 60/88 - Loss: 2.0247\n",
            "Model 5 - Epoch 1 - Batch 70/88 - Loss: 1.9945\n",
            "Model 5 - Epoch 1 - Batch 80/88 - Loss: 2.0326\n",
            "Model 5 - Finished Epoch 1\n",
            "Model 5 - Starting Epoch 2\n",
            "Model 5 - Epoch 2 - Batch 10/88 - Loss: 1.9094\n",
            "Model 5 - Epoch 2 - Batch 20/88 - Loss: 1.9360\n",
            "Model 5 - Epoch 2 - Batch 30/88 - Loss: 2.0393\n",
            "Model 5 - Epoch 2 - Batch 40/88 - Loss: 1.9851\n",
            "Model 5 - Epoch 2 - Batch 50/88 - Loss: 2.1317\n",
            "Model 5 - Epoch 2 - Batch 60/88 - Loss: 2.0779\n",
            "Model 5 - Epoch 2 - Batch 70/88 - Loss: 1.9550\n",
            "Model 5 - Epoch 2 - Batch 80/88 - Loss: 2.1119\n",
            "Model 5 - Finished Epoch 2\n",
            "Model 5 - Starting Epoch 3\n",
            "Model 5 - Epoch 3 - Batch 10/88 - Loss: 1.8633\n",
            "Model 5 - Epoch 3 - Batch 20/88 - Loss: 1.9991\n",
            "Model 5 - Epoch 3 - Batch 30/88 - Loss: 1.9964\n",
            "Model 5 - Epoch 3 - Batch 40/88 - Loss: 1.8784\n",
            "Model 5 - Epoch 3 - Batch 50/88 - Loss: 1.9296\n",
            "Model 5 - Epoch 3 - Batch 60/88 - Loss: 2.0527\n",
            "Model 5 - Epoch 3 - Batch 70/88 - Loss: 2.0862\n",
            "Model 5 - Epoch 3 - Batch 80/88 - Loss: 1.9862\n",
            "Model 5 - Finished Epoch 3\n",
            "Model 5 - Starting Epoch 4\n",
            "Model 5 - Epoch 4 - Batch 10/88 - Loss: 1.8435\n",
            "Model 5 - Epoch 4 - Batch 20/88 - Loss: 1.9639\n",
            "Model 5 - Epoch 4 - Batch 30/88 - Loss: 1.8406\n",
            "Model 5 - Epoch 4 - Batch 40/88 - Loss: 1.9485\n",
            "Model 5 - Epoch 4 - Batch 50/88 - Loss: 1.9597\n",
            "Model 5 - Epoch 4 - Batch 60/88 - Loss: 1.8961\n",
            "Model 5 - Epoch 4 - Batch 70/88 - Loss: 1.9424\n",
            "Model 5 - Epoch 4 - Batch 80/88 - Loss: 1.9679\n",
            "Model 5 - Finished Epoch 4\n",
            "Model 5 - Starting Epoch 5\n",
            "Model 5 - Epoch 5 - Batch 10/88 - Loss: 1.9349\n",
            "Model 5 - Epoch 5 - Batch 20/88 - Loss: 1.9408\n",
            "Model 5 - Epoch 5 - Batch 30/88 - Loss: 1.9744\n",
            "Model 5 - Epoch 5 - Batch 40/88 - Loss: 2.0169\n",
            "Model 5 - Epoch 5 - Batch 50/88 - Loss: 1.9254\n",
            "Model 5 - Epoch 5 - Batch 60/88 - Loss: 2.1209\n",
            "Model 5 - Epoch 5 - Batch 70/88 - Loss: 2.0567\n",
            "Model 5 - Epoch 5 - Batch 80/88 - Loss: 1.9555\n",
            "Model 5 - Finished Epoch 5\n",
            "Model 5 - Starting Epoch 6\n",
            "Model 5 - Epoch 6 - Batch 10/88 - Loss: 2.0099\n",
            "Model 5 - Epoch 6 - Batch 20/88 - Loss: 1.9040\n",
            "Model 5 - Epoch 6 - Batch 30/88 - Loss: 2.0026\n",
            "Model 5 - Epoch 6 - Batch 40/88 - Loss: 1.9031\n",
            "Model 5 - Epoch 6 - Batch 50/88 - Loss: 1.9800\n",
            "Model 5 - Epoch 6 - Batch 60/88 - Loss: 1.9505\n",
            "Model 5 - Epoch 6 - Batch 70/88 - Loss: 2.0859\n",
            "Model 5 - Epoch 6 - Batch 80/88 - Loss: 1.9587\n",
            "Model 5 - Finished Epoch 6\n",
            "Model 5 - Starting Epoch 7\n",
            "Model 5 - Epoch 7 - Batch 10/88 - Loss: 1.8291\n",
            "Model 5 - Epoch 7 - Batch 20/88 - Loss: 1.8958\n",
            "Model 5 - Epoch 7 - Batch 30/88 - Loss: 1.8663\n",
            "Model 5 - Epoch 7 - Batch 40/88 - Loss: 1.9372\n",
            "Model 5 - Epoch 7 - Batch 50/88 - Loss: 1.8835\n",
            "Model 5 - Epoch 7 - Batch 60/88 - Loss: 2.0034\n",
            "Model 5 - Epoch 7 - Batch 70/88 - Loss: 2.0041\n",
            "Model 5 - Epoch 7 - Batch 80/88 - Loss: 1.9781\n",
            "Model 5 - Finished Epoch 7\n",
            "Model 5 - Starting Epoch 8\n",
            "Model 5 - Epoch 8 - Batch 10/88 - Loss: 1.8758\n",
            "Model 5 - Epoch 8 - Batch 20/88 - Loss: 2.0037\n",
            "Model 5 - Epoch 8 - Batch 30/88 - Loss: 1.8631\n",
            "Model 5 - Epoch 8 - Batch 40/88 - Loss: 1.9908\n",
            "Model 5 - Epoch 8 - Batch 50/88 - Loss: 1.9433\n",
            "Model 5 - Epoch 8 - Batch 60/88 - Loss: 2.0397\n",
            "Model 5 - Epoch 8 - Batch 70/88 - Loss: 2.0112\n",
            "Model 5 - Epoch 8 - Batch 80/88 - Loss: 1.9736\n",
            "Model 5 - Finished Epoch 8\n",
            "Model 5 - Starting Epoch 9\n",
            "Model 5 - Epoch 9 - Batch 10/88 - Loss: 1.8120\n",
            "Model 5 - Epoch 9 - Batch 20/88 - Loss: 1.8890\n",
            "Model 5 - Epoch 9 - Batch 30/88 - Loss: 1.8988\n",
            "Model 5 - Epoch 9 - Batch 40/88 - Loss: 2.0633\n",
            "Model 5 - Epoch 9 - Batch 50/88 - Loss: 1.7931\n",
            "Model 5 - Epoch 9 - Batch 60/88 - Loss: 2.0900\n",
            "Model 5 - Epoch 9 - Batch 70/88 - Loss: 1.9896\n",
            "Model 5 - Epoch 9 - Batch 80/88 - Loss: 2.0146\n",
            "Model 5 - Finished Epoch 9\n",
            "Model 5 - Starting Epoch 10\n",
            "Model 5 - Epoch 10 - Batch 10/88 - Loss: 1.8962\n",
            "Model 5 - Epoch 10 - Batch 20/88 - Loss: 1.9340\n",
            "Model 5 - Epoch 10 - Batch 30/88 - Loss: 1.9206\n",
            "Model 5 - Epoch 10 - Batch 40/88 - Loss: 2.0803\n",
            "Model 5 - Epoch 10 - Batch 50/88 - Loss: 1.9086\n",
            "Model 5 - Epoch 10 - Batch 60/88 - Loss: 1.9050\n",
            "Model 5 - Epoch 10 - Batch 70/88 - Loss: 1.9251\n",
            "Model 5 - Epoch 10 - Batch 80/88 - Loss: 1.9825\n",
            "Model 5 - Finished Epoch 10\n",
            "Model 5 done training for 10 epochs in generation 7\n",
            "Model 6 - Starting Epoch 1\n",
            "Model 6 - Epoch 1 - Batch 10/88 - Loss: 2.2178\n",
            "Model 6 - Epoch 1 - Batch 20/88 - Loss: 2.1594\n",
            "Model 6 - Epoch 1 - Batch 30/88 - Loss: 2.2932\n",
            "Model 6 - Epoch 1 - Batch 40/88 - Loss: 2.1462\n",
            "Model 6 - Epoch 1 - Batch 50/88 - Loss: 2.1471\n",
            "Model 6 - Epoch 1 - Batch 60/88 - Loss: 2.1996\n",
            "Model 6 - Epoch 1 - Batch 70/88 - Loss: 2.1652\n",
            "Model 6 - Epoch 1 - Batch 80/88 - Loss: 2.2650\n",
            "Model 6 - Finished Epoch 1\n",
            "Model 6 - Starting Epoch 2\n",
            "Model 6 - Epoch 2 - Batch 10/88 - Loss: 2.1625\n",
            "Model 6 - Epoch 2 - Batch 20/88 - Loss: 2.2320\n",
            "Model 6 - Epoch 2 - Batch 30/88 - Loss: 2.0918\n",
            "Model 6 - Epoch 2 - Batch 40/88 - Loss: 2.0434\n",
            "Model 6 - Epoch 2 - Batch 50/88 - Loss: 2.1766\n",
            "Model 6 - Epoch 2 - Batch 60/88 - Loss: 2.1868\n",
            "Model 6 - Epoch 2 - Batch 70/88 - Loss: 2.1317\n",
            "Model 6 - Epoch 2 - Batch 80/88 - Loss: 2.1042\n",
            "Model 6 - Finished Epoch 2\n",
            "Model 6 - Starting Epoch 3\n",
            "Model 6 - Epoch 3 - Batch 10/88 - Loss: 2.1820\n",
            "Model 6 - Epoch 3 - Batch 20/88 - Loss: 2.1803\n",
            "Model 6 - Epoch 3 - Batch 30/88 - Loss: 2.0989\n",
            "Model 6 - Epoch 3 - Batch 40/88 - Loss: 2.0518\n",
            "Model 6 - Epoch 3 - Batch 50/88 - Loss: 2.1382\n",
            "Model 6 - Epoch 3 - Batch 60/88 - Loss: 2.2091\n",
            "Model 6 - Epoch 3 - Batch 70/88 - Loss: 2.2775\n",
            "Model 6 - Epoch 3 - Batch 80/88 - Loss: 2.2274\n",
            "Model 6 - Finished Epoch 3\n",
            "Model 6 - Starting Epoch 4\n",
            "Model 6 - Epoch 4 - Batch 10/88 - Loss: 2.0395\n",
            "Model 6 - Epoch 4 - Batch 20/88 - Loss: 2.1352\n",
            "Model 6 - Epoch 4 - Batch 30/88 - Loss: 2.1973\n",
            "Model 6 - Epoch 4 - Batch 40/88 - Loss: 2.2814\n",
            "Model 6 - Epoch 4 - Batch 50/88 - Loss: 2.1982\n",
            "Model 6 - Epoch 4 - Batch 60/88 - Loss: 2.1977\n",
            "Model 6 - Epoch 4 - Batch 70/88 - Loss: 2.1443\n",
            "Model 6 - Epoch 4 - Batch 80/88 - Loss: 2.2236\n",
            "Model 6 - Finished Epoch 4\n",
            "Model 6 - Starting Epoch 5\n",
            "Model 6 - Epoch 5 - Batch 10/88 - Loss: 2.1019\n",
            "Model 6 - Epoch 5 - Batch 20/88 - Loss: 2.0075\n",
            "Model 6 - Epoch 5 - Batch 30/88 - Loss: 2.1123\n",
            "Model 6 - Epoch 5 - Batch 40/88 - Loss: 2.1248\n",
            "Model 6 - Epoch 5 - Batch 50/88 - Loss: 2.0578\n",
            "Model 6 - Epoch 5 - Batch 60/88 - Loss: 2.1392\n",
            "Model 6 - Epoch 5 - Batch 70/88 - Loss: 2.1423\n",
            "Model 6 - Epoch 5 - Batch 80/88 - Loss: 2.2594\n",
            "Model 6 - Finished Epoch 5\n",
            "Model 6 - Starting Epoch 6\n",
            "Model 6 - Epoch 6 - Batch 10/88 - Loss: 2.2281\n",
            "Model 6 - Epoch 6 - Batch 20/88 - Loss: 2.1518\n",
            "Model 6 - Epoch 6 - Batch 30/88 - Loss: 2.1084\n",
            "Model 6 - Epoch 6 - Batch 40/88 - Loss: 2.0952\n",
            "Model 6 - Epoch 6 - Batch 50/88 - Loss: 2.1942\n",
            "Model 6 - Epoch 6 - Batch 60/88 - Loss: 2.1393\n",
            "Model 6 - Epoch 6 - Batch 70/88 - Loss: 2.0404\n",
            "Model 6 - Epoch 6 - Batch 80/88 - Loss: 2.1397\n",
            "Model 6 - Finished Epoch 6\n",
            "Model 6 - Starting Epoch 7\n",
            "Model 6 - Epoch 7 - Batch 10/88 - Loss: 2.0510\n",
            "Model 6 - Epoch 7 - Batch 20/88 - Loss: 2.1246\n",
            "Model 6 - Epoch 7 - Batch 30/88 - Loss: 2.1696\n",
            "Model 6 - Epoch 7 - Batch 40/88 - Loss: 2.1133\n",
            "Model 6 - Epoch 7 - Batch 50/88 - Loss: 2.1988\n",
            "Model 6 - Epoch 7 - Batch 60/88 - Loss: 2.0481\n",
            "Model 6 - Epoch 7 - Batch 70/88 - Loss: 2.1252\n",
            "Model 6 - Epoch 7 - Batch 80/88 - Loss: 2.0737\n",
            "Model 6 - Finished Epoch 7\n",
            "Model 6 - Starting Epoch 8\n",
            "Model 6 - Epoch 8 - Batch 10/88 - Loss: 2.0491\n",
            "Model 6 - Epoch 8 - Batch 20/88 - Loss: 2.0236\n",
            "Model 6 - Epoch 8 - Batch 30/88 - Loss: 2.0749\n",
            "Model 6 - Epoch 8 - Batch 40/88 - Loss: 2.1090\n",
            "Model 6 - Epoch 8 - Batch 50/88 - Loss: 2.0889\n",
            "Model 6 - Epoch 8 - Batch 60/88 - Loss: 1.9630\n",
            "Model 6 - Epoch 8 - Batch 70/88 - Loss: 2.0932\n",
            "Model 6 - Epoch 8 - Batch 80/88 - Loss: 2.1663\n",
            "Model 6 - Finished Epoch 8\n",
            "Model 6 - Starting Epoch 9\n",
            "Model 6 - Epoch 9 - Batch 10/88 - Loss: 1.9444\n",
            "Model 6 - Epoch 9 - Batch 20/88 - Loss: 1.9571\n",
            "Model 6 - Epoch 9 - Batch 30/88 - Loss: 2.1293\n",
            "Model 6 - Epoch 9 - Batch 40/88 - Loss: 2.0843\n",
            "Model 6 - Epoch 9 - Batch 50/88 - Loss: 2.0669\n",
            "Model 6 - Epoch 9 - Batch 60/88 - Loss: 2.0826\n",
            "Model 6 - Epoch 9 - Batch 70/88 - Loss: 2.1179\n",
            "Model 6 - Epoch 9 - Batch 80/88 - Loss: 2.1679\n",
            "Model 6 - Finished Epoch 9\n",
            "Model 6 - Starting Epoch 10\n",
            "Model 6 - Epoch 10 - Batch 10/88 - Loss: 2.0579\n",
            "Model 6 - Epoch 10 - Batch 20/88 - Loss: 2.0351\n",
            "Model 6 - Epoch 10 - Batch 30/88 - Loss: 2.0225\n",
            "Model 6 - Epoch 10 - Batch 40/88 - Loss: 2.0530\n",
            "Model 6 - Epoch 10 - Batch 50/88 - Loss: 2.1035\n",
            "Model 6 - Epoch 10 - Batch 60/88 - Loss: 2.1069\n",
            "Model 6 - Epoch 10 - Batch 70/88 - Loss: 2.0920\n",
            "Model 6 - Epoch 10 - Batch 80/88 - Loss: 2.1117\n",
            "Model 6 - Finished Epoch 10\n",
            "Model 6 done training for 10 epochs in generation 7\n",
            "Model 7 - Starting Epoch 1\n",
            "Model 7 - Epoch 1 - Batch 10/88 - Loss: 1.8477\n",
            "Model 7 - Epoch 1 - Batch 20/88 - Loss: 1.6767\n",
            "Model 7 - Epoch 1 - Batch 30/88 - Loss: 1.8052\n",
            "Model 7 - Epoch 1 - Batch 40/88 - Loss: 1.8479\n",
            "Model 7 - Epoch 1 - Batch 50/88 - Loss: 1.7299\n",
            "Model 7 - Epoch 1 - Batch 60/88 - Loss: 1.8963\n",
            "Model 7 - Epoch 1 - Batch 70/88 - Loss: 1.9390\n",
            "Model 7 - Epoch 1 - Batch 80/88 - Loss: 1.8783\n",
            "Model 7 - Finished Epoch 1\n",
            "Model 7 - Starting Epoch 2\n",
            "Model 7 - Epoch 2 - Batch 10/88 - Loss: 1.7364\n",
            "Model 7 - Epoch 2 - Batch 20/88 - Loss: 1.7904\n",
            "Model 7 - Epoch 2 - Batch 30/88 - Loss: 1.9337\n",
            "Model 7 - Epoch 2 - Batch 40/88 - Loss: 1.8294\n",
            "Model 7 - Epoch 2 - Batch 50/88 - Loss: 1.7635\n",
            "Model 7 - Epoch 2 - Batch 60/88 - Loss: 1.7798\n",
            "Model 7 - Epoch 2 - Batch 70/88 - Loss: 1.7627\n",
            "Model 7 - Epoch 2 - Batch 80/88 - Loss: 1.8306\n",
            "Model 7 - Finished Epoch 2\n",
            "Model 7 - Starting Epoch 3\n",
            "Model 7 - Epoch 3 - Batch 10/88 - Loss: 1.6207\n",
            "Model 7 - Epoch 3 - Batch 20/88 - Loss: 1.7965\n",
            "Model 7 - Epoch 3 - Batch 30/88 - Loss: 1.8296\n",
            "Model 7 - Epoch 3 - Batch 40/88 - Loss: 1.8025\n",
            "Model 7 - Epoch 3 - Batch 50/88 - Loss: 1.9257\n",
            "Model 7 - Epoch 3 - Batch 60/88 - Loss: 1.9019\n",
            "Model 7 - Epoch 3 - Batch 70/88 - Loss: 1.9204\n",
            "Model 7 - Epoch 3 - Batch 80/88 - Loss: 1.8207\n",
            "Model 7 - Finished Epoch 3\n",
            "Model 7 - Starting Epoch 4\n",
            "Model 7 - Epoch 4 - Batch 10/88 - Loss: 1.7126\n",
            "Model 7 - Epoch 4 - Batch 20/88 - Loss: 1.7735\n",
            "Model 7 - Epoch 4 - Batch 30/88 - Loss: 1.7906\n",
            "Model 7 - Epoch 4 - Batch 40/88 - Loss: 1.7063\n",
            "Model 7 - Epoch 4 - Batch 50/88 - Loss: 1.8176\n",
            "Model 7 - Epoch 4 - Batch 60/88 - Loss: 1.7103\n",
            "Model 7 - Epoch 4 - Batch 70/88 - Loss: 1.8465\n",
            "Model 7 - Epoch 4 - Batch 80/88 - Loss: 1.7968\n",
            "Model 7 - Finished Epoch 4\n",
            "Model 7 - Starting Epoch 5\n",
            "Model 7 - Epoch 5 - Batch 10/88 - Loss: 1.7456\n",
            "Model 7 - Epoch 5 - Batch 20/88 - Loss: 1.8346\n",
            "Model 7 - Epoch 5 - Batch 30/88 - Loss: 1.8704\n",
            "Model 7 - Epoch 5 - Batch 40/88 - Loss: 1.8003\n",
            "Model 7 - Epoch 5 - Batch 50/88 - Loss: 1.7536\n",
            "Model 7 - Epoch 5 - Batch 60/88 - Loss: 1.7660\n",
            "Model 7 - Epoch 5 - Batch 70/88 - Loss: 1.9214\n",
            "Model 7 - Epoch 5 - Batch 80/88 - Loss: 1.8018\n",
            "Model 7 - Finished Epoch 5\n",
            "Model 7 - Starting Epoch 6\n",
            "Model 7 - Epoch 6 - Batch 10/88 - Loss: 1.7501\n",
            "Model 7 - Epoch 6 - Batch 20/88 - Loss: 1.7813\n",
            "Model 7 - Epoch 6 - Batch 30/88 - Loss: 1.6961\n",
            "Model 7 - Epoch 6 - Batch 40/88 - Loss: 1.6568\n",
            "Model 7 - Epoch 6 - Batch 50/88 - Loss: 1.8390\n",
            "Model 7 - Epoch 6 - Batch 60/88 - Loss: 1.8089\n",
            "Model 7 - Epoch 6 - Batch 70/88 - Loss: 1.8120\n",
            "Model 7 - Epoch 6 - Batch 80/88 - Loss: 1.9201\n",
            "Model 7 - Finished Epoch 6\n",
            "Model 7 - Starting Epoch 7\n",
            "Model 7 - Epoch 7 - Batch 10/88 - Loss: 1.6854\n",
            "Model 7 - Epoch 7 - Batch 20/88 - Loss: 1.6715\n",
            "Model 7 - Epoch 7 - Batch 30/88 - Loss: 1.6656\n",
            "Model 7 - Epoch 7 - Batch 40/88 - Loss: 1.7367\n",
            "Model 7 - Epoch 7 - Batch 50/88 - Loss: 1.8672\n",
            "Model 7 - Epoch 7 - Batch 60/88 - Loss: 1.7249\n",
            "Model 7 - Epoch 7 - Batch 70/88 - Loss: 1.8512\n",
            "Model 7 - Epoch 7 - Batch 80/88 - Loss: 1.7333\n",
            "Model 7 - Finished Epoch 7\n",
            "Model 7 - Starting Epoch 8\n",
            "Model 7 - Epoch 8 - Batch 10/88 - Loss: 1.7458\n",
            "Model 7 - Epoch 8 - Batch 20/88 - Loss: 1.8040\n",
            "Model 7 - Epoch 8 - Batch 30/88 - Loss: 1.7497\n",
            "Model 7 - Epoch 8 - Batch 40/88 - Loss: 1.7785\n",
            "Model 7 - Epoch 8 - Batch 50/88 - Loss: 1.7284\n",
            "Model 7 - Epoch 8 - Batch 60/88 - Loss: 1.8060\n",
            "Model 7 - Epoch 8 - Batch 70/88 - Loss: 1.7508\n",
            "Model 7 - Epoch 8 - Batch 80/88 - Loss: 1.7541\n",
            "Model 7 - Finished Epoch 8\n",
            "Model 7 - Starting Epoch 9\n",
            "Model 7 - Epoch 9 - Batch 10/88 - Loss: 1.7865\n",
            "Model 7 - Epoch 9 - Batch 20/88 - Loss: 1.7843\n",
            "Model 7 - Epoch 9 - Batch 30/88 - Loss: 1.6804\n",
            "Model 7 - Epoch 9 - Batch 40/88 - Loss: 1.6915\n",
            "Model 7 - Epoch 9 - Batch 50/88 - Loss: 1.6998\n",
            "Model 7 - Epoch 9 - Batch 60/88 - Loss: 1.7892\n",
            "Model 7 - Epoch 9 - Batch 70/88 - Loss: 1.7307\n",
            "Model 7 - Epoch 9 - Batch 80/88 - Loss: 1.7295\n",
            "Model 7 - Finished Epoch 9\n",
            "Model 7 - Starting Epoch 10\n",
            "Model 7 - Epoch 10 - Batch 10/88 - Loss: 1.6892\n",
            "Model 7 - Epoch 10 - Batch 20/88 - Loss: 1.7089\n",
            "Model 7 - Epoch 10 - Batch 30/88 - Loss: 1.7524\n",
            "Model 7 - Epoch 10 - Batch 40/88 - Loss: 1.7229\n",
            "Model 7 - Epoch 10 - Batch 50/88 - Loss: 1.6246\n",
            "Model 7 - Epoch 10 - Batch 60/88 - Loss: 1.8338\n",
            "Model 7 - Epoch 10 - Batch 70/88 - Loss: 1.7663\n",
            "Model 7 - Epoch 10 - Batch 80/88 - Loss: 1.7208\n",
            "Model 7 - Finished Epoch 10\n",
            "Model 7 done training for 10 epochs in generation 7\n",
            "Model 8 - Starting Epoch 1\n",
            "Model 8 - Epoch 1 - Batch 10/88 - Loss: 1.8459\n",
            "Model 8 - Epoch 1 - Batch 20/88 - Loss: 1.8784\n",
            "Model 8 - Epoch 1 - Batch 30/88 - Loss: 1.9235\n",
            "Model 8 - Epoch 1 - Batch 40/88 - Loss: 2.0145\n",
            "Model 8 - Epoch 1 - Batch 50/88 - Loss: 1.8997\n",
            "Model 8 - Epoch 1 - Batch 60/88 - Loss: 1.9220\n",
            "Model 8 - Epoch 1 - Batch 70/88 - Loss: 1.9151\n",
            "Model 8 - Epoch 1 - Batch 80/88 - Loss: 1.9806\n",
            "Model 8 - Finished Epoch 1\n",
            "Model 8 - Starting Epoch 2\n",
            "Model 8 - Epoch 2 - Batch 10/88 - Loss: 1.9565\n",
            "Model 8 - Epoch 2 - Batch 20/88 - Loss: 1.9462\n",
            "Model 8 - Epoch 2 - Batch 30/88 - Loss: 1.8188\n",
            "Model 8 - Epoch 2 - Batch 40/88 - Loss: 1.9673\n",
            "Model 8 - Epoch 2 - Batch 50/88 - Loss: 1.8962\n",
            "Model 8 - Epoch 2 - Batch 60/88 - Loss: 1.9772\n",
            "Model 8 - Epoch 2 - Batch 70/88 - Loss: 1.9051\n",
            "Model 8 - Epoch 2 - Batch 80/88 - Loss: 1.8083\n",
            "Model 8 - Finished Epoch 2\n",
            "Model 8 - Starting Epoch 3\n",
            "Model 8 - Epoch 3 - Batch 10/88 - Loss: 1.8600\n",
            "Model 8 - Epoch 3 - Batch 20/88 - Loss: 1.8166\n",
            "Model 8 - Epoch 3 - Batch 30/88 - Loss: 1.8661\n",
            "Model 8 - Epoch 3 - Batch 40/88 - Loss: 1.8767\n",
            "Model 8 - Epoch 3 - Batch 50/88 - Loss: 1.8749\n",
            "Model 8 - Epoch 3 - Batch 60/88 - Loss: 1.8524\n",
            "Model 8 - Epoch 3 - Batch 70/88 - Loss: 1.9532\n",
            "Model 8 - Epoch 3 - Batch 80/88 - Loss: 2.0049\n",
            "Model 8 - Finished Epoch 3\n",
            "Model 8 - Starting Epoch 4\n",
            "Model 8 - Epoch 4 - Batch 10/88 - Loss: 1.7878\n",
            "Model 8 - Epoch 4 - Batch 20/88 - Loss: 1.9005\n",
            "Model 8 - Epoch 4 - Batch 30/88 - Loss: 1.8532\n",
            "Model 8 - Epoch 4 - Batch 40/88 - Loss: 1.9268\n",
            "Model 8 - Epoch 4 - Batch 50/88 - Loss: 1.9268\n",
            "Model 8 - Epoch 4 - Batch 60/88 - Loss: 1.8927\n",
            "Model 8 - Epoch 4 - Batch 70/88 - Loss: 1.9178\n",
            "Model 8 - Epoch 4 - Batch 80/88 - Loss: 1.9380\n",
            "Model 8 - Finished Epoch 4\n",
            "Model 8 - Starting Epoch 5\n",
            "Model 8 - Epoch 5 - Batch 10/88 - Loss: 1.8427\n",
            "Model 8 - Epoch 5 - Batch 20/88 - Loss: 1.8911\n",
            "Model 8 - Epoch 5 - Batch 30/88 - Loss: 1.8453\n",
            "Model 8 - Epoch 5 - Batch 40/88 - Loss: 1.8399\n",
            "Model 8 - Epoch 5 - Batch 50/88 - Loss: 1.9226\n",
            "Model 8 - Epoch 5 - Batch 60/88 - Loss: 1.7956\n",
            "Model 8 - Epoch 5 - Batch 70/88 - Loss: 1.8166\n",
            "Model 8 - Epoch 5 - Batch 80/88 - Loss: 1.8555\n",
            "Model 8 - Finished Epoch 5\n",
            "Model 8 - Starting Epoch 6\n",
            "Model 8 - Epoch 6 - Batch 10/88 - Loss: 1.7590\n",
            "Model 8 - Epoch 6 - Batch 20/88 - Loss: 1.8561\n",
            "Model 8 - Epoch 6 - Batch 30/88 - Loss: 1.8094\n",
            "Model 8 - Epoch 6 - Batch 40/88 - Loss: 1.8470\n",
            "Model 8 - Epoch 6 - Batch 50/88 - Loss: 1.7985\n",
            "Model 8 - Epoch 6 - Batch 60/88 - Loss: 1.9260\n",
            "Model 8 - Epoch 6 - Batch 70/88 - Loss: 1.9157\n",
            "Model 8 - Epoch 6 - Batch 80/88 - Loss: 1.7832\n",
            "Model 8 - Finished Epoch 6\n",
            "Model 8 - Starting Epoch 7\n",
            "Model 8 - Epoch 7 - Batch 10/88 - Loss: 1.9799\n",
            "Model 8 - Epoch 7 - Batch 20/88 - Loss: 1.8030\n",
            "Model 8 - Epoch 7 - Batch 30/88 - Loss: 1.8866\n",
            "Model 8 - Epoch 7 - Batch 40/88 - Loss: 1.7925\n",
            "Model 8 - Epoch 7 - Batch 50/88 - Loss: 1.8042\n",
            "Model 8 - Epoch 7 - Batch 60/88 - Loss: 1.8089\n",
            "Model 8 - Epoch 7 - Batch 70/88 - Loss: 1.9692\n",
            "Model 8 - Epoch 7 - Batch 80/88 - Loss: 1.8086\n",
            "Model 8 - Finished Epoch 7\n",
            "Model 8 - Starting Epoch 8\n",
            "Model 8 - Epoch 8 - Batch 10/88 - Loss: 1.7695\n",
            "Model 8 - Epoch 8 - Batch 20/88 - Loss: 1.9091\n",
            "Model 8 - Epoch 8 - Batch 30/88 - Loss: 1.7408\n",
            "Model 8 - Epoch 8 - Batch 40/88 - Loss: 1.7752\n",
            "Model 8 - Epoch 8 - Batch 50/88 - Loss: 1.9037\n",
            "Model 8 - Epoch 8 - Batch 60/88 - Loss: 1.7725\n",
            "Model 8 - Epoch 8 - Batch 70/88 - Loss: 1.9048\n",
            "Model 8 - Epoch 8 - Batch 80/88 - Loss: 1.8014\n",
            "Model 8 - Finished Epoch 8\n",
            "Model 8 - Starting Epoch 9\n",
            "Model 8 - Epoch 9 - Batch 10/88 - Loss: 1.8962\n",
            "Model 8 - Epoch 9 - Batch 20/88 - Loss: 1.8196\n",
            "Model 8 - Epoch 9 - Batch 30/88 - Loss: 1.8099\n",
            "Model 8 - Epoch 9 - Batch 40/88 - Loss: 1.9512\n",
            "Model 8 - Epoch 9 - Batch 50/88 - Loss: 1.7596\n",
            "Model 8 - Epoch 9 - Batch 60/88 - Loss: 1.8060\n",
            "Model 8 - Epoch 9 - Batch 70/88 - Loss: 1.8605\n",
            "Model 8 - Epoch 9 - Batch 80/88 - Loss: 1.8623\n",
            "Model 8 - Finished Epoch 9\n",
            "Model 8 - Starting Epoch 10\n",
            "Model 8 - Epoch 10 - Batch 10/88 - Loss: 1.7547\n",
            "Model 8 - Epoch 10 - Batch 20/88 - Loss: 1.6974\n",
            "Model 8 - Epoch 10 - Batch 30/88 - Loss: 1.8028\n",
            "Model 8 - Epoch 10 - Batch 40/88 - Loss: 1.7417\n",
            "Model 8 - Epoch 10 - Batch 50/88 - Loss: 1.7885\n",
            "Model 8 - Epoch 10 - Batch 60/88 - Loss: 1.8749\n",
            "Model 8 - Epoch 10 - Batch 70/88 - Loss: 1.9601\n",
            "Model 8 - Epoch 10 - Batch 80/88 - Loss: 1.8793\n",
            "Model 8 - Finished Epoch 10\n",
            "Model 8 done training for 10 epochs in generation 7\n",
            "Model 9 - Starting Epoch 1\n",
            "Model 9 - Epoch 1 - Batch 10/88 - Loss: 2.0871\n",
            "Model 9 - Epoch 1 - Batch 20/88 - Loss: 2.1677\n",
            "Model 9 - Epoch 1 - Batch 30/88 - Loss: 2.0601\n",
            "Model 9 - Epoch 1 - Batch 40/88 - Loss: 2.1160\n",
            "Model 9 - Epoch 1 - Batch 50/88 - Loss: 2.0633\n",
            "Model 9 - Epoch 1 - Batch 60/88 - Loss: 2.2689\n",
            "Model 9 - Epoch 1 - Batch 70/88 - Loss: 2.1268\n",
            "Model 9 - Epoch 1 - Batch 80/88 - Loss: 2.2944\n",
            "Model 9 - Finished Epoch 1\n",
            "Model 9 - Starting Epoch 2\n",
            "Model 9 - Epoch 2 - Batch 10/88 - Loss: 1.9756\n",
            "Model 9 - Epoch 2 - Batch 20/88 - Loss: 2.1581\n",
            "Model 9 - Epoch 2 - Batch 30/88 - Loss: 2.2123\n",
            "Model 9 - Epoch 2 - Batch 40/88 - Loss: 2.2107\n",
            "Model 9 - Epoch 2 - Batch 50/88 - Loss: 2.0800\n",
            "Model 9 - Epoch 2 - Batch 60/88 - Loss: 2.1500\n",
            "Model 9 - Epoch 2 - Batch 70/88 - Loss: 2.2146\n",
            "Model 9 - Epoch 2 - Batch 80/88 - Loss: 2.2260\n",
            "Model 9 - Finished Epoch 2\n",
            "Model 9 - Starting Epoch 3\n",
            "Model 9 - Epoch 3 - Batch 10/88 - Loss: 2.1482\n",
            "Model 9 - Epoch 3 - Batch 20/88 - Loss: 2.1063\n",
            "Model 9 - Epoch 3 - Batch 30/88 - Loss: 2.2459\n",
            "Model 9 - Epoch 3 - Batch 40/88 - Loss: 2.0898\n",
            "Model 9 - Epoch 3 - Batch 50/88 - Loss: 2.1829\n",
            "Model 9 - Epoch 3 - Batch 60/88 - Loss: 2.1436\n",
            "Model 9 - Epoch 3 - Batch 70/88 - Loss: 2.2039\n",
            "Model 9 - Epoch 3 - Batch 80/88 - Loss: 2.1531\n",
            "Model 9 - Finished Epoch 3\n",
            "Model 9 - Starting Epoch 4\n",
            "Model 9 - Epoch 4 - Batch 10/88 - Loss: 2.0640\n",
            "Model 9 - Epoch 4 - Batch 20/88 - Loss: 2.0019\n",
            "Model 9 - Epoch 4 - Batch 30/88 - Loss: 2.1172\n",
            "Model 9 - Epoch 4 - Batch 40/88 - Loss: 2.1383\n",
            "Model 9 - Epoch 4 - Batch 50/88 - Loss: 2.2301\n",
            "Model 9 - Epoch 4 - Batch 60/88 - Loss: 2.1119\n",
            "Model 9 - Epoch 4 - Batch 70/88 - Loss: 2.1885\n",
            "Model 9 - Epoch 4 - Batch 80/88 - Loss: 2.2348\n",
            "Model 9 - Finished Epoch 4\n",
            "Model 9 - Starting Epoch 5\n",
            "Model 9 - Epoch 5 - Batch 10/88 - Loss: 2.1237\n",
            "Model 9 - Epoch 5 - Batch 20/88 - Loss: 2.0007\n",
            "Model 9 - Epoch 5 - Batch 30/88 - Loss: 2.2059\n",
            "Model 9 - Epoch 5 - Batch 40/88 - Loss: 2.2377\n",
            "Model 9 - Epoch 5 - Batch 50/88 - Loss: 2.1268\n",
            "Model 9 - Epoch 5 - Batch 60/88 - Loss: 2.2165\n",
            "Model 9 - Epoch 5 - Batch 70/88 - Loss: 2.0659\n",
            "Model 9 - Epoch 5 - Batch 80/88 - Loss: 2.0769\n",
            "Model 9 - Finished Epoch 5\n",
            "Model 9 - Starting Epoch 6\n",
            "Model 9 - Epoch 6 - Batch 10/88 - Loss: 2.0370\n",
            "Model 9 - Epoch 6 - Batch 20/88 - Loss: 2.1485\n",
            "Model 9 - Epoch 6 - Batch 30/88 - Loss: 2.0078\n",
            "Model 9 - Epoch 6 - Batch 40/88 - Loss: 2.1104\n",
            "Model 9 - Epoch 6 - Batch 50/88 - Loss: 2.1770\n",
            "Model 9 - Epoch 6 - Batch 60/88 - Loss: 2.1090\n",
            "Model 9 - Epoch 6 - Batch 70/88 - Loss: 2.0092\n",
            "Model 9 - Epoch 6 - Batch 80/88 - Loss: 2.1648\n",
            "Model 9 - Finished Epoch 6\n",
            "Model 9 - Starting Epoch 7\n",
            "Model 9 - Epoch 7 - Batch 10/88 - Loss: 2.0058\n",
            "Model 9 - Epoch 7 - Batch 20/88 - Loss: 2.0288\n",
            "Model 9 - Epoch 7 - Batch 30/88 - Loss: 2.1008\n",
            "Model 9 - Epoch 7 - Batch 40/88 - Loss: 2.1516\n",
            "Model 9 - Epoch 7 - Batch 50/88 - Loss: 2.1309\n",
            "Model 9 - Epoch 7 - Batch 60/88 - Loss: 2.1617\n",
            "Model 9 - Epoch 7 - Batch 70/88 - Loss: 2.0955\n",
            "Model 9 - Epoch 7 - Batch 80/88 - Loss: 2.1751\n",
            "Model 9 - Finished Epoch 7\n",
            "Model 9 - Starting Epoch 8\n",
            "Model 9 - Epoch 8 - Batch 10/88 - Loss: 2.0760\n",
            "Model 9 - Epoch 8 - Batch 20/88 - Loss: 2.0803\n",
            "Model 9 - Epoch 8 - Batch 30/88 - Loss: 1.9846\n",
            "Model 9 - Epoch 8 - Batch 40/88 - Loss: 2.2148\n",
            "Model 9 - Epoch 8 - Batch 50/88 - Loss: 2.1278\n",
            "Model 9 - Epoch 8 - Batch 60/88 - Loss: 2.1119\n",
            "Model 9 - Epoch 8 - Batch 70/88 - Loss: 2.0988\n",
            "Model 9 - Epoch 8 - Batch 80/88 - Loss: 2.0641\n",
            "Model 9 - Finished Epoch 8\n",
            "Model 9 - Starting Epoch 9\n",
            "Model 9 - Epoch 9 - Batch 10/88 - Loss: 2.0689\n",
            "Model 9 - Epoch 9 - Batch 20/88 - Loss: 2.0511\n",
            "Model 9 - Epoch 9 - Batch 30/88 - Loss: 2.0919\n",
            "Model 9 - Epoch 9 - Batch 40/88 - Loss: 2.1285\n",
            "Model 9 - Epoch 9 - Batch 50/88 - Loss: 1.9828\n",
            "Model 9 - Epoch 9 - Batch 60/88 - Loss: 2.1035\n",
            "Model 9 - Epoch 9 - Batch 70/88 - Loss: 2.0727\n",
            "Model 9 - Epoch 9 - Batch 80/88 - Loss: 2.0181\n",
            "Model 9 - Finished Epoch 9\n",
            "Model 9 - Starting Epoch 10\n",
            "Model 9 - Epoch 10 - Batch 10/88 - Loss: 2.0690\n",
            "Model 9 - Epoch 10 - Batch 20/88 - Loss: 1.9973\n",
            "Model 9 - Epoch 10 - Batch 30/88 - Loss: 2.0989\n",
            "Model 9 - Epoch 10 - Batch 40/88 - Loss: 2.1325\n",
            "Model 9 - Epoch 10 - Batch 50/88 - Loss: 2.0779\n",
            "Model 9 - Epoch 10 - Batch 60/88 - Loss: 2.0386\n",
            "Model 9 - Epoch 10 - Batch 70/88 - Loss: 2.0393\n",
            "Model 9 - Epoch 10 - Batch 80/88 - Loss: 2.0891\n",
            "Model 9 - Finished Epoch 10\n",
            "Model 9 done training for 10 epochs in generation 7\n",
            "  🎉 New best model with accuracy: 0.6744\n",
            "  ✅ Best Acc This Gen: 0.6744 | Worst: 0.6112\n",
            "\n",
            "🌱 Generation 8 | Population size: 5\n",
            "Model 1 - Starting Epoch 1\n",
            "Model 1 - Epoch 1 - Batch 10/88 - Loss: 1.5180\n",
            "Model 1 - Epoch 1 - Batch 20/88 - Loss: 1.6043\n",
            "Model 1 - Epoch 1 - Batch 30/88 - Loss: 1.4758\n",
            "Model 1 - Epoch 1 - Batch 40/88 - Loss: 1.5469\n",
            "Model 1 - Epoch 1 - Batch 50/88 - Loss: 1.5975\n",
            "Model 1 - Epoch 1 - Batch 60/88 - Loss: 1.5883\n",
            "Model 1 - Epoch 1 - Batch 70/88 - Loss: 1.6114\n",
            "Model 1 - Epoch 1 - Batch 80/88 - Loss: 1.5361\n",
            "Model 1 - Finished Epoch 1\n",
            "Model 1 - Starting Epoch 2\n",
            "Model 1 - Epoch 2 - Batch 10/88 - Loss: 1.4737\n",
            "Model 1 - Epoch 2 - Batch 20/88 - Loss: 1.4793\n",
            "Model 1 - Epoch 2 - Batch 30/88 - Loss: 1.5502\n",
            "Model 1 - Epoch 2 - Batch 40/88 - Loss: 1.5196\n",
            "Model 1 - Epoch 2 - Batch 50/88 - Loss: 1.5525\n",
            "Model 1 - Epoch 2 - Batch 60/88 - Loss: 1.6123\n",
            "Model 1 - Epoch 2 - Batch 70/88 - Loss: 1.5019\n",
            "Model 1 - Epoch 2 - Batch 80/88 - Loss: 1.5545\n",
            "Model 1 - Finished Epoch 2\n",
            "Model 1 - Starting Epoch 3\n",
            "Model 1 - Epoch 3 - Batch 10/88 - Loss: 1.5894\n",
            "Model 1 - Epoch 3 - Batch 20/88 - Loss: 1.5424\n",
            "Model 1 - Epoch 3 - Batch 30/88 - Loss: 1.5612\n",
            "Model 1 - Epoch 3 - Batch 40/88 - Loss: 1.5250\n",
            "Model 1 - Epoch 3 - Batch 50/88 - Loss: 1.5474\n",
            "Model 1 - Epoch 3 - Batch 60/88 - Loss: 1.5197\n",
            "Model 1 - Epoch 3 - Batch 70/88 - Loss: 1.4876\n",
            "Model 1 - Epoch 3 - Batch 80/88 - Loss: 1.6050\n",
            "Model 1 - Finished Epoch 3\n",
            "Model 1 - Starting Epoch 4\n",
            "Model 1 - Epoch 4 - Batch 10/88 - Loss: 1.4266\n",
            "Model 1 - Epoch 4 - Batch 20/88 - Loss: 1.5163\n",
            "Model 1 - Epoch 4 - Batch 30/88 - Loss: 1.6023\n",
            "Model 1 - Epoch 4 - Batch 40/88 - Loss: 1.4667\n",
            "Model 1 - Epoch 4 - Batch 50/88 - Loss: 1.4605\n",
            "Model 1 - Epoch 4 - Batch 60/88 - Loss: 1.5638\n",
            "Model 1 - Epoch 4 - Batch 70/88 - Loss: 1.6157\n",
            "Model 1 - Epoch 4 - Batch 80/88 - Loss: 1.5715\n",
            "Model 1 - Finished Epoch 4\n",
            "Model 1 - Starting Epoch 5\n",
            "Model 1 - Epoch 5 - Batch 10/88 - Loss: 1.4523\n",
            "Model 1 - Epoch 5 - Batch 20/88 - Loss: 1.4089\n",
            "Model 1 - Epoch 5 - Batch 30/88 - Loss: 1.5151\n",
            "Model 1 - Epoch 5 - Batch 40/88 - Loss: 1.4689\n",
            "Model 1 - Epoch 5 - Batch 50/88 - Loss: 1.4699\n",
            "Model 1 - Epoch 5 - Batch 60/88 - Loss: 1.6475\n",
            "Model 1 - Epoch 5 - Batch 70/88 - Loss: 1.4858\n",
            "Model 1 - Epoch 5 - Batch 80/88 - Loss: 1.5252\n",
            "Model 1 - Finished Epoch 5\n",
            "Model 1 - Starting Epoch 6\n",
            "Model 1 - Epoch 6 - Batch 10/88 - Loss: 1.4997\n",
            "Model 1 - Epoch 6 - Batch 20/88 - Loss: 1.6239\n",
            "Model 1 - Epoch 6 - Batch 30/88 - Loss: 1.5461\n",
            "Model 1 - Epoch 6 - Batch 40/88 - Loss: 1.5095\n",
            "Model 1 - Epoch 6 - Batch 50/88 - Loss: 1.5396\n",
            "Model 1 - Epoch 6 - Batch 60/88 - Loss: 1.5267\n",
            "Model 1 - Epoch 6 - Batch 70/88 - Loss: 1.5444\n",
            "Model 1 - Epoch 6 - Batch 80/88 - Loss: 1.4742\n",
            "Model 1 - Finished Epoch 6\n",
            "Model 1 - Starting Epoch 7\n",
            "Model 1 - Epoch 7 - Batch 10/88 - Loss: 1.4422\n",
            "Model 1 - Epoch 7 - Batch 20/88 - Loss: 1.4392\n",
            "Model 1 - Epoch 7 - Batch 30/88 - Loss: 1.4804\n",
            "Model 1 - Epoch 7 - Batch 40/88 - Loss: 1.3447\n",
            "Model 1 - Epoch 7 - Batch 50/88 - Loss: 1.5767\n",
            "Model 1 - Epoch 7 - Batch 60/88 - Loss: 1.5309\n",
            "Model 1 - Epoch 7 - Batch 70/88 - Loss: 1.5513\n",
            "Model 1 - Epoch 7 - Batch 80/88 - Loss: 1.5112\n",
            "Model 1 - Finished Epoch 7\n",
            "Model 1 - Starting Epoch 8\n",
            "Model 1 - Epoch 8 - Batch 10/88 - Loss: 1.4132\n",
            "Model 1 - Epoch 8 - Batch 20/88 - Loss: 1.5130\n",
            "Model 1 - Epoch 8 - Batch 30/88 - Loss: 1.4785\n",
            "Model 1 - Epoch 8 - Batch 40/88 - Loss: 1.5075\n",
            "Model 1 - Epoch 8 - Batch 50/88 - Loss: 1.5078\n",
            "Model 1 - Epoch 8 - Batch 60/88 - Loss: 1.4622\n",
            "Model 1 - Epoch 8 - Batch 70/88 - Loss: 1.5022\n",
            "Model 1 - Epoch 8 - Batch 80/88 - Loss: 1.4875\n",
            "Model 1 - Finished Epoch 8\n",
            "Model 1 - Starting Epoch 9\n",
            "Model 1 - Epoch 9 - Batch 10/88 - Loss: 1.3167\n",
            "Model 1 - Epoch 9 - Batch 20/88 - Loss: 1.3904\n",
            "Model 1 - Epoch 9 - Batch 30/88 - Loss: 1.5119\n",
            "Model 1 - Epoch 9 - Batch 40/88 - Loss: 1.4350\n",
            "Model 1 - Epoch 9 - Batch 50/88 - Loss: 1.4278\n",
            "Model 1 - Epoch 9 - Batch 60/88 - Loss: 1.5170\n",
            "Model 1 - Epoch 9 - Batch 70/88 - Loss: 1.4616\n",
            "Model 1 - Epoch 9 - Batch 80/88 - Loss: 1.5608\n",
            "Model 1 - Finished Epoch 9\n",
            "Model 1 - Starting Epoch 10\n",
            "Model 1 - Epoch 10 - Batch 10/88 - Loss: 1.4774\n",
            "Model 1 - Epoch 10 - Batch 20/88 - Loss: 1.5168\n",
            "Model 1 - Epoch 10 - Batch 30/88 - Loss: 1.4676\n",
            "Model 1 - Epoch 10 - Batch 40/88 - Loss: 1.5588\n",
            "Model 1 - Epoch 10 - Batch 50/88 - Loss: 1.5023\n",
            "Model 1 - Epoch 10 - Batch 60/88 - Loss: 1.3668\n",
            "Model 1 - Epoch 10 - Batch 70/88 - Loss: 1.4608\n",
            "Model 1 - Epoch 10 - Batch 80/88 - Loss: 1.5639\n",
            "Model 1 - Finished Epoch 10\n",
            "Model 1 done training for 10 epochs in generation 8\n",
            "Model 2 - Starting Epoch 1\n",
            "Model 2 - Epoch 1 - Batch 10/88 - Loss: 1.7488\n",
            "Model 2 - Epoch 1 - Batch 20/88 - Loss: 1.8365\n",
            "Model 2 - Epoch 1 - Batch 30/88 - Loss: 1.7684\n",
            "Model 2 - Epoch 1 - Batch 40/88 - Loss: 1.7557\n",
            "Model 2 - Epoch 1 - Batch 50/88 - Loss: 1.7820\n",
            "Model 2 - Epoch 1 - Batch 60/88 - Loss: 1.8019\n",
            "Model 2 - Epoch 1 - Batch 70/88 - Loss: 1.7890\n",
            "Model 2 - Epoch 1 - Batch 80/88 - Loss: 1.7467\n",
            "Model 2 - Finished Epoch 1\n",
            "Model 2 - Starting Epoch 2\n",
            "Model 2 - Epoch 2 - Batch 10/88 - Loss: 1.7552\n",
            "Model 2 - Epoch 2 - Batch 20/88 - Loss: 1.7650\n",
            "Model 2 - Epoch 2 - Batch 30/88 - Loss: 1.7967\n",
            "Model 2 - Epoch 2 - Batch 40/88 - Loss: 1.7582\n",
            "Model 2 - Epoch 2 - Batch 50/88 - Loss: 1.7767\n",
            "Model 2 - Epoch 2 - Batch 60/88 - Loss: 1.7770\n",
            "Model 2 - Epoch 2 - Batch 70/88 - Loss: 1.8787\n",
            "Model 2 - Epoch 2 - Batch 80/88 - Loss: 1.9180\n",
            "Model 2 - Finished Epoch 2\n",
            "Model 2 - Starting Epoch 3\n",
            "Model 2 - Epoch 3 - Batch 10/88 - Loss: 1.6582\n",
            "Model 2 - Epoch 3 - Batch 20/88 - Loss: 1.7276\n",
            "Model 2 - Epoch 3 - Batch 30/88 - Loss: 1.7466\n",
            "Model 2 - Epoch 3 - Batch 40/88 - Loss: 1.8128\n",
            "Model 2 - Epoch 3 - Batch 50/88 - Loss: 1.7205\n",
            "Model 2 - Epoch 3 - Batch 60/88 - Loss: 1.7507\n",
            "Model 2 - Epoch 3 - Batch 70/88 - Loss: 1.7520\n",
            "Model 2 - Epoch 3 - Batch 80/88 - Loss: 1.8508\n",
            "Model 2 - Finished Epoch 3\n",
            "Model 2 - Starting Epoch 4\n",
            "Model 2 - Epoch 4 - Batch 10/88 - Loss: 1.6944\n",
            "Model 2 - Epoch 4 - Batch 20/88 - Loss: 1.7538\n",
            "Model 2 - Epoch 4 - Batch 30/88 - Loss: 1.7359\n",
            "Model 2 - Epoch 4 - Batch 40/88 - Loss: 1.8241\n",
            "Model 2 - Epoch 4 - Batch 50/88 - Loss: 1.7582\n",
            "Model 2 - Epoch 4 - Batch 60/88 - Loss: 1.8447\n",
            "Model 2 - Epoch 4 - Batch 70/88 - Loss: 1.8768\n",
            "Model 2 - Epoch 4 - Batch 80/88 - Loss: 1.8699\n",
            "Model 2 - Finished Epoch 4\n",
            "Model 2 - Starting Epoch 5\n",
            "Model 2 - Epoch 5 - Batch 10/88 - Loss: 1.6783\n",
            "Model 2 - Epoch 5 - Batch 20/88 - Loss: 1.8257\n",
            "Model 2 - Epoch 5 - Batch 30/88 - Loss: 1.8487\n",
            "Model 2 - Epoch 5 - Batch 40/88 - Loss: 1.7782\n",
            "Model 2 - Epoch 5 - Batch 50/88 - Loss: 1.8120\n",
            "Model 2 - Epoch 5 - Batch 60/88 - Loss: 1.8011\n",
            "Model 2 - Epoch 5 - Batch 70/88 - Loss: 1.7001\n",
            "Model 2 - Epoch 5 - Batch 80/88 - Loss: 1.8291\n",
            "Model 2 - Finished Epoch 5\n",
            "Model 2 - Starting Epoch 6\n",
            "Model 2 - Epoch 6 - Batch 10/88 - Loss: 1.7971\n",
            "Model 2 - Epoch 6 - Batch 20/88 - Loss: 1.8070\n",
            "Model 2 - Epoch 6 - Batch 30/88 - Loss: 1.7193\n",
            "Model 2 - Epoch 6 - Batch 40/88 - Loss: 1.6367\n",
            "Model 2 - Epoch 6 - Batch 50/88 - Loss: 1.7596\n",
            "Model 2 - Epoch 6 - Batch 60/88 - Loss: 1.7680\n",
            "Model 2 - Epoch 6 - Batch 70/88 - Loss: 1.8812\n",
            "Model 2 - Epoch 6 - Batch 80/88 - Loss: 1.6943\n",
            "Model 2 - Finished Epoch 6\n",
            "Model 2 - Starting Epoch 7\n",
            "Model 2 - Epoch 7 - Batch 10/88 - Loss: 1.7105\n",
            "Model 2 - Epoch 7 - Batch 20/88 - Loss: 1.7498\n",
            "Model 2 - Epoch 7 - Batch 30/88 - Loss: 1.7792\n",
            "Model 2 - Epoch 7 - Batch 40/88 - Loss: 1.7157\n",
            "Model 2 - Epoch 7 - Batch 50/88 - Loss: 1.7901\n",
            "Model 2 - Epoch 7 - Batch 60/88 - Loss: 1.7465\n",
            "Model 2 - Epoch 7 - Batch 70/88 - Loss: 1.8629\n",
            "Model 2 - Epoch 7 - Batch 80/88 - Loss: 1.7956\n",
            "Model 2 - Finished Epoch 7\n",
            "Model 2 - Starting Epoch 8\n",
            "Model 2 - Epoch 8 - Batch 10/88 - Loss: 1.8285\n",
            "Model 2 - Epoch 8 - Batch 20/88 - Loss: 1.7997\n",
            "Model 2 - Epoch 8 - Batch 30/88 - Loss: 1.7913\n",
            "Model 2 - Epoch 8 - Batch 40/88 - Loss: 1.6734\n",
            "Model 2 - Epoch 8 - Batch 50/88 - Loss: 1.8163\n",
            "Model 2 - Epoch 8 - Batch 60/88 - Loss: 1.9097\n",
            "Model 2 - Epoch 8 - Batch 70/88 - Loss: 1.6553\n",
            "Model 2 - Epoch 8 - Batch 80/88 - Loss: 1.7634\n",
            "Model 2 - Finished Epoch 8\n",
            "Model 2 - Starting Epoch 9\n",
            "Model 2 - Epoch 9 - Batch 10/88 - Loss: 1.9005\n",
            "Model 2 - Epoch 9 - Batch 20/88 - Loss: 1.6908\n",
            "Model 2 - Epoch 9 - Batch 30/88 - Loss: 1.6755\n",
            "Model 2 - Epoch 9 - Batch 40/88 - Loss: 1.7721\n",
            "Model 2 - Epoch 9 - Batch 50/88 - Loss: 1.7859\n",
            "Model 2 - Epoch 9 - Batch 60/88 - Loss: 1.7745\n",
            "Model 2 - Epoch 9 - Batch 70/88 - Loss: 1.7186\n",
            "Model 2 - Epoch 9 - Batch 80/88 - Loss: 1.7007\n",
            "Model 2 - Finished Epoch 9\n",
            "Model 2 - Starting Epoch 10\n",
            "Model 2 - Epoch 10 - Batch 10/88 - Loss: 1.5780\n",
            "Model 2 - Epoch 10 - Batch 20/88 - Loss: 1.7909\n",
            "Model 2 - Epoch 10 - Batch 30/88 - Loss: 1.7283\n",
            "Model 2 - Epoch 10 - Batch 40/88 - Loss: 1.9128\n",
            "Model 2 - Epoch 10 - Batch 50/88 - Loss: 1.6638\n",
            "Model 2 - Epoch 10 - Batch 60/88 - Loss: 1.7378\n",
            "Model 2 - Epoch 10 - Batch 70/88 - Loss: 1.7439\n",
            "Model 2 - Epoch 10 - Batch 80/88 - Loss: 1.7993\n",
            "Model 2 - Finished Epoch 10\n",
            "Model 2 done training for 10 epochs in generation 8\n",
            "Model 3 - Starting Epoch 1\n",
            "Model 3 - Epoch 1 - Batch 10/88 - Loss: 1.8179\n",
            "Model 3 - Epoch 1 - Batch 20/88 - Loss: 1.9092\n",
            "Model 3 - Epoch 1 - Batch 30/88 - Loss: 1.9123\n",
            "Model 3 - Epoch 1 - Batch 40/88 - Loss: 2.0051\n",
            "Model 3 - Epoch 1 - Batch 50/88 - Loss: 1.9188\n",
            "Model 3 - Epoch 1 - Batch 60/88 - Loss: 1.8621\n",
            "Model 3 - Epoch 1 - Batch 70/88 - Loss: 1.9413\n",
            "Model 3 - Epoch 1 - Batch 80/88 - Loss: 1.9936\n",
            "Model 3 - Finished Epoch 1\n",
            "Model 3 - Starting Epoch 2\n",
            "Model 3 - Epoch 2 - Batch 10/88 - Loss: 1.8999\n",
            "Model 3 - Epoch 2 - Batch 20/88 - Loss: 1.8419\n",
            "Model 3 - Epoch 2 - Batch 30/88 - Loss: 1.8833\n",
            "Model 3 - Epoch 2 - Batch 40/88 - Loss: 1.8929\n",
            "Model 3 - Epoch 2 - Batch 50/88 - Loss: 1.8916\n",
            "Model 3 - Epoch 2 - Batch 60/88 - Loss: 2.0462\n",
            "Model 3 - Epoch 2 - Batch 70/88 - Loss: 1.8783\n",
            "Model 3 - Epoch 2 - Batch 80/88 - Loss: 1.9563\n",
            "Model 3 - Finished Epoch 2\n",
            "Model 3 - Starting Epoch 3\n",
            "Model 3 - Epoch 3 - Batch 10/88 - Loss: 1.8615\n",
            "Model 3 - Epoch 3 - Batch 20/88 - Loss: 2.0079\n",
            "Model 3 - Epoch 3 - Batch 30/88 - Loss: 1.9501\n",
            "Model 3 - Epoch 3 - Batch 40/88 - Loss: 1.9923\n",
            "Model 3 - Epoch 3 - Batch 50/88 - Loss: 2.0195\n",
            "Model 3 - Epoch 3 - Batch 60/88 - Loss: 1.8182\n",
            "Model 3 - Epoch 3 - Batch 70/88 - Loss: 1.8850\n",
            "Model 3 - Epoch 3 - Batch 80/88 - Loss: 1.9065\n",
            "Model 3 - Finished Epoch 3\n",
            "Model 3 - Starting Epoch 4\n",
            "Model 3 - Epoch 4 - Batch 10/88 - Loss: 1.8583\n",
            "Model 3 - Epoch 4 - Batch 20/88 - Loss: 2.0126\n",
            "Model 3 - Epoch 4 - Batch 30/88 - Loss: 1.8745\n",
            "Model 3 - Epoch 4 - Batch 40/88 - Loss: 2.0084\n",
            "Model 3 - Epoch 4 - Batch 50/88 - Loss: 1.8881\n",
            "Model 3 - Epoch 4 - Batch 60/88 - Loss: 1.7651\n",
            "Model 3 - Epoch 4 - Batch 70/88 - Loss: 1.8473\n",
            "Model 3 - Epoch 4 - Batch 80/88 - Loss: 1.8825\n",
            "Model 3 - Finished Epoch 4\n",
            "Model 3 - Starting Epoch 5\n",
            "Model 3 - Epoch 5 - Batch 10/88 - Loss: 1.8829\n",
            "Model 3 - Epoch 5 - Batch 20/88 - Loss: 1.8198\n",
            "Model 3 - Epoch 5 - Batch 30/88 - Loss: 1.8519\n",
            "Model 3 - Epoch 5 - Batch 40/88 - Loss: 2.0157\n",
            "Model 3 - Epoch 5 - Batch 50/88 - Loss: 1.9312\n",
            "Model 3 - Epoch 5 - Batch 60/88 - Loss: 1.8509\n",
            "Model 3 - Epoch 5 - Batch 70/88 - Loss: 1.9462\n",
            "Model 3 - Epoch 5 - Batch 80/88 - Loss: 1.9463\n",
            "Model 3 - Finished Epoch 5\n",
            "Model 3 - Starting Epoch 6\n",
            "Model 3 - Epoch 6 - Batch 10/88 - Loss: 1.8353\n",
            "Model 3 - Epoch 6 - Batch 20/88 - Loss: 1.9155\n",
            "Model 3 - Epoch 6 - Batch 30/88 - Loss: 1.8029\n",
            "Model 3 - Epoch 6 - Batch 40/88 - Loss: 1.8390\n",
            "Model 3 - Epoch 6 - Batch 50/88 - Loss: 1.8246\n",
            "Model 3 - Epoch 6 - Batch 60/88 - Loss: 1.8587\n",
            "Model 3 - Epoch 6 - Batch 70/88 - Loss: 1.7938\n",
            "Model 3 - Epoch 6 - Batch 80/88 - Loss: 1.9500\n",
            "Model 3 - Finished Epoch 6\n",
            "Model 3 - Starting Epoch 7\n",
            "Model 3 - Epoch 7 - Batch 10/88 - Loss: 1.8689\n",
            "Model 3 - Epoch 7 - Batch 20/88 - Loss: 1.8459\n",
            "Model 3 - Epoch 7 - Batch 30/88 - Loss: 1.6891\n",
            "Model 3 - Epoch 7 - Batch 40/88 - Loss: 1.8884\n",
            "Model 3 - Epoch 7 - Batch 50/88 - Loss: 1.8507\n",
            "Model 3 - Epoch 7 - Batch 60/88 - Loss: 1.7622\n",
            "Model 3 - Epoch 7 - Batch 70/88 - Loss: 1.8916\n",
            "Model 3 - Epoch 7 - Batch 80/88 - Loss: 1.9253\n",
            "Model 3 - Finished Epoch 7\n",
            "Model 3 - Starting Epoch 8\n",
            "Model 3 - Epoch 8 - Batch 10/88 - Loss: 1.7463\n",
            "Model 3 - Epoch 8 - Batch 20/88 - Loss: 1.9633\n",
            "Model 3 - Epoch 8 - Batch 30/88 - Loss: 1.8756\n",
            "Model 3 - Epoch 8 - Batch 40/88 - Loss: 1.7973\n",
            "Model 3 - Epoch 8 - Batch 50/88 - Loss: 1.8928\n",
            "Model 3 - Epoch 8 - Batch 60/88 - Loss: 1.9287\n",
            "Model 3 - Epoch 8 - Batch 70/88 - Loss: 1.8452\n",
            "Model 3 - Epoch 8 - Batch 80/88 - Loss: 1.9705\n",
            "Model 3 - Finished Epoch 8\n",
            "Model 3 - Starting Epoch 9\n",
            "Model 3 - Epoch 9 - Batch 10/88 - Loss: 1.8682\n",
            "Model 3 - Epoch 9 - Batch 20/88 - Loss: 1.8121\n",
            "Model 3 - Epoch 9 - Batch 30/88 - Loss: 1.8563\n",
            "Model 3 - Epoch 9 - Batch 40/88 - Loss: 1.8760\n",
            "Model 3 - Epoch 9 - Batch 50/88 - Loss: 1.8491\n",
            "Model 3 - Epoch 9 - Batch 60/88 - Loss: 2.0018\n",
            "Model 3 - Epoch 9 - Batch 70/88 - Loss: 1.7533\n",
            "Model 3 - Epoch 9 - Batch 80/88 - Loss: 1.8987\n",
            "Model 3 - Finished Epoch 9\n",
            "Model 3 - Starting Epoch 10\n",
            "Model 3 - Epoch 10 - Batch 10/88 - Loss: 1.8421\n",
            "Model 3 - Epoch 10 - Batch 20/88 - Loss: 1.7977\n",
            "Model 3 - Epoch 10 - Batch 30/88 - Loss: 1.8601\n",
            "Model 3 - Epoch 10 - Batch 40/88 - Loss: 1.7662\n",
            "Model 3 - Epoch 10 - Batch 50/88 - Loss: 1.9978\n",
            "Model 3 - Epoch 10 - Batch 60/88 - Loss: 1.7946\n",
            "Model 3 - Epoch 10 - Batch 70/88 - Loss: 1.8975\n",
            "Model 3 - Epoch 10 - Batch 80/88 - Loss: 1.8071\n",
            "Model 3 - Finished Epoch 10\n",
            "Model 3 done training for 10 epochs in generation 8\n",
            "Model 4 - Starting Epoch 1\n",
            "Model 4 - Epoch 1 - Batch 10/88 - Loss: 1.8403\n",
            "Model 4 - Epoch 1 - Batch 20/88 - Loss: 1.8479\n",
            "Model 4 - Epoch 1 - Batch 30/88 - Loss: 1.8034\n",
            "Model 4 - Epoch 1 - Batch 40/88 - Loss: 1.8912\n",
            "Model 4 - Epoch 1 - Batch 50/88 - Loss: 1.7792\n",
            "Model 4 - Epoch 1 - Batch 60/88 - Loss: 1.7880\n",
            "Model 4 - Epoch 1 - Batch 70/88 - Loss: 1.9739\n",
            "Model 4 - Epoch 1 - Batch 80/88 - Loss: 1.8117\n",
            "Model 4 - Finished Epoch 1\n",
            "Model 4 - Starting Epoch 2\n",
            "Model 4 - Epoch 2 - Batch 10/88 - Loss: 1.9804\n",
            "Model 4 - Epoch 2 - Batch 20/88 - Loss: 1.8835\n",
            "Model 4 - Epoch 2 - Batch 30/88 - Loss: 1.8156\n",
            "Model 4 - Epoch 2 - Batch 40/88 - Loss: 1.8452\n",
            "Model 4 - Epoch 2 - Batch 50/88 - Loss: 1.8926\n",
            "Model 4 - Epoch 2 - Batch 60/88 - Loss: 1.8686\n",
            "Model 4 - Epoch 2 - Batch 70/88 - Loss: 2.0130\n",
            "Model 4 - Epoch 2 - Batch 80/88 - Loss: 1.7405\n",
            "Model 4 - Finished Epoch 2\n",
            "Model 4 - Starting Epoch 3\n",
            "Model 4 - Epoch 3 - Batch 10/88 - Loss: 1.7695\n",
            "Model 4 - Epoch 3 - Batch 20/88 - Loss: 1.8027\n",
            "Model 4 - Epoch 3 - Batch 30/88 - Loss: 1.8973\n",
            "Model 4 - Epoch 3 - Batch 40/88 - Loss: 1.9428\n",
            "Model 4 - Epoch 3 - Batch 50/88 - Loss: 1.9146\n",
            "Model 4 - Epoch 3 - Batch 60/88 - Loss: 1.8630\n",
            "Model 4 - Epoch 3 - Batch 70/88 - Loss: 1.8860\n",
            "Model 4 - Epoch 3 - Batch 80/88 - Loss: 2.0041\n",
            "Model 4 - Finished Epoch 3\n",
            "Model 4 - Starting Epoch 4\n",
            "Model 4 - Epoch 4 - Batch 10/88 - Loss: 1.8775\n",
            "Model 4 - Epoch 4 - Batch 20/88 - Loss: 1.7950\n",
            "Model 4 - Epoch 4 - Batch 30/88 - Loss: 1.8924\n",
            "Model 4 - Epoch 4 - Batch 40/88 - Loss: 1.8529\n",
            "Model 4 - Epoch 4 - Batch 50/88 - Loss: 1.8502\n",
            "Model 4 - Epoch 4 - Batch 60/88 - Loss: 1.9208\n",
            "Model 4 - Epoch 4 - Batch 70/88 - Loss: 1.8434\n",
            "Model 4 - Epoch 4 - Batch 80/88 - Loss: 1.9469\n",
            "Model 4 - Finished Epoch 4\n",
            "Model 4 - Starting Epoch 5\n",
            "Model 4 - Epoch 5 - Batch 10/88 - Loss: 1.9020\n",
            "Model 4 - Epoch 5 - Batch 20/88 - Loss: 1.8388\n",
            "Model 4 - Epoch 5 - Batch 30/88 - Loss: 1.8620\n",
            "Model 4 - Epoch 5 - Batch 40/88 - Loss: 1.9442\n",
            "Model 4 - Epoch 5 - Batch 50/88 - Loss: 1.8447\n",
            "Model 4 - Epoch 5 - Batch 60/88 - Loss: 1.8495\n",
            "Model 4 - Epoch 5 - Batch 70/88 - Loss: 1.8215\n",
            "Model 4 - Epoch 5 - Batch 80/88 - Loss: 1.8554\n",
            "Model 4 - Finished Epoch 5\n",
            "Model 4 - Starting Epoch 6\n",
            "Model 4 - Epoch 6 - Batch 10/88 - Loss: 1.8326\n",
            "Model 4 - Epoch 6 - Batch 20/88 - Loss: 1.7925\n",
            "Model 4 - Epoch 6 - Batch 30/88 - Loss: 1.8366\n",
            "Model 4 - Epoch 6 - Batch 40/88 - Loss: 1.7708\n",
            "Model 4 - Epoch 6 - Batch 50/88 - Loss: 1.8615\n",
            "Model 4 - Epoch 6 - Batch 60/88 - Loss: 1.9050\n",
            "Model 4 - Epoch 6 - Batch 70/88 - Loss: 1.8190\n",
            "Model 4 - Epoch 6 - Batch 80/88 - Loss: 1.7380\n",
            "Model 4 - Finished Epoch 6\n",
            "Model 4 - Starting Epoch 7\n",
            "Model 4 - Epoch 7 - Batch 10/88 - Loss: 1.7485\n",
            "Model 4 - Epoch 7 - Batch 20/88 - Loss: 1.9866\n",
            "Model 4 - Epoch 7 - Batch 30/88 - Loss: 1.8139\n",
            "Model 4 - Epoch 7 - Batch 40/88 - Loss: 1.8430\n",
            "Model 4 - Epoch 7 - Batch 50/88 - Loss: 1.8498\n",
            "Model 4 - Epoch 7 - Batch 60/88 - Loss: 1.7407\n",
            "Model 4 - Epoch 7 - Batch 70/88 - Loss: 1.8785\n",
            "Model 4 - Epoch 7 - Batch 80/88 - Loss: 1.7183\n",
            "Model 4 - Finished Epoch 7\n",
            "Model 4 - Starting Epoch 8\n",
            "Model 4 - Epoch 8 - Batch 10/88 - Loss: 1.8322\n",
            "Model 4 - Epoch 8 - Batch 20/88 - Loss: 1.7113\n",
            "Model 4 - Epoch 8 - Batch 30/88 - Loss: 1.7832\n",
            "Model 4 - Epoch 8 - Batch 40/88 - Loss: 1.7524\n",
            "Model 4 - Epoch 8 - Batch 50/88 - Loss: 1.8052\n",
            "Model 4 - Epoch 8 - Batch 60/88 - Loss: 1.9432\n",
            "Model 4 - Epoch 8 - Batch 70/88 - Loss: 1.8107\n",
            "Model 4 - Epoch 8 - Batch 80/88 - Loss: 1.9098\n",
            "Model 4 - Finished Epoch 8\n",
            "Model 4 - Starting Epoch 9\n",
            "Model 4 - Epoch 9 - Batch 10/88 - Loss: 1.7952\n",
            "Model 4 - Epoch 9 - Batch 20/88 - Loss: 1.8486\n",
            "Model 4 - Epoch 9 - Batch 30/88 - Loss: 1.8316\n",
            "Model 4 - Epoch 9 - Batch 40/88 - Loss: 1.8006\n",
            "Model 4 - Epoch 9 - Batch 50/88 - Loss: 1.7753\n",
            "Model 4 - Epoch 9 - Batch 60/88 - Loss: 1.7898\n",
            "Model 4 - Epoch 9 - Batch 70/88 - Loss: 1.8427\n",
            "Model 4 - Epoch 9 - Batch 80/88 - Loss: 1.9044\n",
            "Model 4 - Finished Epoch 9\n",
            "Model 4 - Starting Epoch 10\n",
            "Model 4 - Epoch 10 - Batch 10/88 - Loss: 1.8671\n",
            "Model 4 - Epoch 10 - Batch 20/88 - Loss: 1.7025\n",
            "Model 4 - Epoch 10 - Batch 30/88 - Loss: 1.7177\n",
            "Model 4 - Epoch 10 - Batch 40/88 - Loss: 1.8433\n",
            "Model 4 - Epoch 10 - Batch 50/88 - Loss: 1.8328\n",
            "Model 4 - Epoch 10 - Batch 60/88 - Loss: 1.8270\n",
            "Model 4 - Epoch 10 - Batch 70/88 - Loss: 1.9808\n",
            "Model 4 - Epoch 10 - Batch 80/88 - Loss: 1.9409\n",
            "Model 4 - Finished Epoch 10\n",
            "Model 4 done training for 10 epochs in generation 8\n",
            "Model 5 - Starting Epoch 1\n",
            "Model 5 - Epoch 1 - Batch 10/88 - Loss: 1.6926\n",
            "Model 5 - Epoch 1 - Batch 20/88 - Loss: 1.6148\n",
            "Model 5 - Epoch 1 - Batch 30/88 - Loss: 1.6351\n",
            "Model 5 - Epoch 1 - Batch 40/88 - Loss: 1.7127\n",
            "Model 5 - Epoch 1 - Batch 50/88 - Loss: 1.6803\n",
            "Model 5 - Epoch 1 - Batch 60/88 - Loss: 1.7004\n",
            "Model 5 - Epoch 1 - Batch 70/88 - Loss: 1.7189\n",
            "Model 5 - Epoch 1 - Batch 80/88 - Loss: 1.6746\n",
            "Model 5 - Finished Epoch 1\n",
            "Model 5 - Starting Epoch 2\n",
            "Model 5 - Epoch 2 - Batch 10/88 - Loss: 1.7163\n",
            "Model 5 - Epoch 2 - Batch 20/88 - Loss: 1.6738\n",
            "Model 5 - Epoch 2 - Batch 30/88 - Loss: 1.6114\n",
            "Model 5 - Epoch 2 - Batch 40/88 - Loss: 1.7068\n",
            "Model 5 - Epoch 2 - Batch 50/88 - Loss: 1.8260\n",
            "Model 5 - Epoch 2 - Batch 60/88 - Loss: 1.7412\n",
            "Model 5 - Epoch 2 - Batch 70/88 - Loss: 1.6572\n",
            "Model 5 - Epoch 2 - Batch 80/88 - Loss: 1.5779\n",
            "Model 5 - Finished Epoch 2\n",
            "Model 5 - Starting Epoch 3\n",
            "Model 5 - Epoch 3 - Batch 10/88 - Loss: 1.6299\n",
            "Model 5 - Epoch 3 - Batch 20/88 - Loss: 1.6330\n",
            "Model 5 - Epoch 3 - Batch 30/88 - Loss: 1.6851\n",
            "Model 5 - Epoch 3 - Batch 40/88 - Loss: 1.6681\n",
            "Model 5 - Epoch 3 - Batch 50/88 - Loss: 1.7220\n",
            "Model 5 - Epoch 3 - Batch 60/88 - Loss: 1.7477\n",
            "Model 5 - Epoch 3 - Batch 70/88 - Loss: 1.7374\n",
            "Model 5 - Epoch 3 - Batch 80/88 - Loss: 1.7064\n",
            "Model 5 - Finished Epoch 3\n",
            "Model 5 - Starting Epoch 4\n",
            "Model 5 - Epoch 4 - Batch 10/88 - Loss: 1.6713\n",
            "Model 5 - Epoch 4 - Batch 20/88 - Loss: 1.6152\n",
            "Model 5 - Epoch 4 - Batch 30/88 - Loss: 1.7775\n",
            "Model 5 - Epoch 4 - Batch 40/88 - Loss: 1.6248\n",
            "Model 5 - Epoch 4 - Batch 50/88 - Loss: 1.7381\n",
            "Model 5 - Epoch 4 - Batch 60/88 - Loss: 1.6041\n",
            "Model 5 - Epoch 4 - Batch 70/88 - Loss: 1.8014\n",
            "Model 5 - Epoch 4 - Batch 80/88 - Loss: 1.7045\n",
            "Model 5 - Finished Epoch 4\n",
            "Model 5 - Starting Epoch 5\n",
            "Model 5 - Epoch 5 - Batch 10/88 - Loss: 1.6979\n",
            "Model 5 - Epoch 5 - Batch 20/88 - Loss: 1.7280\n",
            "Model 5 - Epoch 5 - Batch 30/88 - Loss: 1.6262\n",
            "Model 5 - Epoch 5 - Batch 40/88 - Loss: 1.7226\n",
            "Model 5 - Epoch 5 - Batch 50/88 - Loss: 1.7539\n",
            "Model 5 - Epoch 5 - Batch 60/88 - Loss: 1.6861\n",
            "Model 5 - Epoch 5 - Batch 70/88 - Loss: 1.8183\n",
            "Model 5 - Epoch 5 - Batch 80/88 - Loss: 1.6258\n",
            "Model 5 - Finished Epoch 5\n",
            "Model 5 - Starting Epoch 6\n",
            "Model 5 - Epoch 6 - Batch 10/88 - Loss: 1.6848\n",
            "Model 5 - Epoch 6 - Batch 20/88 - Loss: 1.6741\n",
            "Model 5 - Epoch 6 - Batch 30/88 - Loss: 1.6819\n",
            "Model 5 - Epoch 6 - Batch 40/88 - Loss: 1.6226\n",
            "Model 5 - Epoch 6 - Batch 50/88 - Loss: 1.7492\n",
            "Model 5 - Epoch 6 - Batch 60/88 - Loss: 1.6382\n",
            "Model 5 - Epoch 6 - Batch 70/88 - Loss: 1.6997\n",
            "Model 5 - Epoch 6 - Batch 80/88 - Loss: 1.6582\n",
            "Model 5 - Finished Epoch 6\n",
            "Model 5 - Starting Epoch 7\n",
            "Model 5 - Epoch 7 - Batch 10/88 - Loss: 1.6896\n",
            "Model 5 - Epoch 7 - Batch 20/88 - Loss: 1.5405\n",
            "Model 5 - Epoch 7 - Batch 30/88 - Loss: 1.6832\n",
            "Model 5 - Epoch 7 - Batch 40/88 - Loss: 1.5897\n",
            "Model 5 - Epoch 7 - Batch 50/88 - Loss: 1.7295\n",
            "Model 5 - Epoch 7 - Batch 60/88 - Loss: 1.6984\n",
            "Model 5 - Epoch 7 - Batch 70/88 - Loss: 1.7143\n",
            "Model 5 - Epoch 7 - Batch 80/88 - Loss: 1.6834\n",
            "Model 5 - Finished Epoch 7\n",
            "Model 5 - Starting Epoch 8\n",
            "Model 5 - Epoch 8 - Batch 10/88 - Loss: 1.6990\n",
            "Model 5 - Epoch 8 - Batch 20/88 - Loss: 1.5961\n",
            "Model 5 - Epoch 8 - Batch 30/88 - Loss: 1.6011\n",
            "Model 5 - Epoch 8 - Batch 40/88 - Loss: 1.5923\n",
            "Model 5 - Epoch 8 - Batch 50/88 - Loss: 1.7093\n",
            "Model 5 - Epoch 8 - Batch 60/88 - Loss: 1.6874\n",
            "Model 5 - Epoch 8 - Batch 70/88 - Loss: 1.7519\n",
            "Model 5 - Epoch 8 - Batch 80/88 - Loss: 1.8157\n",
            "Model 5 - Finished Epoch 8\n",
            "Model 5 - Starting Epoch 9\n",
            "Model 5 - Epoch 9 - Batch 10/88 - Loss: 1.6413\n",
            "Model 5 - Epoch 9 - Batch 20/88 - Loss: 1.5471\n",
            "Model 5 - Epoch 9 - Batch 30/88 - Loss: 1.6218\n",
            "Model 5 - Epoch 9 - Batch 40/88 - Loss: 1.6349\n",
            "Model 5 - Epoch 9 - Batch 50/88 - Loss: 1.7437\n",
            "Model 5 - Epoch 9 - Batch 60/88 - Loss: 1.7390\n",
            "Model 5 - Epoch 9 - Batch 70/88 - Loss: 1.7579\n",
            "Model 5 - Epoch 9 - Batch 80/88 - Loss: 1.5730\n",
            "Model 5 - Finished Epoch 9\n",
            "Model 5 - Starting Epoch 10\n",
            "Model 5 - Epoch 10 - Batch 10/88 - Loss: 1.5812\n",
            "Model 5 - Epoch 10 - Batch 20/88 - Loss: 1.5644\n",
            "Model 5 - Epoch 10 - Batch 30/88 - Loss: 1.6256\n",
            "Model 5 - Epoch 10 - Batch 40/88 - Loss: 1.6406\n",
            "Model 5 - Epoch 10 - Batch 50/88 - Loss: 1.6681\n",
            "Model 5 - Epoch 10 - Batch 60/88 - Loss: 1.5796\n",
            "Model 5 - Epoch 10 - Batch 70/88 - Loss: 1.7651\n",
            "Model 5 - Epoch 10 - Batch 80/88 - Loss: 1.6313\n",
            "Model 5 - Finished Epoch 10\n",
            "Model 5 done training for 10 epochs in generation 8\n",
            "  🎉 New best model with accuracy: 0.6861\n",
            "  ✅ Best Acc This Gen: 0.6861 | Worst: 0.6361\n",
            "\n",
            "🌱 Generation 9 | Population size: 3\n",
            "Model 1 - Starting Epoch 1\n",
            "Model 1 - Epoch 1 - Batch 10/88 - Loss: 1.4136\n",
            "Model 1 - Epoch 1 - Batch 20/88 - Loss: 1.5047\n",
            "Model 1 - Epoch 1 - Batch 30/88 - Loss: 1.4587\n",
            "Model 1 - Epoch 1 - Batch 40/88 - Loss: 1.4956\n",
            "Model 1 - Epoch 1 - Batch 50/88 - Loss: 1.5834\n",
            "Model 1 - Epoch 1 - Batch 60/88 - Loss: 1.4714\n",
            "Model 1 - Epoch 1 - Batch 70/88 - Loss: 1.4914\n",
            "Model 1 - Epoch 1 - Batch 80/88 - Loss: 1.4907\n",
            "Model 1 - Finished Epoch 1\n",
            "Model 1 - Starting Epoch 2\n",
            "Model 1 - Epoch 2 - Batch 10/88 - Loss: 1.3505\n",
            "Model 1 - Epoch 2 - Batch 20/88 - Loss: 1.4607\n",
            "Model 1 - Epoch 2 - Batch 30/88 - Loss: 1.4996\n",
            "Model 1 - Epoch 2 - Batch 40/88 - Loss: 1.4365\n",
            "Model 1 - Epoch 2 - Batch 50/88 - Loss: 1.4274\n",
            "Model 1 - Epoch 2 - Batch 60/88 - Loss: 1.4563\n",
            "Model 1 - Epoch 2 - Batch 70/88 - Loss: 1.5051\n",
            "Model 1 - Epoch 2 - Batch 80/88 - Loss: 1.4639\n",
            "Model 1 - Finished Epoch 2\n",
            "Model 1 - Starting Epoch 3\n",
            "Model 1 - Epoch 3 - Batch 10/88 - Loss: 1.4377\n",
            "Model 1 - Epoch 3 - Batch 20/88 - Loss: 1.3785\n",
            "Model 1 - Epoch 3 - Batch 30/88 - Loss: 1.4866\n",
            "Model 1 - Epoch 3 - Batch 40/88 - Loss: 1.4777\n",
            "Model 1 - Epoch 3 - Batch 50/88 - Loss: 1.4953\n",
            "Model 1 - Epoch 3 - Batch 60/88 - Loss: 1.4000\n",
            "Model 1 - Epoch 3 - Batch 70/88 - Loss: 1.4933\n",
            "Model 1 - Epoch 3 - Batch 80/88 - Loss: 1.4684\n",
            "Model 1 - Finished Epoch 3\n",
            "Model 1 - Starting Epoch 4\n",
            "Model 1 - Epoch 4 - Batch 10/88 - Loss: 1.4934\n",
            "Model 1 - Epoch 4 - Batch 20/88 - Loss: 1.4506\n",
            "Model 1 - Epoch 4 - Batch 30/88 - Loss: 1.4093\n",
            "Model 1 - Epoch 4 - Batch 40/88 - Loss: 1.4240\n",
            "Model 1 - Epoch 4 - Batch 50/88 - Loss: 1.3666\n",
            "Model 1 - Epoch 4 - Batch 60/88 - Loss: 1.5317\n",
            "Model 1 - Epoch 4 - Batch 70/88 - Loss: 1.4793\n",
            "Model 1 - Epoch 4 - Batch 80/88 - Loss: 1.5477\n",
            "Model 1 - Finished Epoch 4\n",
            "Model 1 - Starting Epoch 5\n",
            "Model 1 - Epoch 5 - Batch 10/88 - Loss: 1.5112\n",
            "Model 1 - Epoch 5 - Batch 20/88 - Loss: 1.3823\n",
            "Model 1 - Epoch 5 - Batch 30/88 - Loss: 1.4325\n",
            "Model 1 - Epoch 5 - Batch 40/88 - Loss: 1.3209\n",
            "Model 1 - Epoch 5 - Batch 50/88 - Loss: 1.4839\n",
            "Model 1 - Epoch 5 - Batch 60/88 - Loss: 1.4057\n",
            "Model 1 - Epoch 5 - Batch 70/88 - Loss: 1.4461\n",
            "Model 1 - Epoch 5 - Batch 80/88 - Loss: 1.3434\n",
            "Model 1 - Finished Epoch 5\n",
            "Model 1 - Starting Epoch 6\n",
            "Model 1 - Epoch 6 - Batch 10/88 - Loss: 1.4181\n",
            "Model 1 - Epoch 6 - Batch 20/88 - Loss: 1.3715\n",
            "Model 1 - Epoch 6 - Batch 30/88 - Loss: 1.3942\n",
            "Model 1 - Epoch 6 - Batch 40/88 - Loss: 1.3613\n",
            "Model 1 - Epoch 6 - Batch 50/88 - Loss: 1.3964\n",
            "Model 1 - Epoch 6 - Batch 60/88 - Loss: 1.4564\n",
            "Model 1 - Epoch 6 - Batch 70/88 - Loss: 1.5023\n",
            "Model 1 - Epoch 6 - Batch 80/88 - Loss: 1.4381\n",
            "Model 1 - Finished Epoch 6\n",
            "Model 1 - Starting Epoch 7\n",
            "Model 1 - Epoch 7 - Batch 10/88 - Loss: 1.4329\n",
            "Model 1 - Epoch 7 - Batch 20/88 - Loss: 1.4107\n",
            "Model 1 - Epoch 7 - Batch 30/88 - Loss: 1.3049\n",
            "Model 1 - Epoch 7 - Batch 40/88 - Loss: 1.3943\n",
            "Model 1 - Epoch 7 - Batch 50/88 - Loss: 1.4711\n",
            "Model 1 - Epoch 7 - Batch 60/88 - Loss: 1.4549\n",
            "Model 1 - Epoch 7 - Batch 70/88 - Loss: 1.4632\n",
            "Model 1 - Epoch 7 - Batch 80/88 - Loss: 1.4093\n",
            "Model 1 - Finished Epoch 7\n",
            "Model 1 - Starting Epoch 8\n",
            "Model 1 - Epoch 8 - Batch 10/88 - Loss: 1.4680\n",
            "Model 1 - Epoch 8 - Batch 20/88 - Loss: 1.5004\n",
            "Model 1 - Epoch 8 - Batch 30/88 - Loss: 1.4390\n",
            "Model 1 - Epoch 8 - Batch 40/88 - Loss: 1.3942\n",
            "Model 1 - Epoch 8 - Batch 50/88 - Loss: 1.4358\n",
            "Model 1 - Epoch 8 - Batch 60/88 - Loss: 1.4464\n",
            "Model 1 - Epoch 8 - Batch 70/88 - Loss: 1.4290\n",
            "Model 1 - Epoch 8 - Batch 80/88 - Loss: 1.3685\n",
            "Model 1 - Finished Epoch 8\n",
            "Model 1 - Starting Epoch 9\n",
            "Model 1 - Epoch 9 - Batch 10/88 - Loss: 1.3301\n",
            "Model 1 - Epoch 9 - Batch 20/88 - Loss: 1.3505\n",
            "Model 1 - Epoch 9 - Batch 30/88 - Loss: 1.4224\n",
            "Model 1 - Epoch 9 - Batch 40/88 - Loss: 1.5036\n",
            "Model 1 - Epoch 9 - Batch 50/88 - Loss: 1.4723\n",
            "Model 1 - Epoch 9 - Batch 60/88 - Loss: 1.5276\n",
            "Model 1 - Epoch 9 - Batch 70/88 - Loss: 1.3398\n",
            "Model 1 - Epoch 9 - Batch 80/88 - Loss: 1.5198\n",
            "Model 1 - Finished Epoch 9\n",
            "Model 1 - Starting Epoch 10\n",
            "Model 1 - Epoch 10 - Batch 10/88 - Loss: 1.4382\n",
            "Model 1 - Epoch 10 - Batch 20/88 - Loss: 1.2721\n",
            "Model 1 - Epoch 10 - Batch 30/88 - Loss: 1.3888\n",
            "Model 1 - Epoch 10 - Batch 40/88 - Loss: 1.3452\n",
            "Model 1 - Epoch 10 - Batch 50/88 - Loss: 1.4345\n",
            "Model 1 - Epoch 10 - Batch 60/88 - Loss: 1.4814\n",
            "Model 1 - Epoch 10 - Batch 70/88 - Loss: 1.4151\n",
            "Model 1 - Epoch 10 - Batch 80/88 - Loss: 1.4566\n",
            "Model 1 - Finished Epoch 10\n",
            "Model 1 done training for 10 epochs in generation 9\n",
            "Model 2 - Starting Epoch 1\n",
            "Model 2 - Epoch 1 - Batch 10/88 - Loss: 1.7252\n",
            "Model 2 - Epoch 1 - Batch 20/88 - Loss: 1.6651\n",
            "Model 2 - Epoch 1 - Batch 30/88 - Loss: 1.7738\n",
            "Model 2 - Epoch 1 - Batch 40/88 - Loss: 1.7239\n",
            "Model 2 - Epoch 1 - Batch 50/88 - Loss: 1.7269\n",
            "Model 2 - Epoch 1 - Batch 60/88 - Loss: 1.7912\n",
            "Model 2 - Epoch 1 - Batch 70/88 - Loss: 1.7146\n",
            "Model 2 - Epoch 1 - Batch 80/88 - Loss: 1.8210\n",
            "Model 2 - Finished Epoch 1\n",
            "Model 2 - Starting Epoch 2\n",
            "Model 2 - Epoch 2 - Batch 10/88 - Loss: 1.8442\n",
            "Model 2 - Epoch 2 - Batch 20/88 - Loss: 1.7098\n",
            "Model 2 - Epoch 2 - Batch 30/88 - Loss: 1.7276\n",
            "Model 2 - Epoch 2 - Batch 40/88 - Loss: 1.6476\n",
            "Model 2 - Epoch 2 - Batch 50/88 - Loss: 1.7629\n",
            "Model 2 - Epoch 2 - Batch 60/88 - Loss: 1.7044\n",
            "Model 2 - Epoch 2 - Batch 70/88 - Loss: 1.7876\n",
            "Model 2 - Epoch 2 - Batch 80/88 - Loss: 1.7070\n",
            "Model 2 - Finished Epoch 2\n",
            "Model 2 - Starting Epoch 3\n",
            "Model 2 - Epoch 3 - Batch 10/88 - Loss: 1.6924\n",
            "Model 2 - Epoch 3 - Batch 20/88 - Loss: 1.6781\n",
            "Model 2 - Epoch 3 - Batch 30/88 - Loss: 1.6536\n",
            "Model 2 - Epoch 3 - Batch 40/88 - Loss: 1.6891\n",
            "Model 2 - Epoch 3 - Batch 50/88 - Loss: 1.7250\n",
            "Model 2 - Epoch 3 - Batch 60/88 - Loss: 1.7623\n",
            "Model 2 - Epoch 3 - Batch 70/88 - Loss: 1.7408\n",
            "Model 2 - Epoch 3 - Batch 80/88 - Loss: 1.7675\n",
            "Model 2 - Finished Epoch 3\n",
            "Model 2 - Starting Epoch 4\n",
            "Model 2 - Epoch 4 - Batch 10/88 - Loss: 1.6903\n",
            "Model 2 - Epoch 4 - Batch 20/88 - Loss: 1.7300\n",
            "Model 2 - Epoch 4 - Batch 30/88 - Loss: 1.7048\n",
            "Model 2 - Epoch 4 - Batch 40/88 - Loss: 1.6894\n",
            "Model 2 - Epoch 4 - Batch 50/88 - Loss: 1.7404\n",
            "Model 2 - Epoch 4 - Batch 60/88 - Loss: 1.7063\n",
            "Model 2 - Epoch 4 - Batch 70/88 - Loss: 1.7501\n",
            "Model 2 - Epoch 4 - Batch 80/88 - Loss: 1.5924\n",
            "Model 2 - Finished Epoch 4\n",
            "Model 2 - Starting Epoch 5\n",
            "Model 2 - Epoch 5 - Batch 10/88 - Loss: 1.7267\n",
            "Model 2 - Epoch 5 - Batch 20/88 - Loss: 1.7476\n",
            "Model 2 - Epoch 5 - Batch 30/88 - Loss: 1.7844\n",
            "Model 2 - Epoch 5 - Batch 40/88 - Loss: 1.7330\n",
            "Model 2 - Epoch 5 - Batch 50/88 - Loss: 1.7147\n",
            "Model 2 - Epoch 5 - Batch 60/88 - Loss: 1.6186\n",
            "Model 2 - Epoch 5 - Batch 70/88 - Loss: 1.7520\n",
            "Model 2 - Epoch 5 - Batch 80/88 - Loss: 1.7899\n",
            "Model 2 - Finished Epoch 5\n",
            "Model 2 - Starting Epoch 6\n",
            "Model 2 - Epoch 6 - Batch 10/88 - Loss: 1.7104\n",
            "Model 2 - Epoch 6 - Batch 20/88 - Loss: 1.7172\n",
            "Model 2 - Epoch 6 - Batch 30/88 - Loss: 1.6265\n",
            "Model 2 - Epoch 6 - Batch 40/88 - Loss: 1.6735\n",
            "Model 2 - Epoch 6 - Batch 50/88 - Loss: 1.7505\n",
            "Model 2 - Epoch 6 - Batch 60/88 - Loss: 1.7648\n",
            "Model 2 - Epoch 6 - Batch 70/88 - Loss: 1.7049\n",
            "Model 2 - Epoch 6 - Batch 80/88 - Loss: 1.7253\n",
            "Model 2 - Finished Epoch 6\n",
            "Model 2 - Starting Epoch 7\n",
            "Model 2 - Epoch 7 - Batch 10/88 - Loss: 1.6780\n",
            "Model 2 - Epoch 7 - Batch 20/88 - Loss: 1.7238\n",
            "Model 2 - Epoch 7 - Batch 30/88 - Loss: 1.6778\n",
            "Model 2 - Epoch 7 - Batch 40/88 - Loss: 1.6539\n",
            "Model 2 - Epoch 7 - Batch 50/88 - Loss: 1.7718\n",
            "Model 2 - Epoch 7 - Batch 60/88 - Loss: 1.5504\n",
            "Model 2 - Epoch 7 - Batch 70/88 - Loss: 1.7431\n",
            "Model 2 - Epoch 7 - Batch 80/88 - Loss: 1.7280\n",
            "Model 2 - Finished Epoch 7\n",
            "Model 2 - Starting Epoch 8\n",
            "Model 2 - Epoch 8 - Batch 10/88 - Loss: 1.6613\n",
            "Model 2 - Epoch 8 - Batch 20/88 - Loss: 1.7953\n",
            "Model 2 - Epoch 8 - Batch 30/88 - Loss: 1.6600\n",
            "Model 2 - Epoch 8 - Batch 40/88 - Loss: 1.6399\n",
            "Model 2 - Epoch 8 - Batch 50/88 - Loss: 1.5917\n",
            "Model 2 - Epoch 8 - Batch 60/88 - Loss: 1.7589\n",
            "Model 2 - Epoch 8 - Batch 70/88 - Loss: 1.5790\n",
            "Model 2 - Epoch 8 - Batch 80/88 - Loss: 1.7047\n",
            "Model 2 - Finished Epoch 8\n",
            "Model 2 - Starting Epoch 9\n",
            "Model 2 - Epoch 9 - Batch 10/88 - Loss: 1.6236\n",
            "Model 2 - Epoch 9 - Batch 20/88 - Loss: 1.8107\n",
            "Model 2 - Epoch 9 - Batch 30/88 - Loss: 1.6670\n",
            "Model 2 - Epoch 9 - Batch 40/88 - Loss: 1.7323\n",
            "Model 2 - Epoch 9 - Batch 50/88 - Loss: 1.6671\n",
            "Model 2 - Epoch 9 - Batch 60/88 - Loss: 1.7317\n",
            "Model 2 - Epoch 9 - Batch 70/88 - Loss: 1.7437\n",
            "Model 2 - Epoch 9 - Batch 80/88 - Loss: 1.7552\n",
            "Model 2 - Finished Epoch 9\n",
            "Model 2 - Starting Epoch 10\n",
            "Model 2 - Epoch 10 - Batch 10/88 - Loss: 1.7016\n",
            "Model 2 - Epoch 10 - Batch 20/88 - Loss: 1.6373\n",
            "Model 2 - Epoch 10 - Batch 30/88 - Loss: 1.7490\n",
            "Model 2 - Epoch 10 - Batch 40/88 - Loss: 1.6803\n",
            "Model 2 - Epoch 10 - Batch 50/88 - Loss: 1.6246\n",
            "Model 2 - Epoch 10 - Batch 60/88 - Loss: 1.6770\n",
            "Model 2 - Epoch 10 - Batch 70/88 - Loss: 1.7970\n",
            "Model 2 - Epoch 10 - Batch 80/88 - Loss: 1.6235\n",
            "Model 2 - Finished Epoch 10\n",
            "Model 2 done training for 10 epochs in generation 9\n",
            "Model 3 - Starting Epoch 1\n",
            "Model 3 - Epoch 1 - Batch 10/88 - Loss: 1.8371\n",
            "Model 3 - Epoch 1 - Batch 20/88 - Loss: 1.7453\n",
            "Model 3 - Epoch 1 - Batch 30/88 - Loss: 1.7927\n",
            "Model 3 - Epoch 1 - Batch 40/88 - Loss: 1.7919\n",
            "Model 3 - Epoch 1 - Batch 50/88 - Loss: 1.7791\n",
            "Model 3 - Epoch 1 - Batch 60/88 - Loss: 1.9662\n",
            "Model 3 - Epoch 1 - Batch 70/88 - Loss: 1.7694\n",
            "Model 3 - Epoch 1 - Batch 80/88 - Loss: 1.8340\n",
            "Model 3 - Finished Epoch 1\n",
            "Model 3 - Starting Epoch 2\n",
            "Model 3 - Epoch 2 - Batch 10/88 - Loss: 1.7425\n",
            "Model 3 - Epoch 2 - Batch 20/88 - Loss: 1.8429\n",
            "Model 3 - Epoch 2 - Batch 30/88 - Loss: 1.9193\n",
            "Model 3 - Epoch 2 - Batch 40/88 - Loss: 1.8080\n",
            "Model 3 - Epoch 2 - Batch 50/88 - Loss: 1.7172\n",
            "Model 3 - Epoch 2 - Batch 60/88 - Loss: 1.7909\n",
            "Model 3 - Epoch 2 - Batch 70/88 - Loss: 1.8091\n",
            "Model 3 - Epoch 2 - Batch 80/88 - Loss: 1.7972\n",
            "Model 3 - Finished Epoch 2\n",
            "Model 3 - Starting Epoch 3\n",
            "Model 3 - Epoch 3 - Batch 10/88 - Loss: 1.8612\n",
            "Model 3 - Epoch 3 - Batch 20/88 - Loss: 1.7544\n",
            "Model 3 - Epoch 3 - Batch 30/88 - Loss: 1.8036\n",
            "Model 3 - Epoch 3 - Batch 40/88 - Loss: 1.8545\n",
            "Model 3 - Epoch 3 - Batch 50/88 - Loss: 1.6942\n",
            "Model 3 - Epoch 3 - Batch 60/88 - Loss: 1.8244\n",
            "Model 3 - Epoch 3 - Batch 70/88 - Loss: 1.7885\n",
            "Model 3 - Epoch 3 - Batch 80/88 - Loss: 1.9107\n",
            "Model 3 - Finished Epoch 3\n",
            "Model 3 - Starting Epoch 4\n",
            "Model 3 - Epoch 4 - Batch 10/88 - Loss: 1.7939\n",
            "Model 3 - Epoch 4 - Batch 20/88 - Loss: 1.7858\n",
            "Model 3 - Epoch 4 - Batch 30/88 - Loss: 1.6374\n",
            "Model 3 - Epoch 4 - Batch 40/88 - Loss: 1.8644\n",
            "Model 3 - Epoch 4 - Batch 50/88 - Loss: 1.7605\n",
            "Model 3 - Epoch 4 - Batch 60/88 - Loss: 1.8709\n",
            "Model 3 - Epoch 4 - Batch 70/88 - Loss: 1.7178\n",
            "Model 3 - Epoch 4 - Batch 80/88 - Loss: 1.8366\n",
            "Model 3 - Finished Epoch 4\n",
            "Model 3 - Starting Epoch 5\n",
            "Model 3 - Epoch 5 - Batch 10/88 - Loss: 1.7593\n",
            "Model 3 - Epoch 5 - Batch 20/88 - Loss: 1.7147\n",
            "Model 3 - Epoch 5 - Batch 30/88 - Loss: 1.8365\n",
            "Model 3 - Epoch 5 - Batch 40/88 - Loss: 1.7156\n",
            "Model 3 - Epoch 5 - Batch 50/88 - Loss: 1.7539\n",
            "Model 3 - Epoch 5 - Batch 60/88 - Loss: 1.8217\n",
            "Model 3 - Epoch 5 - Batch 70/88 - Loss: 1.7993\n",
            "Model 3 - Epoch 5 - Batch 80/88 - Loss: 1.8246\n",
            "Model 3 - Finished Epoch 5\n",
            "Model 3 - Starting Epoch 6\n",
            "Model 3 - Epoch 6 - Batch 10/88 - Loss: 1.8164\n",
            "Model 3 - Epoch 6 - Batch 20/88 - Loss: 1.7897\n",
            "Model 3 - Epoch 6 - Batch 30/88 - Loss: 1.8138\n",
            "Model 3 - Epoch 6 - Batch 40/88 - Loss: 1.7524\n",
            "Model 3 - Epoch 6 - Batch 50/88 - Loss: 1.6525\n",
            "Model 3 - Epoch 6 - Batch 60/88 - Loss: 1.7633\n",
            "Model 3 - Epoch 6 - Batch 70/88 - Loss: 1.7901\n",
            "Model 3 - Epoch 6 - Batch 80/88 - Loss: 1.7020\n",
            "Model 3 - Finished Epoch 6\n",
            "Model 3 - Starting Epoch 7\n",
            "Model 3 - Epoch 7 - Batch 10/88 - Loss: 1.7373\n",
            "Model 3 - Epoch 7 - Batch 20/88 - Loss: 1.8039\n",
            "Model 3 - Epoch 7 - Batch 30/88 - Loss: 1.6961\n",
            "Model 3 - Epoch 7 - Batch 40/88 - Loss: 1.7957\n",
            "Model 3 - Epoch 7 - Batch 50/88 - Loss: 1.9595\n",
            "Model 3 - Epoch 7 - Batch 60/88 - Loss: 1.6926\n",
            "Model 3 - Epoch 7 - Batch 70/88 - Loss: 1.7340\n",
            "Model 3 - Epoch 7 - Batch 80/88 - Loss: 1.7790\n",
            "Model 3 - Finished Epoch 7\n",
            "Model 3 - Starting Epoch 8\n",
            "Model 3 - Epoch 8 - Batch 10/88 - Loss: 1.6646\n",
            "Model 3 - Epoch 8 - Batch 20/88 - Loss: 1.7676\n",
            "Model 3 - Epoch 8 - Batch 30/88 - Loss: 1.8248\n",
            "Model 3 - Epoch 8 - Batch 40/88 - Loss: 1.9319\n",
            "Model 3 - Epoch 8 - Batch 50/88 - Loss: 1.7399\n",
            "Model 3 - Epoch 8 - Batch 60/88 - Loss: 1.6920\n",
            "Model 3 - Epoch 8 - Batch 70/88 - Loss: 1.8869\n",
            "Model 3 - Epoch 8 - Batch 80/88 - Loss: 1.8063\n",
            "Model 3 - Finished Epoch 8\n",
            "Model 3 - Starting Epoch 9\n",
            "Model 3 - Epoch 9 - Batch 10/88 - Loss: 1.8103\n",
            "Model 3 - Epoch 9 - Batch 20/88 - Loss: 1.7732\n",
            "Model 3 - Epoch 9 - Batch 30/88 - Loss: 1.6937\n",
            "Model 3 - Epoch 9 - Batch 40/88 - Loss: 1.7712\n",
            "Model 3 - Epoch 9 - Batch 50/88 - Loss: 1.7225\n",
            "Model 3 - Epoch 9 - Batch 60/88 - Loss: 1.8429\n",
            "Model 3 - Epoch 9 - Batch 70/88 - Loss: 1.6543\n",
            "Model 3 - Epoch 9 - Batch 80/88 - Loss: 1.7997\n",
            "Model 3 - Finished Epoch 9\n",
            "Model 3 - Starting Epoch 10\n",
            "Model 3 - Epoch 10 - Batch 10/88 - Loss: 1.6758\n",
            "Model 3 - Epoch 10 - Batch 20/88 - Loss: 1.6500\n",
            "Model 3 - Epoch 10 - Batch 30/88 - Loss: 1.8020\n",
            "Model 3 - Epoch 10 - Batch 40/88 - Loss: 1.8180\n",
            "Model 3 - Epoch 10 - Batch 50/88 - Loss: 1.8356\n",
            "Model 3 - Epoch 10 - Batch 60/88 - Loss: 1.6675\n",
            "Model 3 - Epoch 10 - Batch 70/88 - Loss: 1.7344\n",
            "Model 3 - Epoch 10 - Batch 80/88 - Loss: 1.8258\n",
            "Model 3 - Finished Epoch 10\n",
            "Model 3 done training for 10 epochs in generation 9\n",
            "  ✅ Best Acc This Gen: 0.6853 | Worst: 0.6486\n",
            "\n",
            "Final Test Accuracy: 67.9167 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import Omniglot\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Resize transform: resize to 28x28 + ToTensor()\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize images to 28x28\n",
        "    transforms.ToTensor(),        # Convert PIL image to tensor and scale to [0,1]\n",
        "])\n",
        "\n",
        "# Load Omniglot dataset (background=True for training, False for testing)\n",
        "train_dataset = Omniglot(root='./data', background=True, download=True, transform=transform)\n",
        "test_dataset = Omniglot(root='./data', background=False, download=True, transform=transform)\n",
        "\n",
        "# Combine train and test for one dataset\n",
        "full_data = train_dataset + test_dataset\n",
        "\n",
        "# Extract data and labels into numpy arrays\n",
        "X = []\n",
        "y = []\n",
        "for img, label in full_data:\n",
        "    X.append(img.numpy())  # Keep as image tensor (1, 28, 28)\n",
        "    y.append(label)\n",
        "\n",
        "X = np.stack(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Limit to first 50 classes to keep problem manageable\n",
        "max_classes = 200\n",
        "mask = y < max_classes\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# Encode labels 0..49\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train/val/test split: 70% train, 15% val, 15% test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=0.1765, random_state=42, stratify=y_trainval\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(torch.utils.data.TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(torch.utils.data.TensorDataset(X_val_t, y_val_t), batch_size=batch_size)\n",
        "test_loader = DataLoader(torch.utils.data.TensorDataset(X_test_t, y_test_t), batch_size=batch_size)\n",
        "\n",
        "# ====== Define CNN Model (unchanged) ======\n",
        "class OmniglotCNN(nn.Module):\n",
        "    def __init__(self, num_classes=max_classes):\n",
        "        super(OmniglotCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # input channel=1 (grayscale)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# ====== Evaluation function (unchanged) ======\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(yb.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    return acc\n",
        "\n",
        "# ====== Training function for one epoch (adapted for population training) ======\n",
        "def train_one_epoch(model, optimizer, criterion, dataloader, model_id=None, epoch_num=None, log_every=10):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    print(f\"Model {model_id} - Starting Epoch {epoch_num}\")\n",
        "    for batch_idx, (xb, yb) in enumerate(dataloader, 1):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % log_every == 0:\n",
        "            avg_loss = running_loss / log_every\n",
        "            running_loss = 0\n",
        "            print(f\"Model {model_id} - Epoch {epoch_num} - Batch {batch_idx}/{len(dataloader)} - Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Model {model_id} - Finished Epoch {epoch_num}\")\n",
        "\n",
        "# ====== Evolutionary Training loop (population based) ======\n",
        "def evolution_training_until_one(population_size=10, survival_rate=0.6, epochs_per_gen=10, log_every_batches=10):\n",
        "    population, optimizers = [], []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for _ in range(population_size):\n",
        "        model = OmniglotCNN().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        population.append(model)\n",
        "        optimizers.append(optimizer)\n",
        "\n",
        "    best_model, best_optimizer, best_score = None, None, -float('inf')\n",
        "    generation = 0\n",
        "\n",
        "    while len(population) > 1:\n",
        "        generation += 1\n",
        "        print(f\"\\n🌱 Generation {generation} | Population size: {len(population)}\")\n",
        "\n",
        "        for i, (model, optimizer) in enumerate(zip(population, optimizers)):\n",
        "            for epoch in range(1, epochs_per_gen + 1):\n",
        "                train_one_epoch(model, optimizer, criterion, train_loader,\n",
        "                                model_id=i+1, epoch_num=epoch, log_every=log_every_batches)\n",
        "            print(f\"Model {i+1} done training for {epochs_per_gen} epochs in generation {generation}\")\n",
        "\n",
        "        scores = [evaluate_model(m, val_loader) for m in population]\n",
        "        sorted_indices = np.argsort(scores)[::-1]\n",
        "\n",
        "        if scores[sorted_indices[0]] > best_score:\n",
        "            best_score = scores[sorted_indices[0]]\n",
        "            best_model = population[sorted_indices[0]]\n",
        "            best_optimizer = optimizers[sorted_indices[0]]\n",
        "            print(f\"  🎉 New best model with accuracy: {best_score:.4f}\")\n",
        "\n",
        "        survivors = max(1, int(len(population) * survival_rate))\n",
        "        population = [population[i] for i in sorted_indices[:survivors]]\n",
        "        optimizers = [optimizers[i] for i in sorted_indices[:survivors]]\n",
        "\n",
        "        if best_model not in population:\n",
        "            population.append(best_model)\n",
        "            optimizers.append(best_optimizer)\n",
        "            print(\"  🔄 Best model preserved with elitism\")\n",
        "\n",
        "        print(f\"  ✅ Best Acc This Gen: {scores[sorted_indices[0]]:.4f} | Worst: {scores[sorted_indices[-1]]:.4f}\")\n",
        "\n",
        "    torch.save(best_model.state_dict(), \"best_omniglot_cnn_model.pth\")\n",
        "    return best_model\n",
        "\n",
        "# ====== Run Training ======\n",
        "best_model = evolution_training_until_one(\n",
        "    population_size=200,\n",
        "    survival_rate=0.6,\n",
        "    epochs_per_gen=10,\n",
        "    log_every_batches=10\n",
        ")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "test_acc = evaluate_model(best_model, test_loader) * 100\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f} %\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import Omniglot\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Resize transform: resize to 28x28 + ToTensor()\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize images to 28x28\n",
        "    transforms.ToTensor(),        # Convert PIL image to tensor and scale to [0,1]\n",
        "])\n",
        "\n",
        "# Load Omniglot dataset (background=True for training, False for testing)\n",
        "train_dataset = Omniglot(root='./data', background=True, download=True, transform=transform)\n",
        "test_dataset = Omniglot(root='./data', background=False, download=True, transform=transform)\n",
        "\n",
        "# Combine train and test for one dataset\n",
        "full_data = train_dataset + test_dataset\n",
        "\n",
        "# Extract data and labels into numpy arrays\n",
        "X = []\n",
        "y = []\n",
        "for img, label in full_data:\n",
        "    X.append(img.numpy())  # Keep as image tensor (1, 28, 28)\n",
        "    y.append(label)\n",
        "\n",
        "X = np.stack(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Limit to first 50 classes to keep problem manageable\n",
        "max_classes = 200\n",
        "mask = y < max_classes\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# Encode labels 0..49\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train/val/test split: 70% train, 15% val, 15% test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=0.1765, random_state=42, stratify=y_trainval\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(torch.utils.data.TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(torch.utils.data.TensorDataset(X_val_t, y_val_t), batch_size=batch_size)\n",
        "test_loader = DataLoader(torch.utils.data.TensorDataset(X_test_t, y_test_t), batch_size=batch_size)\n",
        "\n",
        "# ====== Define CNN Model ======\n",
        "class OmniglotCNN(nn.Module):\n",
        "    def __init__(self, num_classes=max_classes):\n",
        "        super(OmniglotCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # input channel=1 (grayscale)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)  # no flatten before CNN\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(yb.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    return acc\n",
        "\n",
        "# Training function for one epoch\n",
        "def train_one_epoch(model, optimizer, criterion, dataloader):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "model = OmniglotCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Optional: Enable cudnn benchmark for performance if CUDA\n",
        "if device.type == 'cuda':\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 120\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, optimizer, criterion, train_loader)\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Final evaluation on test set\n",
        "test_acc = evaluate_model(model, test_loader) * 100\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruCJnTS0q6GI",
        "outputId": "d78be9ab-c1de-4879-d00d-2d2a22e18607"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120 - Train Loss: 5.3065 - Val Accuracy: 0.0050\n",
            "Epoch 5/120 - Train Loss: 5.2999 - Val Accuracy: 0.0050\n",
            "Epoch 10/120 - Train Loss: 5.2998 - Val Accuracy: 0.0050\n",
            "Epoch 15/120 - Train Loss: 5.2994 - Val Accuracy: 0.0050\n",
            "Epoch 20/120 - Train Loss: 5.2992 - Val Accuracy: 0.0050\n",
            "Epoch 25/120 - Train Loss: 5.3003 - Val Accuracy: 0.0050\n",
            "Epoch 30/120 - Train Loss: 5.2992 - Val Accuracy: 0.0050\n",
            "Epoch 35/120 - Train Loss: 5.2991 - Val Accuracy: 0.0050\n",
            "Epoch 40/120 - Train Loss: 5.2991 - Val Accuracy: 0.0050\n",
            "Epoch 45/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 50/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 55/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 60/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 65/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 70/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 75/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 80/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 85/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 90/120 - Train Loss: 5.2991 - Val Accuracy: 0.0050\n",
            "Epoch 95/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 100/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 105/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 110/120 - Train Loss: 5.2991 - Val Accuracy: 0.0050\n",
            "Epoch 115/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "Epoch 120/120 - Train Loss: 5.2990 - Val Accuracy: 0.0050\n",
            "\n",
            "Final Test Accuracy: 0.5000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def generations_to_one(pop_size, survival_rate):\n",
        "    # Calculate how many generations until population shrinks to 1 or less\n",
        "    return 1 + math.ceil(math.log(1 / pop_size) / math.log(survival_rate))\n",
        "\n",
        "def total_epochs_final_model(pop_size, survival_rate, epochs_per_gen):\n",
        "    G = generations_to_one(pop_size, survival_rate)\n",
        "    total = G * epochs_per_gen\n",
        "    return total, G\n",
        "\n",
        "# Example usage:\n",
        "pop_size = 200\n",
        "survival_rate = 0.6\n",
        "epochs_per_gen = 10  # for example\n",
        "\n",
        "\n",
        "total, generations = total_epochs_final_model(pop_size, survival_rate, epochs_per_gen)\n",
        "print(f\"Generations: {generations}, Total epochs for final model: {total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPsHZfON_Yl",
        "outputId": "f762c952-f758-4962-de7e-a3825f757142"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations: 12, Total epochs for final model: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fXlRiq_PinhH"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}
