{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhnDIYAdAyE3RU0pqXPZsy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikalitt1/Clementine-Agriculture/blob/main/Juniper_Setup/clementine_ag_Juniper_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![clementine_logo_noback_small.PNG](data:image/png;base64,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)\n",
        "\n",
        "# Clementine Agriculture Tutorial: Training, deployment and setup of scripts for the Juniper and other modules\n",
        "\n",
        "Welcome!\n",
        "\n",
        "This tutorial will include:\n",
        "* Setting up Jetson Orin Nano for deploymeny in Clementine Ag products\n",
        "*Scripts for product operation, training, and datacollection\n"
      ],
      "metadata": {
        "id": "TzIwCOw6cl5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XtyJoBX6tXJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, run the following commands on the Jetson orin nano after initial boot into Jetpack 6.2 in order have all the packages run properly. These setup commands can be found in the following links as well:\n",
        "\n",
        "* https://docs.ultralytics.com/guides/nvidia-jetson/#install-ultralytics-package\n",
        "* https://docs.ultralytics.com/modes/train/#apple-silicon-mps-training\n",
        "* https://www.youtube.com/watch?v=7P6I2jeJNYo  (CH340 Kernel build)\n",
        "* https://www.youtube.com/watch?v=OYrSADrtSag&t=177s   (NoMachine setup)\n",
        "* https://www.stereolabs.com/en-ca/developers/drivers  (ZED Link Mono install)\n"
      ],
      "metadata": {
        "id": "afqB6KCVc6sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initial Jetson Orin Nano setup"
      ],
      "metadata": {
        "id": "n69TS7qQhmZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Jetson Orin Nano is flashed with Jetpack 6.2, use the following commands to set up nomachine."
      ],
      "metadata": {
        "id": "SHjXsZbpZVRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd Downloads"
      ],
      "metadata": {
        "id": "PMZ-xjSLfAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " wget https://www.nomachine.com/free/arm/v8/deb -O nomachine.deb"
      ],
      "metadata": {
        "id": "RroTtqLMVITj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo dpkg -i nomachine.deb"
      ],
      "metadata": {
        "id": "KOVYvT7IVIN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo systemctl set-default multi-user.target"
      ],
      "metadata": {
        "id": "evEs8KIaVIBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo /usr/NX/bin/nxserver --restart"
      ],
      "metadata": {
        "id": "lb8ixVgGVsCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no machine has issues locking to the dummy display port and creates its on virtual display, however this causes an issue. the standrd camera and GPU initialization on a jetson orin nano requires the detection of a monitor. Normally a dummy display port would work, but not if nomachine creates a virtual display. do the following commands and edits to fix this."
      ],
      "metadata": {
        "id": "9Hb4BOpebJP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo vi /usr/NX/etc/server.cfg"
      ],
      "metadata": {
        "id": "36RFvgbWbiYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this opens nomachine config file, press i to enter insert mode. add the following lines anywhere:"
      ],
      "metadata": {
        "id": "HrwxGLf_bukV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PhysicalDesktopSharing 1\n",
        "VirtualDesktopSharing 0\n"
      ],
      "metadata": {
        "id": "CHpQ9kjwb2Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "press esc then :wq to save and exit the config file. Next you need to restart nomachine server:"
      ],
      "metadata": {
        "id": "7I5JC_wRb4Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo /usr/NX/bin/nxserver --restart\n"
      ],
      "metadata": {
        "id": "ogZSTallcAag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now if you do the following command you should see :0, this means it connected to the physical display, if you see :1001, its the virtual display. Next you can reboot, but to make sure these settings remain and stay functional, i also did the following commands:"
      ],
      "metadata": {
        "id": "RLdB--sjcDlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo systemctl set-default graphical.target\n"
      ],
      "metadata": {
        "id": "bIDI3QSfcQ5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo systemctl enable gdm3\n"
      ],
      "metadata": {
        "id": "nyR8BtGGcdX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo reboot"
      ],
      "metadata": {
        "id": "JgufgSlncfk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything should work properly after this and any argus errors that come up from trying to initialize the caemra should stop."
      ],
      "metadata": {
        "id": "Vm5zunMFchTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix chromium, do the following."
      ],
      "metadata": {
        "id": "jAiTZ3CDdd5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snap download snapd --revision=24724"
      ],
      "metadata": {
        "id": "Y2D2JQMidmMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo snap ack snapd_24724.assert"
      ],
      "metadata": {
        "id": "0FqxCQQ5dmIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo snap install snapd_24724.snap"
      ],
      "metadata": {
        "id": "2HnsLjbudl_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo snap refresh --hold snapd"
      ],
      "metadata": {
        "id": "fzyqBG_Uedau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once nomachine is setup, you can set up the ZED SDK and mono Driver. Download the SDK and driver from the stereolabs website developer page. Do the following commands on the Jetson. Fill in the proper file names."
      ],
      "metadata": {
        "id": "KUB5uzwlV2kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt install libqt5core5a"
      ],
      "metadata": {
        "id": "ydk8xtxgV_dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo dpkg -i stereolabs-zedx_X.X.X-ZED-LINK-YYYY-L4TZZ.Z_arm64.deb"
      ],
      "metadata": {
        "id": "VUcSGZkzWAO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo reboot"
      ],
      "metadata": {
        "id": "YNQQ2eotV_2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Downloads"
      ],
      "metadata": {
        "id": "jlrP-pQ_V_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chmod +x ZED_SDK_Tegra_L4TXX.X_vY.Y.Y.zstd.run"
      ],
      "metadata": {
        "id": "Glhg4EGUWj0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./ZED_SDK_Tegra_L4TXX.X_vY.Y.Y.zstd.run"
      ],
      "metadata": {
        "id": "nwalhqM0Wqd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo reboot"
      ],
      "metadata": {
        "id": "gSc-UPLxWyjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the following commands to install the ROS2 hmble, make sure to download and install the workspace from my github and extract it in your home directory:"
      ],
      "metadata": {
        "id": "c_u10OMKg-9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt update\n",
        "sudo apt install -y curl gnupg lsb-release\n",
        "sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n"
      ],
      "metadata": {
        "id": "MvUF43oahLrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"deb [arch=arm64 signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\n"
      ],
      "metadata": {
        "id": "agEBQUB_hPW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt update\n",
        "sudo apt install ros-humble-desktop\n"
      ],
      "metadata": {
        "id": "DP_LaTtNhRMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt install python3-colcon-common-extensions python3-rosdep python3-rosinstall-generator python3-vcstool build-essential\n"
      ],
      "metadata": {
        "id": "sAkjMN6kouG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source /opt/ros/humble/setup.bash\n"
      ],
      "metadata": {
        "id": "HkT-VvKThixL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python3 -m pip install flask\n"
      ],
      "metadata": {
        "id": "y5LK5TiphbJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~/ros2_humble_ws"
      ],
      "metadata": {
        "id": "pbibupwshd7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colcon build"
      ],
      "metadata": {
        "id": "shlktT2ghl0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source install/setup.bash"
      ],
      "metadata": {
        "id": "S77IXopkhndF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ros2 run zed_camera_pkg yolo_node\n",
        "ros2 run zed_camera_pkg velocity_node\n",
        "ros2 run zed_camera_pkg stream_node\n",
        "ros2 run rqt_image_view rqt_image_view\n"
      ],
      "metadata": {
        "id": "BqsKmwmIhwA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next step is to start a virtual environment on the Jetson ON, but because this computer will not be used as a general device and is only running the scripts for the Juniper, its not mandatory."
      ],
      "metadata": {
        "id": "LGeSwPqtfMBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt install python3-venv\n",
        "python3 -m venv depthai\n",
        "source depthai/bin/activate"
      ],
      "metadata": {
        "id": "tB5NsN60fCcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone github repository\n",
        "git clone https://github.com/luxonis/depthai-python.git\n",
        "cd depthai-python\n",
        "python3 examples/install_requirements.py"
      ],
      "metadata": {
        "id": "43y641egfFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"export OPENBLAS_CORETYPE=ARMV8\" >> ~/.bashrc"
      ],
      "metadata": {
        "id": "J5neOPOBff_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have copy and pasted each of these commands into a freshly botted Jetson ON running Jetpack 6.2, the OAK D Lite camera should work when connected to the Jetson ON via USB port.\n",
        "\n",
        "The next commands should also be run on the Jetson ON, they are the setup commands so that Ultralytics YOLO models can run on the Jetson ON properly."
      ],
      "metadata": {
        "id": "diJH3S1SfkB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt update\n",
        "sudo apt install python3-pip -y\n",
        "pip install -U pip"
      ],
      "metadata": {
        "id": "N9vZtxuagH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "id": "fYh1ppgJgcpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo reboot"
      ],
      "metadata": {
        "id": "CkmIOYiAggbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.5.0a0+872d972e41.nv24.08-cp310-cp310-linux_aarch64.whl\n",
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.20.0a0+afc54f7-cp310-cp310-linux_aarch64.whl"
      ],
      "metadata": {
        "id": "33cd-Ak2gjNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/arm64/cuda-keyring_1.1-1_all.deb\n",
        "sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install libcusparselt0 libcusparselt-dev"
      ],
      "metadata": {
        "id": "Dhuzs-vqgm0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/onnxruntime_gpu-1.20.0-cp310-cp310-linux_aarch64.whl"
      ],
      "metadata": {
        "id": "j5UA-DCJgpoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "0IOBfKIQgszv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since numpy will be version 1.23.5 on the Jetson ON, you also need to install that version in this colab to ensure when any training scripts are run in this colab it will use the same version of numpy. If you do not do this the trained model will not work once you transfer it to the Jeton ON. Run the command below in this colab:"
      ],
      "metadata": {
        "id": "JgSRaCSugwhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "41OIb71lhFUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c5705-a79e-4761-89ed-11773ad68255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Collecting Data for Object Detection"
      ],
      "metadata": {
        "id": "oc4FqfhgLg_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect an ZED X Mini camera to this computer via a CSI CAble, run the below script and collect as much high quality video feed of the objecdts you want to identify:"
      ],
      "metadata": {
        "id": "DgZNbk6sLsJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyzed.sl as sl\n",
        "import cv2\n",
        "import signal\n",
        "\n",
        "stop = False\n",
        "\n",
        "def signal_handler(sig, frame):\n",
        "    global stop\n",
        "    stop = True\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "def main():\n",
        "    # Create ZED Camera object\n",
        "    zed = sl.Camera()\n",
        "\n",
        "    # Init parameters\n",
        "    init_params = sl.InitParameters()\n",
        "    init_params.camera_resolution = sl.RESOLUTION.HD1080  # or sl.RESOLUTION.VGA for smaller\n",
        "    init_params.camera_fps = 30\n",
        "\n",
        "    # Open camera\n",
        "    if zed.open(init_params) != sl.ERROR_CODE.SUCCESS:\n",
        "        print(\"Failed to open ZED camera\")\n",
        "        return\n",
        "\n",
        "    runtime_params = sl.RuntimeParameters()\n",
        "\n",
        "    # Set frame size for VideoWriter (must match camera_resolution)\n",
        "    frame_width = 1920\n",
        "    frame_height = 1080\n",
        "\n",
        "    output_file = \"belt_training20.avi\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")  # Use XVID codec\n",
        "\n",
        "    out = cv2.VideoWriter(output_file, fourcc, 60, (frame_width, frame_height))\n",
        "\n",
        "    if not out.isOpened():\n",
        "        print(\"Failed to open VideoWriter with XVID codec.\")\n",
        "        zed.close()\n",
        "        return\n",
        "\n",
        "    image = sl.Mat()\n",
        "\n",
        "    print(\"Recording started. Press Ctrl+C or 'q' to stop.\")\n",
        "\n",
        "    while not stop:\n",
        "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
        "            zed.retrieve_image(image, sl.VIEW.LEFT)\n",
        "            frame = image.get_data()\n",
        "\n",
        "            # Convert BGRA to BGR (drop alpha)\n",
        "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "            # Resize if needed (should already match, but just in case)\n",
        "            if (frame_bgr.shape[1], frame_bgr.shape[0]) != (frame_width, frame_height):\n",
        "                frame_resized = cv2.resize(frame_bgr, (frame_width, frame_height))\n",
        "            else:\n",
        "                frame_resized = frame_bgr\n",
        "\n",
        "            out.write(frame_resized)\n",
        "\n",
        "            preview = cv2.resize(frame_bgr, (640, 360))\n",
        "            cv2.imshow(\"ZED X Mini Preview\", preview)\n",
        "\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            print(\"Grab failed\")\n",
        "\n",
        "    print(\"Stopping recording...\")\n",
        "    out.release()\n",
        "    zed.close()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Video saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "b2ifvGjwL2UN",
        "outputId": "082522c7-103c-4ac2-a507-f59d7d1950bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a5e381080b58>:14: DeprecationWarning: RGB is deprecated, use CAM_A or address camera by name instead.\n",
            "  cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot find any device with given deviceInfo",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a5e381080b58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Connect to the device and start the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connected to OAK-D Lite. Press 'q' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot find any device with given deviceInfo"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training Ultralytics YOLO11n Model"
      ],
      "metadata": {
        "id": "MKcechtjhjHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The annotating and dataset preparation can be done for any model in Roboflow, make sure to export the dataset as a zip file in \"YOLO V5 Pytorch format\". From here on it will be assumed the dataset is already prepared and saved."
      ],
      "metadata": {
        "id": "x6QX4RJnh5sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the dataset zip file from roboflow in the side bar, then run the following commands to trin the yolo11n model. make sure to change the file names accordingly, and modify the data.yaml file to match the actual paths to the train, test, and validation paths."
      ],
      "metadata": {
        "id": "eupLTHgrjDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "uSWYGGvvxNxl",
        "outputId": "8df89aa7-4f67-420a-818a-2713efca9fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.18-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.92)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.25.0+cu128)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.35.2)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: polars-runtime-32==1.35.2 in /usr/local/lib/python3.12/dist-packages (from polars>=0.20.0->ultralytics) (1.35.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.8.0->ultralytics) (1.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.18-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.18 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y"
      ],
      "metadata": {
        "id": "5csHburhjQzR",
        "outputId": "5c14c4be-b235-434e-9f32-69bb8ff6f927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Co\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Co\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Co\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Co\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [357 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,385 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.2 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,787 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,870 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,613 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,115 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,914 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,772 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,643 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,302 kB]\n",
            "Fetched 39.9 MB in 8s (5,233 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.10 python3.10-distutils -y"
      ],
      "metadata": {
        "id": "4ZNBZaCVjQn7",
        "outputId": "5ad17872-133c-430a-94a5-54d987f5026d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3-distutils set to manually installed.\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.14).\n",
            "python3.10 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 88 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py"
      ],
      "metadata": {
        "id": "9Zl_yEXSjQkI",
        "outputId": "801de7c4-67a7-4179-c3ba-241944c12b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2142k  100 2142k    0     0  5900k      0 --:--:-- --:--:-- --:--:-- 5917k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 get-pip.py"
      ],
      "metadata": {
        "id": "9mUsARs8jQea",
        "outputId": "04a9966d-f899-43cd-94f0-af9ced422ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip\n",
            "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.46.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting packaging>=24.0 (from wheel)\n",
            "  Downloading packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-82.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.46.3-py3-none-any.whl (30 kB)\n",
            "Downloading packaging-26.0-py3-none-any.whl (74 kB)\n",
            "Installing collected packages: setuptools, pip, packaging, wheel\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [wheel]\n",
            "\u001b[1A\u001b[2KSuccessfully installed packaging-26.0 pip-26.0.1 setuptools-82.0.0 wheel-0.46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 -m pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "_cahWQaVjQPl",
        "outputId": "cd238773-4441-4117-f713-e6b243b36974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhfsBjSfxUeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c173ce2-5a03-47ce-95ab-038def7378af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/belt4_data.zip -d /content/dataset/"
      ],
      "metadata": {
        "id": "hT1Yr_IOnepJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!yolo detect train data=/content/dataset/data.yaml model=yolo26n.pt epochs=200 imgsz=640"
      ],
      "metadata": {
        "id": "4s9BFLFIjl1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861d6d58-d9ee-46e1-c25c-f0f0c66d50f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.18 🚀 Python-3.12.12 torch-2.10.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=200, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo26n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5, 3, True]        \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    119808  ultralytics.nn.modules.block.C3k2            [384, 128, 1, True]           \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     34304  ultralytics.nn.modules.block.C3k2            [256, 64, 1, True]            \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     95232  ultralytics.nn.modules.block.C3k2            [192, 128, 1, True]           \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    463104  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True, 0.5, True]\n",
            " 23        [16, 19, 22]  1    241566  ultralytics.nn.modules.head.Detect           [1, 1, True, [64, 128, 256]]  \n",
            "YOLO26n summary: 260 layers, 2,504,190 parameters, 2,504,190 gradients, 5.8 GFLOPs\n",
            "\n",
            "Transferred 606/708 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1547.0±720.8 MB/s, size: 55.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/train/labels.cache... 1534 images, 3 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1534/1534 279.7Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1114.3±588.7 MB/s, size: 59.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/valid/labels.cache... 127 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 127/127 5.8Mit/s 0.0s\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 114 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias(decay=0.0)\n",
            "Plotting labels to /content/runs/detect/train3/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train3\u001b[0m\n",
            "Starting training for 200 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/200      2.45G      1.498      2.932    0.00524         86        640: 100% ━━━━━━━━━━━━ 96/96 1.9it/s 49.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.9s/it 7.6s\n",
            "                   all        127        619        0.9      0.722      0.896      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/200      2.96G      1.478      1.768   0.004998        103        640: 52% ━━━━━━────── 50/96 3.7it/s 15.3s<12.6s"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "once run, the model weights can be found in the following file path in the side bar: /yolov5/runs/train/exp/weights/best.pt\n",
        "\n",
        "you can now send the model weights (best.pt) to the Jetson ON."
      ],
      "metadata": {
        "id": "L5ovZ-KCoRRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now on the Jetson ON, you cannot directly run the best.pt file, you first need to convert it to a .engine file with the following command, make sure to run this command on the Jetson ON!"
      ],
      "metadata": {
        "id": "iAyPQbbzoraP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo export model=yolo11n.pt format=engine # creates 'yolo11n.engine'"
      ],
      "metadata": {
        "id": "qXYkq93lo1G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now the object detection setup is done, now its time to train two random forest models to get the W.A.S.P arms to aim accurately at the targets identified by the YOLO11n model."
      ],
      "metadata": {
        "id": "5Uldq807pAwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Training Random Forest Aiming Model"
      ],
      "metadata": {
        "id": "0ibMl9ejpM8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests are a type of decision tree which can be thought of as a precursor to neural nets. They are better for maping pixel and depth coordinates to servo motor agles then traditional nurel nets because they are more robust when new data it has not been trainined on is presented. In the future the W.A.S.P will be training using reinforced learning methods in order to make sure it aims accurately and properly in a number of different scenarios. Unfortunately, Random forests cannot be fine tuned in real time like reinforced learning models. but for ealy models random forests are more then enough to aim accurately in most settings. The code below should be ran on the Jetson ON, it is used to collect data for the random forests model, remember to change the pin numbers for each arm:"
      ],
      "metadata": {
        "id": "0svO5lmJpToF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import depthai as dai\n",
        "import time\n",
        "import threading\n",
        "import numpy as np\n",
        "import csv\n",
        "from smbus2 import SMBus\n",
        "import math  # For atan2\n",
        "\n",
        "# PCA9685 Constants\n",
        "PCA9685_ADDRESS = 0x40\n",
        "MODE1 = 0x00\n",
        "MODE2 = 0x01\n",
        "LED0_ON_L = 0x06\n",
        "LED0_ON_H = 0x07\n",
        "LED0_OFF_L = 0x08\n",
        "LED0_OFF_H = 0x09\n",
        "\n",
        "bus = SMBus(7)\n",
        "\n",
        "# Initialize PCA9685\n",
        "def init_pca9685():\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE1, 0x00)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, MODE2, 0x04)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Set PWM for servo motors\n",
        "def set_pwm(channel, pulse_width):\n",
        "    on_time = 0\n",
        "    off_time = int(pulse_width)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_ON_L + 4 * channel, on_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_ON_H + 4 * channel, (on_time >> 8) & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_OFF_L + 4 * channel, off_time & 0xFF)\n",
        "    bus.write_byte_data(PCA9685_ADDRESS, LED0_OFF_H + 4 * channel, (off_time >> 8) & 0xFF)\n",
        "\n",
        "def angle_to_pulse_width(angle, min_us=500, max_us=2500):\n",
        "    return int((angle / 180.0) * (max_us - min_us) + min_us)\n",
        "\n",
        "# Control the laser diode\n",
        "def control_diode(on=True):\n",
        "    pulse_width = int((0.3 / 100.0) * 4095)\n",
        "    set_pwm(13, pulse_width if on else 0)\n",
        "\n",
        "# Save data to CSV\n",
        "def save_to_csv(angle_0, angle_1, depth, center_x, center_y, tilt_x, tilt_y):\n",
        "    with open('mlp_training_data_long.csv', mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([angle_0, angle_1, depth, center_x, center_y, tilt_x, tilt_y])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_zigzag_path(x_range, y_range, x_step, y_step, start_corner='top-left', zigzag_axis='horizontal'):\n",
        "    path = []\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "\n",
        "    # Generate the axis ranges using numpy to support floats\n",
        "    x_forward = np.arange(x_min, x_max + x_step, x_step)\n",
        "    x_backward = x_forward[::-1]\n",
        "    y_forward = np.arange(y_min, y_max + y_step, y_step)\n",
        "    y_backward = y_forward[::-1]\n",
        "\n",
        "    corners = {\n",
        "        'top-left':     (x_forward,  y_forward),\n",
        "        'top-right':    (x_backward, y_forward),\n",
        "        'bottom-left':  (x_forward,  y_backward),\n",
        "        'bottom-right': (x_backward, y_backward)\n",
        "    }\n",
        "\n",
        "    if start_corner not in corners:\n",
        "        raise ValueError(f\"Invalid start_corner '{start_corner}'. Choose from: {list(corners.keys())}\")\n",
        "    if zigzag_axis not in ['horizontal', 'vertical']:\n",
        "        raise ValueError(\"zigzag_axis must be 'horizontal' or 'vertical'\")\n",
        "\n",
        "    x_order, y_order = corners[start_corner]\n",
        "\n",
        "    if zigzag_axis == 'horizontal':\n",
        "        for i, y in enumerate(y_order):\n",
        "            x_values = x_order if i % 2 == 0 else x_order[::-1]\n",
        "            for x in x_values:\n",
        "                path.append((round(x, 6), round(y, 6)))\n",
        "    else:  # vertical\n",
        "        for i, x in enumerate(x_order):\n",
        "            y_values = y_order if i % 2 == 0 else y_order[::-1]\n",
        "            for y in y_values:\n",
        "                path.append((round(x, 6), round(y, 6)))\n",
        "\n",
        "    return path\n",
        "\n",
        "\n",
        "\n",
        "# Calculate tilt from accelerometer data\n",
        "def calculate_tilt(accel_x, accel_y, accel_z):\n",
        "    tilt_x = math.atan2(accel_x, math.sqrt(accel_y**2 + accel_z**2))  # Pitch\n",
        "    tilt_y = math.atan2(accel_y, math.sqrt(accel_x**2 + accel_z**2))  # Roll\n",
        "    return math.degrees(tilt_x), math.degrees(tilt_y)  # Convert radians to degrees\n",
        "\n",
        "def move_servos(angle_list, video_queue, depth_queue, imu_queue, stop_event):\n",
        "    angle_index = 0\n",
        "    direction = 1  # 1 for forward, -1 for reverse\n",
        "\n",
        "    while not stop_event.is_set():\n",
        "        angle_x, angle_y = angle_list[angle_index]\n",
        "\n",
        "        # Capture frame and depth before moving the servos\n",
        "        frame_data = video_queue.get()\n",
        "        frame = frame_data.getCvFrame()\n",
        "\n",
        "        depth_data = depth_queue.get()\n",
        "        depth_frame = depth_data.getFrame()\n",
        "\n",
        "        imu_data = imu_queue.get()\n",
        "        imu_packets = imu_data.packets\n",
        "\n",
        "        tilt_x, tilt_y = 0, 0  # Default tilt values\n",
        "        for imu_packet in imu_packets:\n",
        "            gyro_data = imu_packet.gyroscope\n",
        "            accel_data = imu_packet.acceleroMeter  # Assuming accelerometer data is available\n",
        "\n",
        "            accel_x = accel_data.x\n",
        "            accel_y = accel_data.y\n",
        "            accel_z = accel_data.z\n",
        "\n",
        "            tilt_x, tilt_y = calculate_tilt(accel_x, accel_y, accel_z)\n",
        "\n",
        "            print(f\"Tilt Data - Pitch: {tilt_x:.2f}, Roll: {tilt_y:.2f}\")\n",
        "\n",
        "        # Process frame to find brightest spot\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        # Convert to grayscale and blur to reduce noise\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # Apply a threshold to extract bright regions\n",
        "        _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Find contours of bright areas\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if contours:\n",
        "            # Find the largest bright area (by contour area)\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "            # Get the bounding box and center of the largest contour\n",
        "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "            center_x = x + w // 2\n",
        "            center_y = y + h // 2\n",
        "\n",
        "            # Optionally draw the bounding box and center\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "            cv2.circle(frame, (center_x, center_y), 3, (0, 0, 255), -1)\n",
        "        else:\n",
        "            center_x, center_y = -1, -1  # No bright spot found\n",
        "\n",
        "\n",
        "        # Draw a bounding circle around the brightest spot\n",
        "        cv2.circle(frame, (center_x, center_y), 2, (0, 255, 0), 2)\n",
        "\n",
        "        depth_value = np.mean(depth_frame)\n",
        "\n",
        "        # Save the data\n",
        "        save_to_csv(angle_x, angle_y, round(depth_value), center_x, center_y, round(tilt_x, 2), round(tilt_y, 2))\n",
        "\n",
        "        # Move servos\n",
        "        set_pwm(15, angle_to_pulse_width(angle_x))\n",
        "        set_pwm(14, angle_to_pulse_width(angle_y))\n",
        "\n",
        "        control_diode(True)  # Turn on laser\n",
        "\n",
        "        # Update index with direction\n",
        "        angle_index += direction\n",
        "\n",
        "        # Reverse direction at ends\n",
        "        if angle_index == len(angle_list) - 1 or angle_index == 0:\n",
        "            direction *= -1\n",
        "\n",
        "        time.sleep(0.5)  # Adjust delay if needed\n",
        "\n",
        "\n",
        "# Setup depthai pipeline\n",
        "def setup_depthai_pipeline():\n",
        "    pipeline = dai.Pipeline()\n",
        "\n",
        "    # Color Camera\n",
        "    cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
        "    cam_rgb.setPreviewSize(640, 360)\n",
        "    cam_rgb.setBoardSocket(dai.CameraBoardSocket.CAM_A)\n",
        "    cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
        "    cam_rgb.setInterleaved(False)\n",
        "    cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
        "    cam_rgb.setFps(35)\n",
        "\n",
        "    # Depth Camera\n",
        "    mono_left = pipeline.create(dai.node.MonoCamera)\n",
        "    mono_right = pipeline.create(dai.node.MonoCamera)\n",
        "    stereo = pipeline.create(dai.node.StereoDepth)\n",
        "\n",
        "    mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "    mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
        "    mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
        "    mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
        "\n",
        "    stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
        "    stereo.setLeftRightCheck(True)\n",
        "    stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)\n",
        "    stereo.setSubpixel(True)\n",
        "\n",
        "    mono_left.out.link(stereo.left)\n",
        "    mono_right.out.link(stereo.right)\n",
        "\n",
        "    #IMU\n",
        "    imu = pipeline.create(dai.node.IMU)\n",
        "    imu.enableIMUSensor(dai.IMUSensor.GYROSCOPE_RAW, 100)\n",
        "    imu.enableIMUSensor(dai.IMUSensor.ACCELEROMETER_RAW, 100)\n",
        "    imu_queue = pipeline.create(dai.node.XLinkOut)\n",
        "    imu_queue.setStreamName(\"imu\")\n",
        "    imu.out.link(imu_queue.input)\n",
        "\n",
        "    # Output Streams\n",
        "    xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
        "    xout_rgb.setStreamName(\"rgb\")\n",
        "    xout_depth = pipeline.create(dai.node.XLinkOut)\n",
        "    xout_depth.setStreamName(\"depth\")\n",
        "\n",
        "    cam_rgb.preview.link(xout_rgb.input)\n",
        "    stereo.depth.link(xout_depth.input)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def main():\n",
        "    init_pca9685()\n",
        "\n",
        "    # Generate the angle list for flat its (25, 90) and (25, 59) for long its (20, 80) and (40, 75)\n",
        "    angle_list = generate_zigzag_path((25, 80), (40, 70), 1, 1, 'top-left', 'horizontal')\n",
        "\n",
        "    # Setup depthai pipeline\n",
        "    pipeline = setup_depthai_pipeline()\n",
        "\n",
        "    # Start depthai device\n",
        "    with dai.Device(pipeline) as device:\n",
        "        video_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
        "        depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
        "        imu_queue = device.getOutputQueue(name=\"imu\", maxSize=4, blocking=False)\n",
        "        # Stop event to manage the thread\n",
        "        stop_event = threading.Event()\n",
        "\n",
        "        # Start the servo movement and data collection in a separate thread\n",
        "        threading.Thread(target=move_servos, args=(angle_list, video_queue, depth_queue, imu_queue, stop_event), daemon=True).start()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                frame_data = video_queue.get()\n",
        "                frame = frame_data.getCvFrame()\n",
        "\n",
        "                # Convert to grayscale and blur to reduce noise\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "                # Apply a threshold to extract bright regions\n",
        "                _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "                # Find contours of bright areas\n",
        "                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                if contours:\n",
        "                    # Find the largest bright area (by contour area)\n",
        "                    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "                    # Get the bounding box and center of the largest contour\n",
        "                    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "                    center_x = x + w // 2\n",
        "                    center_y = y + h // 2\n",
        "\n",
        "                    # Draw the bounding box and center\n",
        "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "                    cv2.circle(frame, (center_x, center_y), 3, (0, 0, 255), -1)\n",
        "                else:\n",
        "                    center_x, center_y = -1, -1\n",
        "                    cv2.circle(frame, (center_x, center_y), 2, (0, 255, 0), 2)\n",
        "\n",
        "                # Display the current frame with circle\n",
        "                cv2.imshow(\"Detection\", frame)\n",
        "\n",
        "                if cv2.waitKey(1) == ord(\"q\"):\n",
        "                    stop_event.set()\n",
        "                    break\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"Shutting down...\")\n",
        "                stop_event.set()\n",
        "                break\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "q2bk22GxFzR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once enough data with diverse depth values has been collected, upload the csv files to the sidebar on this notebook and run the code below to train the random forest model, remeber to change the file names and train both arms using seperate csv files. This means you need to run the following code twice for each csv file for each arm."
      ],
      "metadata": {
        "id": "B8jjw5qjF2eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Load CSV\n",
        "csv_filename = \"/content/RF_Data_Arm3_cleaned.csv\"\n",
        "df = pd.read_csv(csv_filename, header=0)\n",
        "df.columns = [\"angle_x\", \"angle_y\", \"pixel_x\", \"pixel_y\", \"depth\", \"pitch\", \"roll\"]\n",
        "\n",
        "# Features and targets\n",
        "y = df[[\"angle_x\", \"angle_y\"]].values\n",
        "x = df[[\"pixel_x\", \"pixel_y\", \"depth\", \"pitch\", \"roll\"]].values\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Decision Tree Regressor ==========\n",
        "# Initialize and train decision tree\n",
        "tree = DecisionTreeRegressor(max_depth=22, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Save the decision tree model\n",
        "tree_model_path = \"/content/decision_tree_flat_fine2.pkl\"\n",
        "with open(tree_model_path, 'wb') as f:\n",
        "    pickle.dump(tree, f)\n",
        "\n",
        "print(f\"Decision tree trained and saved to {tree_model_path}\")\n",
        "files.download(tree_model_path)\n",
        "\n",
        "# Predict on validation set\n",
        "val_preds_tree = tree.predict(X_val)\n",
        "\n",
        "# Compute MAE for each angle\n",
        "mae_angle0_tree = mean_absolute_error(y_val[:, 0], val_preds_tree[:, 0])\n",
        "mae_angle1_tree = mean_absolute_error(y_val[:, 1], val_preds_tree[:, 1])\n",
        "avg_mae_tree = (mae_angle0_tree + mae_angle1_tree) / 2\n",
        "\n",
        "print(f\"\\nDecision Tree Validation MAE:\")\n",
        "print(f\"  angle_x: {mae_angle0_tree:.2f} degrees\")\n",
        "print(f\"  angle_y: {mae_angle1_tree:.2f} degrees\")\n",
        "print(f\"  Average MAE: {avg_mae_tree:.2f} degrees\")\n",
        "\n",
        "# ====== Compare Predictions to Actuals ======\n",
        "print(\"\\nSample Predictions vs Actual Values for Decision Tree:\")\n",
        "for i in range(10):\n",
        "    predicted = val_preds_tree[i]\n",
        "    actual = y_val[i]\n",
        "    print(f\"[{i}] Predicted: angle_x = {predicted[0]:.2f}, angle_y = {predicted[1]:.2f}  |  Actual: angle_x = {actual[0]:.2f}, angle_y = {actual[1]:.2f}\")\n",
        "\n",
        "\n",
        "# ========== Random Forest Regressor ==========\n",
        "# Initialize and train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Save the Random Forest model\n",
        "rf_model_path = \"/content/RF_Metal_Arm3.pkl\"\n",
        "with open(rf_model_path, 'wb') as f:\n",
        "    pickle.dump(rf, f)\n",
        "\n",
        "print(f\"Random Forest trained and saved to {rf_model_path}\")\n",
        "files.download(rf_model_path)\n",
        "\n",
        "# Predict on validation set for Random Forest\n",
        "val_preds_rf = rf.predict(X_val)\n",
        "\n",
        "# Compute MAE for Random Forest\n",
        "mae_angle0_rf = mean_absolute_error(y_val[:, 0], val_preds_rf[:, 0])\n",
        "mae_angle1_rf = mean_absolute_error(y_val[:, 1], val_preds_rf[:, 1])\n",
        "avg_mae_rf = (mae_angle0_rf + mae_angle1_rf) / 2\n",
        "\n",
        "print(f\"\\nRandom Forest Validation MAE:\")\n",
        "print(f\"  angle_x: {mae_angle0_rf:.2f} degrees\")\n",
        "print(f\"  angle_y: {mae_angle1_rf:.2f} degrees\")\n",
        "print(f\"  Average MAE: {avg_mae_rf:.2f} degrees\")\n",
        "\n",
        "# ====== Compare Predictions to Actuals ======\n",
        "print(\"\\nSample Predictions vs Actual Values for Random Forest:\")\n",
        "for i in range(10):\n",
        "    predicted = val_preds_rf[i]\n",
        "    actual = y_val[i]\n",
        "    print(f\"[{i}] Predicted: angle_x = {predicted[0]:.2f}, angle_y = {predicted[1]:.2f}  |  Actual: angle_x = {actual[0]:.2f}, angle_y = {actual[1]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uriUGi1lGPdn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "d38cb955-85cb-446b-e880-5b66f20779db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree trained and saved to /content/decision_tree_flat_fine2.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e8c69ce-a5df-477a-88b1-d537eb0fe86e\", \"decision_tree_flat_fine2.pkl\", 685162)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Validation MAE:\n",
            "  angle_x: 1.21 degrees\n",
            "  angle_y: 0.23 degrees\n",
            "  Average MAE: 0.72 degrees\n",
            "\n",
            "Sample Predictions vs Actual Values for Decision Tree:\n",
            "[0] Predicted: angle_x = 228.00, angle_y = 200.00  |  Actual: angle_x = 227.50, angle_y = 200.00\n",
            "[1] Predicted: angle_x = 217.00, angle_y = 195.50  |  Actual: angle_x = 221.00, angle_y = 196.00\n",
            "[2] Predicted: angle_x = 224.50, angle_y = 195.00  |  Actual: angle_x = 221.50, angle_y = 195.50\n",
            "[3] Predicted: angle_x = 205.50, angle_y = 207.50  |  Actual: angle_x = 204.50, angle_y = 207.50\n",
            "[4] Predicted: angle_x = 205.50, angle_y = 176.00  |  Actual: angle_x = 205.00, angle_y = 176.00\n",
            "[5] Predicted: angle_x = 219.50, angle_y = 202.00  |  Actual: angle_x = 220.00, angle_y = 202.00\n",
            "[6] Predicted: angle_x = 209.50, angle_y = 206.50  |  Actual: angle_x = 210.00, angle_y = 207.50\n",
            "[7] Predicted: angle_x = 207.00, angle_y = 209.00  |  Actual: angle_x = 204.00, angle_y = 209.50\n",
            "[8] Predicted: angle_x = 219.50, angle_y = 200.50  |  Actual: angle_x = 219.00, angle_y = 199.50\n",
            "[9] Predicted: angle_x = 210.50, angle_y = 186.50  |  Actual: angle_x = 213.50, angle_y = 187.00\n",
            "Random Forest trained and saved to /content/RF_Metal_Arm3.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1791be01-6b20-48c0-b30f-031793ae06a1\", \"RF_Metal_Arm3.pkl\", 43264532)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Validation MAE:\n",
            "  angle_x: 1.13 degrees\n",
            "  angle_y: 0.15 degrees\n",
            "  Average MAE: 0.64 degrees\n",
            "\n",
            "Sample Predictions vs Actual Values for Random Forest:\n",
            "[0] Predicted: angle_x = 226.71, angle_y = 200.01  |  Actual: angle_x = 227.50, angle_y = 200.00\n",
            "[1] Predicted: angle_x = 219.45, angle_y = 196.03  |  Actual: angle_x = 221.00, angle_y = 196.00\n",
            "[2] Predicted: angle_x = 223.37, angle_y = 195.54  |  Actual: angle_x = 221.50, angle_y = 195.50\n",
            "[3] Predicted: angle_x = 205.19, angle_y = 207.53  |  Actual: angle_x = 204.50, angle_y = 207.50\n",
            "[4] Predicted: angle_x = 204.59, angle_y = 175.78  |  Actual: angle_x = 205.00, angle_y = 176.00\n",
            "[5] Predicted: angle_x = 218.87, angle_y = 201.83  |  Actual: angle_x = 220.00, angle_y = 202.00\n",
            "[6] Predicted: angle_x = 211.33, angle_y = 207.59  |  Actual: angle_x = 210.00, angle_y = 207.50\n",
            "[7] Predicted: angle_x = 206.26, angle_y = 209.40  |  Actual: angle_x = 204.00, angle_y = 209.50\n",
            "[8] Predicted: angle_x = 220.16, angle_y = 199.31  |  Actual: angle_x = 219.00, angle_y = 199.50\n",
            "[9] Predicted: angle_x = 211.84, angle_y = 187.12  |  Actual: angle_x = 213.50, angle_y = 187.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the .pkl random forest files have been trained, upload them to the Jetson ON."
      ],
      "metadata": {
        "id": "-l4sAq0nu9DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are NaN values in the csv files you can run the following code to clean it up:"
      ],
      "metadata": {
        "id": "S0Bh_GadUE0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to your original CSV file\n",
        "input_csv = \"/content/RF_Data_Arm3.csv\"\n",
        "\n",
        "# Output path for cleaned CSV\n",
        "output_csv = \"/content/RF_Data_Arm3_cleaned.csv\"\n",
        "\n",
        "# Load the CSV file (no header)\n",
        "df = pd.read_csv(input_csv, header=None)\n",
        "\n",
        "# Set column names\n",
        "df.columns = [\"angle_x\", \"angle_y\", \"pixel_x\", \"pixel_y\", \"depth\", \"pitch\", \"roll\"]\n",
        "\n",
        "# Drop rows with NaNs in input or target columns\n",
        "df_cleaned = df.dropna(subset=[\"angle_x\", \"angle_y\", \"pixel_x\", \"pixel_y\", \"depth\", \"pitch\", \"roll\"])\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV\n",
        "df_cleaned.to_csv(output_csv, index=False, header=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved to: {output_csv}\")\n",
        "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_cleaned)}\")"
      ],
      "metadata": {
        "id": "hJaYMFhgUNQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aedc022-5a58-4934-c580-967258ab847a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset saved to: /content/RF_Data_Arm3_cleaned.csv\n",
            "Original rows: 5348, Cleaned rows: 5346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Aiming While in Motion"
      ],
      "metadata": {
        "id": "PhLW9lIPGZAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ZED X Mini is very low latency, so for aiming in motion a simple Kalman filter is highly effective."
      ],
      "metadata": {
        "id": "UYY0DDsHGhZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Main code"
      ],
      "metadata": {
        "id": "z1qBndkzIRWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Main script will be run in a Ros2 workspace, differ to the Ros2_WS folder for more information:"
      ],
      "metadata": {
        "id": "u-tqOfpEIU7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To increase the inference speed of the Yolo11n ultralytics model run the following two lines on the Jetson ON:"
      ],
      "metadata": {
        "id": "YFV7GoThlkq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sudo nvpmodel -m 0"
      ],
      "metadata": {
        "id": "hiLTERkUluKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo jetson_clocks"
      ],
      "metadata": {
        "id": "MxUgTm6glwq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}